"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4723],{9945:function(n){n.exports=JSON.parse('{"blogPosts":[{"id":"convnext","metadata":{"permalink":"/papers/convnext","source":"@site/papers/2022-08-31-convnext/index.mdx","title":"A ConvNet for the 2020s (ConvNeXt)","description":"Convolution\uc774 \ub9d0\ud569\ub2c8\ub2e4. \ud615 \uc544\uc9c1 \uc8fd\uc9c0 \uc54a\uc558\uc5b4 \uc784\ub9c8.","date":"2022-08-31T00:00:00.000Z","formattedDate":"August 31, 2022","tags":[{"label":"paper-review","permalink":"/papers/tags/paper-review"}],"readingTime":17.16,"truncated":false,"authors":[{"name":"Hyoung-Kyu Song","title":"AI Researcher (Vision)","url":"https://github.com/deepkyu","imageURL":"https://github.com/deepkyu.png","key":"hkyu"}],"frontMatter":{"slug":"convnext","title":"A ConvNet for the 2020s (ConvNeXt)","description":"Convolution\uc774 \ub9d0\ud569\ub2c8\ub2e4. \ud615 \uc544\uc9c1 \uc8fd\uc9c0 \uc54a\uc558\uc5b4 \uc784\ub9c8.","image":"img/default.png","authors":["hkyu"],"tags":["paper-review"]},"nextItem":{"title":"Activate or Not: Learning Customized Activation","permalink":"/papers/acon"}},"content":"import clsx from \'clsx\';\\nimport styles from \'../blog.module.css\';\\n\\nimport figImagenetOneK from \'./image/figure1-imagenet1k.png\';\\nimport figBlock from \'./image/figure2-block.png\';\\nimport figBlockDesign from \'./image/figure3-block-design.png\';\\nimport figPretrain from \'./image/figure4-pretrain.png\';\\nimport figFinetune from \'./image/figure5-finetune.png\';\\n\\n[github](https://github.com/facebookresearch/ConvNeXt)\\n\\n## Main Idea\\n\\nVision Task\uc5d0 Transformer \uae30\ubc18 Architecture\ub97c \uc811\ubaa9\ud558\ub294 ViT\uc758 \ub4f1\uc7a5 \uc774\ud6c4, classification task\uc5d0 \ub300\ud574 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. ViT \uc774\ud6c4\uc5d0 Swim Transformers \ub4f1\uc758 \ubc29\ubc95\ub860 \ub4f1\uc740 segmentation\uc774\ub098 object detection\uc5d0\ub3c4 transformer\ub97c \uc801\uc6a9\ud558\uae30 \uc704\ud574 \ub4f1\uc7a5\ud55c \ubc29\ubc95\ub4e4\uc774\uc5c8\uc2b5\ub2c8\ub2e4. Swim Transformer\uc758 \uacbd\uc6b0, \uc5ec\ub7ec ConvNet\uc744 prior\ub85c \uc0bc\ub294 hybrid\ud55c \ubc29\ubc95\uc73c\ub85c \uc774\ub97c \ud574\uacb0\ud55c \uac83\uc774 \ud2b9\uc9d5\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc, hybrid\ud558\ub2e4\uace0 \ud558\uae30\uc5d0\ub294 \uae30\uc874 Transformer\uc758 \ud798\uc744 \ube4c\ub9b0 \uac83\uc77c \ubfd0, ConvNet \uc790\uccb4\uac00 \uac00\uc9c0\uace0 \uc788\ub294 inductive bias\ub97c \ucd5c\ub300\ud55c \uc0ac\uc6a9\ud55c \ubc29\ubc95\uc740 \uc544\ub2d9\ub2c8\ub2e4.\\n\\n\uc800\uc790\ub294 \uc21c\uc218\ud55c ConvNet(Transformer \uad6c\uc870\ub97c \uacc1\ub4e4\uc774\uc9c0 \uc54a\uc740)\uc758 \ud798\uc744 \ud655\uc778\ud558\uace0\uc790 \uae30\uc874\uc758 standard ResNet(architecture + \ud559\uc2b5 \ubc29\ubc95\ub860)\uc5d0\uc11c vision Transformer\ucc98\ub7fc \ud559\uc2b5\ud560 \uc218 \uc788\uac8c\ub054 \uc9c0\uae08\uae4c\uc9c0 \ub4f1\uc7a5\ud574\uc628 \uc5ec\ub7ec Novelty\ub4e4\uc744 \uc811\ubaa9\ud574\ubcf4\uace0\uc790 \uc2dc\ub3c4\ud569\ub2c8\ub2e4. \ud2b9\ud788, vision Transformer \ubaa8\ub378\ub4e4\uc774 \ub4f1\uc7a5\ud560 \ub54c \ud56d\uc0c1 \uc0c8\ub85c\uc6b4 \ud559\uc2b5 \ubc29\ubc95\ub860\uc744 \ud568\uaed8 \ub4e4\uace0 \ub098\uc640\uc11c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc8fc\uc7a5\ud558\uace0\ub294 \ud558\ub294\ub370, \uadf8 \ud559\uc2b5 \ubc29\ubc95\ub860\ub4e4\uc744 \uae30\uc874 ConvNet\ub4e4\uc5d0 \uc801\uc6a9\ud574\ubcf8 \uc0ac\ub840\uac00 \ub9ce\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. \uc5ec\ub7ec \uc2e4\ud5d8\uc744 \ud1b5\ud574 \ucd5c\uadfc\uc5d0 \ub4f1\uc7a5\ud55c \ud559\uc2b5 \ubc29\ubc95\ub860\uc744 \uc801\uc6a9\ud558\uace0 convolution block design\uc744 \uc0c8\ub86d\uac8c \ub514\uc790\uc778\ud558\uba74 \ub354 \uc9c1\uad00\uc801\uc778 \uad6c\uc870\ub85c Transformer\uc5d0 \uadfc\uc811\ud55c \uc131\ub2a5\uc744 \ubcf4\uc77c \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\\n\\n\uac00\ub054 ConvNeXt \ub17c\ubb38\uc744 \ub4e4\uc5b4\ubcf4\uae30\ub9cc \ud558\uace0 \ucc29\uac01\ud558\ub294 \uac83 \uc911 \ud558\ub098\uac00, ConvNeXt\uc758 FLOPs \uac00 Transformer\ubcf4\ub2e4 \ud604\uc800\ud788 \uc791\uc740 \ucc44\ub85c \uc131\ub2a5\uc744 \ube44\ub4f1\ud558\uac8c \ub0b8 \uac83\uc73c\ub85c \uc544\uc2dc\ub294 \ubd84\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4. ConvNeXt\ub294 architecture\uac00 (inductive bias\uc5d0\uc11c \ub4f1\uc7a5\ud588\ub358) ConvNet\uc744 \uc21c\uc218\ud558\uac8c \uae30\ubc18\uc73c\ub85c \ud588\uc744 \ubfd0, \uadf8 \ubaa8\ub378 \uc0ac\uc774\uc988\uac00 Transformers \ubcf4\ub2e4 \uc808\ub300 \uc791\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \ub2e4\ub9cc, \uc0c1\ub300\uc801\uc73c\ub85c Transformer\ubcf4\ub2e4\ub294 convolution layer\uc5d0 \ub300\ud55c compression \ubc29\ubc95\ub860\ub4e4\uc774 \ub354 \ub9ce\uc774 \uace0\uc548\ub41c\ub9cc\ud07c, \uadf8 \uc9c1\uad00\uc801\uc778 \uad6c\uc870\uc5d0 \uc758\ud574 \ub4f1\uc7a5\ud588\ub358 compression \ubc29\ubc95\ub860\ub4e4\uc744 \ub450\ub8e8 \uc801\uc6a9\ud558\uc5ec \uc55e\uc73c\ub85c \uacbd\ub7c9\ud654\ud560 \uc218 \uc788\ub294 \uac00\ub2a5\uc131\uc774 \uc880 \ub354 \uc788\ub2e4\ub294 \uac8c \uc81c \uac1c\uc778\uc801\uc778 \uc0dd\uac01\uc785\ub2c8\ub2e4.\\n\\n\uc8fc\ub85c ResNet 50 \uc744 \uae30\uc900\uc73c\ub85c \uc131\ub2a5 report\ub97c \uc9c4\ud589\ud558\uace0, \uac01 accuracy\ub294 random seed\ub97c \ub2e4\ub974\uac8c \ud558\uc5ec 3\ubc88\uc529 \uc2e4\ud5d8\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4. \uc694\uc998 \ub17c\ubb38\uce58\uace0 \ucca0\uc800\ud558\ub2e4\uace0 \ubcf4\ub294 \ubd84\ub3c4 \uacc4\uc2dc\uace0, \ud1b5\uacc4\uc801\uc73c\ub85c \ubb34\uc2a8 \uc758\ubbf8\uac00 \uc788\ub0d0\uace0 \ud558\uc2dc\ub294 \ubd84\ub3c4 \uc788\ub354\ub77c\uace0\uc694 \u314e\u314e \ub2e4\ub9cc, publish \uc774\ud6c4 \ud559\uacc4\uc5d0\uc11c \uac80\uc99d\ud558\uba74\uc11c ConvNeXt \uc2e4\ud5d8 \uacb0\uacfc\uac00 \uc798\ubabb\ub410\ub2e4\uace0 \ud558\ub294 \ub17c\ubb38\uc740 \ubcf8 \uc801\uc774 \uc5c6\uc2b5\ub2c8\ub2e4.\\n\\n## Background Knowledge\\n\\n### Examples of Representative ConvNet\\n\\nVGGNet, Inceptions, ResNe(X)t, DenseNet, MobileNet, EfficientNet and RegNet\\n\\n### ConvNet \uc758 \uc8fc\uc694 \ud2b9\uc9d5\ub4e4\\n\\n\uc544\ub798\ub294 \\"sliding window\\"\ub97c \uc0ac\uc6a9\ud558\ub294 convolution\uc5d0\uc11c \uace0\uc548\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uc0dd\uaca8\ub098\ub294 \ud2b9\uc9d5\ub4e4\uc785\ub2c8\ub2e4.\\n\\n- Translation equivariant\\n  - Object detection \ub4f1\uc758 Task\uc5d0 \uc788\uc5b4\uc11c \ud2b9\ud788 \uc720\uc6a9\ud569\ub2c8\ub2e4.\\n  - \uac00\ub054 equivariance, invariance \ud5f7\uac08\ub824 \ud558\uc2dc\ub294 \ubd84\ub4e4 \uacc4\uc154\uc11c \ub9d0\uc500\ub4dc\ub9ac\uba74, Patch\ub97c \uc774\ub3d9\ud558\ub4e0, \uadf8 \uacb0\uacfc\uac12\uc744 \uc774\ub3d9\ud55c\ub2e4\uace0 feature vector \uac12\uc774 \ubc14\ub00c\ub294 \uac74 \uc544\ub2c8\uc5ec\uc11c invariant \ud558\ub2e4\uace0\ub3c4 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. (g: Identity Mapping)\\n- Weight Sharing\\n\\n## \ubcc0\ud6541: Training Methodology\\n\\n\uc800\uc790\ub294 ResNet 50 \uc5d0 DeiT, Swim Transformer\uc640 \uc720\uc0ac\ud55c training recipe\ub97c \uc801\uc6a9\ud558\uc5ec \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc5bc\ub9c8\ub098 \uc77c\uc5b4\ub098\ub294 \uc9c0\ub97c \uc0b4\ud3b4\ubd05\ub2c8\ub2e4.\\n\\n- Training epoch \uc99d\uac00\\n  - 90 -> 300\\n- Optimizer \ubcc0\uacbd\\n  - Adam -> AdamW\\n- Data augmentation\\n  - Mixup, Cutmix, RandAugment, Random Erasing\uc744 \ucd94\uac00\\n- Regularization scheme \ucd94\uac00\\n  - Stochastic depth\\n    - Depth\ub97c \uc774\ub8e8\ub294 ResBlocks \uc911 \uc77c\ubd80\ub97c random\ud558\uac8c drop\ud558\uba74\uc11c \ud559\uc2b5\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c drop \ud55c\ub2e4\ub294 \uc758\ubbf8\ub294 \uad6c\ud604\uc5d0\uc11c \ubd24\uc744 \ub54c\ub294 ResBlock\uacfc identity mapping(skip connection) \uc911 \uc120\ud0dd\ud558\ub294 \uac83\uacfc \uac19\uc2b5\ub2c8\ub2e4.\\n      - \\"... Aims to shrink the depth of a network during training, while keeping it unchanged during testing. This is achieved by randomly dropping entire\xa0ResBlocks\xa0during training and bypassing their transformations through skip connections.\\"\\n  - Label smoothing\\n\\n\uc774 \ubc29\ubc95\ub860 \ubcc0\uacbd\uc744 \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 76.1% -> 78.8% (+2.7%)\\n\\n\uc5b4\uca4c\uba74 traditional ConvNets\uc640 vision Transformers\uc758 \ucc28\uc774\ub294 \ud559\uc2b5 \ubc29\ubc95\uc5d0\uc11c \uc8fc\ub85c \uae30\uc778\ud55c \uac8c \uc544\ub2d0\uae4c \uc2f6\uc744 \uc815\ub3c4\uc758 \ucc28\uc774\uc77c \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.  \\n\uc800\uc790\ub294 \uc774 \ud30c\ud2b8\uc5d0\uc11c \ucc3e\uc740 training recipe\ub97c hyperparameter\uc640 \ud568\uaed8 \uc720\uc9c0\ud558\uba74\uc11c \uc544\ub798\uc758 design \ubcc0\uacbd\ub4e4\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4.\\n\\n## \ubcc0\ud6542: \ud070 \ud2c0\uc5d0\uc11c\uc758 \uad6c\uc870 \ubcc0\uacbd\\n\\n### Stage Compute Ratio\ub97c (3, 3, 9, 3)\uc73c\ub85c \ubcc0\uacbd\\n\\n\uae30\uc874 ResNet\uc758 stage\ubcc4 \ub514\uc790\uc778\uc740 \ub9e4\uc6b0 \uacbd\ud5d8\uc801\uc73c\ub85c \uacb0\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n<img className={styles.figCenter} src={figImagenetOneK} alt=\\"imagenet1k\\" />\\n\\n4\ubc88\uc9f8 stage\uc5d0 layer\uac00 \ub9ce\uae30 \ub54c\ubb38\uc5d0(ResNet50 \uae30\uc900 6), object detection \ub4f1\uc758 downstream task\uc5d0 \uc811\ubaa9\ub418\uae30 \uc704\ud55c backbone\uc73c\ub85c \ub9ce\uc774 \uc4f0\uc77c \uc218 \uc788\uc5c8\uace0, \ud2b9\ud788 \uc774 \ub54c\uc758 feature map \uc0ac\uc774\uc988\uac00 14 x 14 \uc774\uae30\uc5d0 detector head\ub85c\uc11c\uc758 \uc5ed\ud560\ub3c4 \uacb8\ud560 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. Swim Transformer\ub294 \uc774\uc640\ub294 \ube44\uc2b7\ud558\ub098 \uc870\uae08 \ub2e4\ub978 stage ratio\ub97c \ubcf4\uc5ec\uc8fc\ub294\ub370, \uc791\uc740 \ubaa8\ub378\uc758 \uacbd\uc6b0 (1, 1, 3, 1), \ud070 \ubaa8\ub378\uc758 \uacbd\uc6b0 (1, 1, 9, 1)\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\\n\\n\uc800\uc790\ub294 Swim-T\uc640\uc758 FLOPs\ub97c \uc720\uc0ac\ud55c \ube44\uc728\ub85c \uac00\uc838\uac00\uae30 \uc704\ud574, \uae30\uc874\uc758 stage ratio (3, 4, 6, 3)\uc744 (3, 3, 9, 3)\uc73c\ub85c \ubcc0\uacbd\ud588\uc2b5\ub2c8\ub2e4.\\n\\n\uc774 \ubc29\ubc95\ub860 \ubcc0\uacbd\uc744 \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 78.8% -> 79.4 (+0.6%, \ub204\uc801: +3.3%)\\n\\nNetwork design space\uc5d0 \uad00\ud55c \uc5f0\uad6c\ub4e4\uc744 \uc0b4\ud3b4\ubcf4\uba74, \uc774\ubcf4\ub2e4 \ucda9\ubd84\ud788 \ub354 \uc88b\uc740 stage compute ratio\uac00 \uc788\uc744 \uc218\ub3c4 \uc788\ub2e4\uace0 \uc800\uc790\ub294 \ub9d0\ud569\ub2c8\ub2e4.\\n\\n### Patch\ub97c \ub9cc\ub4dc\ub294 \uccab stem layer\uc744 Conv(ks=4, stride=4)\ub85c \ubcc0\uacbd (non-overlapping conv.)\\n\\nStem cell design\uc740 \uc544\ud0a4\ud14d\uccd0 \uac00\uc7a5 \ucc98\uc74c\uc5d0 input image\ub97c \uc5b4\ub5bb\uac8c \ucc98\ub9ac\ud574\uc904 \uc9c0\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4. ResNet\uc5d0\uc11c\ub294 kernel size 7x7, stride 2 \uc758 Conv layer(2x downsample)\uacfc max-pooling(2x downsample)\uc744 \ud1b5\ud574 4x downsample \uc2dc\ud0a4\ub294 stem cell design\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. vision Transformer \ub4e4\uc740 \uc774\ubcf4\ub2e4\ub3c4 \ub354 \uacfc\uac10\ud558\uac8c patch\ub97c \ub9cc\ub4dc\ub294 \uc804\ub7b5\uc744 \uc0ac\uc6a9\ud558\ub294 \ub370, 14x14 \ub610\ub294 16x16\uc758 \uc544\uc8fc \ud070 kernel size\ub97c \uac00\uc9c4 Conv layer\ub97c \uacb9\uce58\ub294 \ubd80\ubd84\uc774 \uc5c6\uac8c\ub054(kernel size\uc640 stride\uac00 \ub3d9\uc77c) \uc124\uc815\ud569\ub2c8\ub2e4. Swim Transformer\uc758 \uacbd\uc6b0, \uc774 design\uc5d0 \ucd94\uac00\ub85c multi-stage design\uc744 \uc704\ud574 4x4 patch size\uc758 layer\ub97c \ubcc4\uac1c\ub85c \ub450\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n\uc800\uc790\ub294 ResNet-style stem cell\ub85c\uc11c kernel size 4x4, stride 4\ub85c \uc124\uc815\ud55c conv. layer(4x downsample)\ub97c stem cell design\uc73c\ub85c \uc120\uc815\ud588\uc2b5\ub2c8\ub2e4.\\n\\n\uc774 \ubc29\ubc95\ub860 \ubcc0\uacbd\uc744 \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 79.4% -> 79.5% (+0.1%, \ub204\uc801: +3.4%)\\n\\nResNet\uc5d0\uc11c patch\ub97c \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc5d0 \uc788\uc5b4\uc11c\ub294 \uc774\ub807\uac8c ViT \ub4f1\uc5d0\uc11c \ubcf4\uc5ec\uc900 \ub354 \uc26c\uc6b4 \ubc29\ubc95\ub4e4\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc73c\ub85c \uad50\uccb4 \uac00\ub2a5\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.\\n\\n## \ubcc0\ud6543: ResNeXt \uc544\uc774\ub514\uc5b4 \uc801\uc6a9\ud558\uae30\\n\\nResNeXt \uc5d0\uc11c\ub294 bottleneck block\uc5d0\uc11c grouped convolution\uc744 \uc0ac\uc6a9\ud568\uc73c\ub85c\uc11c, FLOPs\ub97c \uc904\uc774\uace0 network width(# channel in hidden layers) \ub97c \ub298\ub9b4 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n\uc800\uc790\ub294 grouped convolution\uc758 \ub9e5\ub77d\uc5d0\uc11c depthwise convolution\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Depthwise convolution\uc740 per-channel\ub85c self-attention\uc758 weighted sum\uacfc \ub3d9\uc77c\ud55c \uc5ed\ud560\uc744 \ud558\uac8c \ub418\ub294\ub370\uc694. \uc989, spatial dimension\uc73c\ub85c\ub9cc \uc815\ubcf4\ub97c \uc11e\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.\\n\\n\uc774\ub97c \ud1b5\ud574 FLOPs\ub294 \ud6a8\uacfc\uc801\uc73c\ub85c \uc904\uc774\uba74\uc11c\ub3c4 \uc815\ud655\ub3c4\ub294 \ub354\uc6b1 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Width\ub97c \ud655\uc7a5\ud560 \uc218 \uc788\uac8c \ub41c \ub9cc\ud07c, width\ub97c Swim Transformer\ub97c \ub530\ub77c 64\uc5d0\uc11c 96\uc73c\ub85c \ub298\ub9ac\uac8c \ub418\uc5c8\ub2e4. \uadf8\ub9ac\uace0 \uc774\ub97c \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 79.5% -> 80.5% (+1.0%, \ub204\uc801: +4.4%)\\n\\n<img className={styles.figCenter} src={figBlock} alt=\\"block\\" />\\n\\n(a): ResNeXt block / (b): inverted bottleneck / (c): position switch of depthwise conv layer\\n\\n## \ubcc0\ud6544: Inverted Bottleneck \uc801\uc6a9\ud558\uae30\\n\\nTransformer\uc5d0\uc11c\uc758 inverted bottleneck\uc740 MLP block \uc911 hidden layer\uc758 dimension\uc774 input dimensionqhek 4\ubc30 \ud06c\uac8c \ub514\uc790\uc778 \ub41c \uac83\uc744 \ub9d0\ud569\ub2c8\ub2e4. ConvNet\uc5d0\uc11c\ub3c4 inverted bottleneck\uc740 MobileNetV2\uc5d0\uc11c \ub4f1\uc7a5\ud55c \uc774\ud6c4 \ub450\ub8e8 \uc4f0\uc774\uace0 \uc788\ub294\ub370, layer\uc758 \ud615\ud0dc\ub9cc \ub2e4\ub97c \ubfd0 Transformer\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 inverted block\uacfc \uc720\uc0ac\ud558\ub2e4\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ubc18\uc601\ud558\uba74, \uc704 figure \ub0b4 (a)\uc5d0\uc11c (b)\ub85c\uc758 \ubcc0\ud654\ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. Depthwise convolution(\ubcf4\ub77c\uc0c9)\uc758 FLOPs \ub294 \ub298\uc5b4\ub098\uc9c0\ub9cc \uc804\uccb4 \ub124\ud2b8\uc6cc\ud06c\uc758 FLOPs\ub294 \uc904\uc5b4\ub4dc\ub294\ub370, \uc774\ub294 downsampling\uc744 \uc9c4\ud589\ud558\ub294 residual block \ub0b4\uc758 1x1 conv layer (shortcut layer) \uc758 FLOPs\uac00 \uc904\uc5b4\ub4e4\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4(layer \ubcc4: (384 -> 384) 1x1 conv -> (96 -> 96) 1x1 conv).\\n\\n\uc800\uc790\ub294 inverted bottleneck \uad6c\uc870\ub97c \uc801\uc6a9\ud588\uace0, \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 80.5% -> 80.6% (+0.1%, \ub204\uc801: +4.5%)\\n\\n## \ubcc0\ud6545: \ud070 \uc0ac\uc774\uc988\uc758 Kernel\uc744 \uc774\uc6a9\ud558\uae30\\n\\nVision Transformer\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 self-attention\uc740 non-local\ud55c \ud2b9\uc131\uc744 \uac00\uc9c0\uace0 \uc788\uc5b4, \uc0ac\uc2e4\uc0c1 receptive field\uac00 global\ud558\ub2e4\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc640\ub294 \ubc18\ub300\ub85c ConvNet\uc5d0\uc11c\ub294 GPU\uc758 \ud2b9\uc131\uc744 \uace0\ub824\ud558\uc5ec \ud6a8\uc728\uc801\uc778 \uc5f0\uc0b0\uc744 \uc704\ud574 VGG network\ub97c \uc2dc\uc791\uc73c\ub85c 3x3\uc758 \uc791\uc740 \uc0ac\uc774\uc988\uc758 kernel \uc0ac\uc774\uc988\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. Swim Transformer\ub294 self-attention\uc744 \uc0ac\uc6a9\ud558\ub294 \ub370\uc5d0 \uc788\uc5b4 local window\ub97c \uc801\uc6a9\ud558\ub294\ub370, \uc5ec\uae30\uc11c\ub294 kernel size\ub97c \ucd5c\uc18c 7x7 \ub85c \uc7a1\uc544\uc11c \uc0ac\uc6a9\ud588\uc5c8\uc2b5\ub2c8\ub2e4. \uc800\uc790\ub294 ConvNet\uc5d0\ub3c4 large kernel size\ub97c \uc801\uc6a9\ud574\ubcf4\uace0\uc790 \ud569\ub2c8\ub2e4.\\n\\n\uc6b0\uc120 kernel size\ub97c \ud0a4\uc6b0\uae30 \uc704\ud574\uc11c\ub294 depthwise conv. \uc758 \uc704\uce58\ub97c 1x1 conv. \uc55e\uc73c\ub85c \uac00\uc838\uc640\uc57c \ud569\ub2c8\ub2e4. \uc774\ub294 Transformer\uc5d0\uc11c MSA(Multi-head Self Attention)\uac00 MLP layer \uc55e\uc5d0 \uc788\ub294 \uad6c\uc870\uc640 \uc720\uc0ac\ud558\uac8c \uac00\uc838\uac00\uae30 \uc704\ud568\uc785\ub2c8\ub2e4.\\n\\n\uc774 \uc0c1\ud0dc\uc5d0\uc11c \uc800\uc790\ub294 kernel size\ub97c 3, 5, 7, 9, 11 \ub4f1\uc758 \ud06c\uae30\ub85c \ubc14\uafd4\uac00\uba74\uc11c \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud588\uc2b5\ub2c8\ub2e4. Kernel size\uac00 \ub2ec\ub77c\uc9c4\ub2e4\uace0 FLOPs\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc740 \ubbf8\ubbf8\ud569\ub2c8\ub2e4. \uc800\uc790\ub4e4\uc758 \uc2e4\ud5d8\uc744 \ud1b5\ud574 7x7\ubcf4\ub2e4 \ud070 kernel\uc5d0\uc11c\ub294 \uc131\ub2a5\uc774 \ub354 \uc99d\uac00\ud558\uc9c0 \uc54a\uace0 saturate \ub418\ub294 \uac83\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4.\\n\\n\uc774\ub97c \ud1b5\ud574 inverted bottleneck\uae4c\uc9c0 \uc801\uc6a9\ud588\uc744 \ub54c\uc640 \ub3d9\uc77c\ud55c accuracy\ub97c \ubcf4\uc774\uba74\uc11c\ub3c4 FLOPs\ub294 4.6G\uc5d0\uc11c 4.2G\ub85c \ub0ae\ucd9c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n## \ubcc0\ud6546: Activation, Normalization Layer \ubcc0\uacbd\ud558\uae30\\n\\n<img className={styles.figCenter} src={figBlockDesign} alt=\\"block-design\\" />\\n\\n\uac00\uc7a5 \uc6b0\uce21 ConvNeXt block\uc774 \uc800\uc790\uac00 \ucd5c\uc885\uc801\uc73c\ub85c \ub514\uc790\uc778\ud55c \ud615\ud0dc\uc785\ub2c8\ub2e4. \uc544\ub798 \ub0b4\uc6a9\uc744 \ubaa8\ub450 \ud655\uc778\ud558\uc2dc\uace0 \uc800 block\uc5d0 \ud574\ub2f9\ud558\ub294 \ub0b4\uc6a9\ub4e4\uc774 \uc798 \ubc18\uc601\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud574\ubcf4\uc138\uc694\ud83d\ude0a.\\n\\n### ReLU\ub97c GELU\ub85c \ubcc0\uacbd\\n\\nConvNet\uc5d0\uc11c\ub294 \uc544\uc9c1\ub3c4 ReLU\uac00 \ub450\ub8e8 \uc4f0\uc774\uace0, original Transformer \ub17c\ubb38\uc5d0\uc11c\ub3c4 ReLU\ub97c \uc0ac\uc6a9\ud558\uc9c0\ub9cc, \uadf8 \uc774\ud6c4 \ub4f1\uc7a5\ud55c NLP Transformer(BERT, GPT-2)\uc640 ViT\uc5d0\uc11c\ub294 GELU(Gaussian Error Linear Unit)\uc744 \ub9ce\uc774 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\\n\\n\uc774\ub97c \uc801\uc6a9\ud588\uc744 \ub54c, \ubcc4\ub3c4\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc740 \uc5c6\uc5c8\uc9c0\ub9cc, GELU\uac00 ConvNet\uc5d0\ub3c4 \uc801\uc6a9\ub420 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900 \ubcc0\ud654\uc810\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\n\\n### Activation Function\uc744 \uc801\uac8c \uc4f0\uae30\\n\\nTransformer block\uc5d0\uc11c\ub3c4 MLP block \ub0b4 activation 1\ubc88 \uc4f0\uc774\ub294 \uac83 \uc678\ub85c\ub294 \uc4f0\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\ub294 1x1 conv. \ub4a4\uc5d0\uae4c\uc9c0 activation\uc744 \ubd99\uc774\ub294 ConvNet\uc5d0\uc11c\uc758 \ud589\ud0dc\uc640\ub294 \ub9e4\uc6b0 \ub2e4\ub985\ub2c8\ub2e4. \uc800\uc790\ub294 Transformer block design\uc744 \ub530\ub77c, residual block \ub0b4\uc5d0 \ubaa8\ub4e0 GELU activation\uc744 \uc9c0\uc6b0\uace0, 1x1 layer \uc0ac\uc774\uc5d0 activation \ud558\ub098\ub9cc \ub0a8\uaca8\ub450\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\uc640 \uac19\uc740 \ubcc0\ud654\ub97c \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 80.6% -> 81.3% (+0.7%, \ub204\uc801: +5.2%)\\n\\n\uc774 \uc131\ub2a5\uc740 Swim-T\uc640 \ub3d9\uc77c\ud55c \uc131\ub2a5\uc785\ub2c8\ub2e4.\\n\\n### Normalization\ub3c4 \uc801\uac8c \uc4f0\uae30\\n\\nTransformer\uc5d0\uc11c\ub294 normalization layer\ub3c4 \uc801\uac8c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc800\uc790\ub294 \uc774\ub97c \ub530\ub77c 1x1 conv \uc55e\uc5d0 \ud558\ub098\uc758 BN(Batch Normalization)\ub9cc\uc744 \ub194\ub450\uace0, \ub098\uba38\uc9c0 normalization layer\ub97c \uc9c0\uc6e0\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 81.3% -> 81.4% (+0.1%, \ub204\uc801: +5.3%)\\n\\n\uc7ac\ubc0c\ub294 \uc810\uc740 \uc774\ub7ec\ud55c \ubcc0\ud654\ub97c \ud1b5\ud574 Transfomer\ubcf4\ub2e4\ub3c4 \uc624\ud788\ub824 normalization layer\uac00 \uc801\uc5b4\uc84c\uc2b5\ub2c8\ub2e4. \uc774\ub294 Transformer layer\ub294 block \ucd08\uc785\uc5d0 LN((Layer Normalization) \uc744 \ucde8\ud558\ub294\ub370, \uc800\uc790\ub294 BN\uc744 \ubd99\uc778\ub2e4\uace0 \uc131\ub2a5\uc774 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc544\uc11c \uadf8\ub0e5 \uc9c0\uc6e0\ub2e4\uace0 \ud569\ub2c8\ub2e4.\\n\\n### Batch Normalization \ub300\uc2e0 Layer Normalization\\n\\n\ub9ce\uc740 vision task\uc5d0\uc11c BN\uc774 \uc0ac\ub791\ubc1b\uace0 \uc788\ub294 \uac83\uc740 \uc0ac\uc2e4\uc778\ub370\uc694. \ud558\uc9c0\ub9cc, Transformer\uc5d0\uc11c\ub294 \uc774\ubbf8 LN\uc744 \uc4f0\uba74\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778 \uacbd\uc6b0\uac00 \ub9ce\uc558\uc2b5\ub2c8\ub2e4. \uc800\uc790\ub294 \uc774 \uc0c1\ud0dc\uc5d0\uc11c LN\uc744 \uc801\uc6a9\ud574\uc11c\ub3c4 \ud559\uc2b5\ud558\ub294 \ub370 \ubb34\ub9ac\uac00 \uc5c6\uc74c\uc744 \ud655\uc778\ud588\uace0 \uc624\ud788\ub824 \uc544\ub798\uc640 \uac19\uc740 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 81.4% -> 81.5% (+0.1%, \ub204\uc801: +5.4%)\\n\\n### Separate Downsampling Layer \uc0ac\uc6a9\\n\\n\uc774 \ub0b4\uc6a9\uc740 block \ub0b4\ubd80\uc5d0 \ubc18\uc601\ub41c \ub0b4\uc6a9\uc774 \uc544\ub2c8\ub77c, stage \ub118\uc5b4\uac00\uba74\uc11c downsample\uc744 \ud560 \ub54c\uc5d0 \ud574\ub2f9\ud558\ub294 \ub0b4\uc6a9\uc785\ub2c8\ub2e4. \uc704 diagram\uc5d0\ub294 \ub4dc\ub7ec\ub098\uc9c0 \uc54a\ub294 \ubd80\ubd84\uc774\ub2c8 \ucc38\uace0\ud574\uc8fc\uc138\uc694~    \\nSwim Transformer\uac00 patch merging\uc744 \ud560 \ub54c 2x2 neighborhood patch\ub4e4\uc758 channel\uc744 concat\ud558\uc5ec(4C) \uc774\uc5d0 \ub300\ud574 2C \uc758 channel size\uac00 \ub418\ub3c4\ub85d \ub0b4\ubc49\uc2b5\ub2c8\ub2e4. \uc774\ub97c Conv. \ub85c \ud45c\ud604\ud558\uba74 channel size\uac00 2\ubc30\uac00 \ub418\uace0, kernel size\ub294 2x2, stride 2\uc778 \uac83\uc785\ub2c8\ub2e4. \uc800\uc790\ub294 ResNet\uc5d0\uc11c kernel size 3x3, stride 2, padding 1 \ub85c \ub9c8\uce58 \uacf5\uc2dd\ucc98\ub7fc \uc4f0\uc774\ub358 downsample\ud558\ub294 conv. layer \ub300\uc2e0\uc5d0 Swim Transformer\uc5d0\uc11c\uc758 downsample \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uadf8\ub0e5 \uc774\ub97c \ub300\uce58\uc2dc\ud0a4\uba74 \ud559\uc2b5\uc774 diverge \ub418\ub294\ub370, block-level\uc5d0\uc11c spatial resolution\uc774 \ubcc0\uacbd\ub418\ub294 \uc9c0\uc810\uc5d0 LN\uc744 \uc801\ub2f9\ud788 \ub123\uc5b4\uc8fc\uba74 \uc774\uc5d0 \ub300\ud574 \ud559\uc2b5\uc774 stabilize \ub418\ub294 \uac83\uc744 \ucc3e\uc544\ub0c5\ub2c8\ub2e4. \uc774\ub294 Swim Transformer\ub3c4 \uc801\uc6a9\ub41c \ubc29\uc2dd\uc785\ub2c8\ub2e4.\\n\\n\uadf8\ub9ac\uace0 \uc774\ub97c \ud1b5\ud574 \ucd5c\uc885\uc801\uc73c\ub85c ConvNeXt\ub294 \uc544\ub798\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4.\\n\\n- ResNet-50: 81.5% -> 82.0% (+0.5%, \ub204\uc801: +5.9%)\\n\\n## Training Details\\n\\n\uc800\uc790\ub294 \ubcf8 \ubaa8\ub378\uc744 \uc5b4\ub5bb\uac8c \ud559\uc2b5\ud588\ub294 \uc9c0 Hyperparameter\uc640 \ud559\uc2b5 \uc138\ud305\ub3c4 \ud568\uaed8 \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc2b5\ub2c8\ub2e4.  \\nAugmentation, optimizer \ub4f1 \uc5ec\ub7ec \ubd80\ubd84\uc5d0\uc11c \ub4f1\uc7a5\ud55c \ucd5c\uc2e0\uc758 \ud559\uc2b5 \ubc29\ubc95\ub860\ub4e4\uc744 \ud55c \uacf3\uc5d0 \ubaa8\uc544\ub454 \ub290\ub08c\uc785\ub2c8\ub2e4.\\n\\n### (Pre-)Training Settings\\n\\n<img className={styles.figCenter} src={figPretrain} alt=\\"pretrain\\" />\\n\\n### Finetuning Settings\\n\\n<img className={styles.figCenter} src={figFinetune} alt=\\"finetune\\" />"},{"id":"acon","metadata":{"permalink":"/papers/acon","source":"@site/papers/2021-07-19-paper-review-acon/index.mdx","title":"Activate or Not: Learning Customized Activation","description":"Swish\uc640 ReLU\uc640\uc758 \uad00\uacc4\ub97c \uc124\uba85\ud558\uace0, \ud559\uc2b5\uc774 \uac00\ub2a5\ud55c \ud65c\uc131\ud568\uc218\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4.","date":"2021-07-19T00:00:00.000Z","formattedDate":"July 19, 2021","tags":[{"label":"paper-review","permalink":"/papers/tags/paper-review"},{"label":"activation","permalink":"/papers/tags/activation"}],"readingTime":14.655,"truncated":true,"authors":[{"name":"Hyoung-Kyu Song","title":"AI Researcher (Vision)","url":"https://github.com/deepkyu","imageURL":"https://github.com/deepkyu.png","key":"hkyu"}],"frontMatter":{"slug":"acon","title":"Activate or Not: Learning Customized Activation","description":"Swish\uc640 ReLU\uc640\uc758 \uad00\uacc4\ub97c \uc124\uba85\ud558\uace0, \ud559\uc2b5\uc774 \uac00\ub2a5\ud55c \ud65c\uc131\ud568\uc218\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4.","image":"img/default.png","authors":["hkyu"],"tags":["paper-review","activation"]},"prevItem":{"title":"A ConvNet for the 2020s (ConvNeXt)","permalink":"/papers/convnext"},"nextItem":{"title":"Anycost GANs for Interactive Image Synthesis and Editing","permalink":"/papers/anycost"}},"content":"import clsx from \'clsx\';\\nimport styles from \'../blog.module.css\';\\n\\nimport figSwish from \'./image/figure1_swish.png\';\\nimport figAcon from \'./image/figure4_acon.png\';\\nimport figFamily from \'./image/figure5_maxout_family_acon_family.png\';\\nimport figExample from \'./image/figure6_acon_example.png\';\\nimport figProperty from \'./image/figure7_acon_property.png\';\\nimport figDistribution from \'./image/figure8_meta_acon_distribution.png\';\\nimport figResultFour from \'./image/figure12_result4.png\';\\nimport figResultFive from \'./image/figure13_result5.png\';\\n\\n[![arXiv](https://img.shields.io/badge/arXiv-2009.04759-brightgreen.svg?style=flat-square)](https://arxiv.org/abs/2009.04759)\\n[![githubio](https://img.shields.io/static/v1?message=Official%20Repo&logo=Github&labelColor=grey&color=blue&logoColor=white&label=%20&style=flat-square)](https://github.com/nmaac/acon)\\n\\n> Ma, Ningning, et al. \\"Activate or Not: Learning Customized Activation.\\"  \\n> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\\n\\n<br/>\\n\\n:::info\\n\\n\uc5c5\ubb34 \uc911\uc5d0 \uc9c4\ud589\ub41c \ub17c\ubb38 \ub9ac\ubdf0\ub85c [\ub9c8\uc778\uc988\ub7a9 Brain\ud300 Tech Blog](https://mindslab-ai.github.io)\uc5d0\uc11c\ub3c4 \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\n\\n:::\\n\\n## Contribution\\n\\n- Activation function\ub4e4\uc5d0 \ub300\ud574 \uae30\uc874 Maxout family\uc5d0 \ud574\ub2f9\ud558\ub294 \uc77c\ubc18\ud654\ub97c \ub118\uc5b4 **ACON Family**\ub77c\ub294 \uac1c\ub150\uc73c\ub85c \ud655\uc7a5\ud558\uc5ec \uc77c\ubc18\ud654\ub97c \ud569\ub2c8\ub2e4.\\n- \uc774\ub97c \ud1b5\ud574 ACON Family\uc5d0\uc11c \uac01 activation\uc744 \uacb0\uc815 \uc9d3\ub294 parameter \uc790\uccb4\ub97c learnable\ud558\uac8c \ud558\uc5ec **acon** \uc774\ub77c\ub294 activation\uc744 \uc0c8\ub86d\uac8c \uc81c\uc2dc\ud569\ub2c8\ub2e4.\\n- \uae30\uc874 Swish \ub294 NAS\ub85c \ucc3e\uc740 activation\uc73c\ub85c\uc11c, \ub354 \uc88b\ub2e4\ub294 \uac83\ub9cc \uc54c \ubfd0, \uc65c \uc88b\uc740\uc9c0\ub97c \ubab0\ub790\ub294\ub370, **ACON Family**\uc5d0 \ub300\uc751\ud558\uc5ec \ubd24\uc744 \ub54c, \uc774\ub97c \uc5b4\ub290\uc815\ub3c4 \uc124\uba85\ud560 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4.\\n\\n\x3c!--truncate--\x3e\\n\\n\\n## \uba3c\uc800 \uc54c\uba74 \uc88b\uc740 \uac83\ub4e4\\n\\n### Swish Activation Function [<sup>[1]</sup>](#r1)\\n\\n$$\\n\\\\operatorname{swish}(x):=x \\\\times \\\\sigma(\\\\beta x)=\\\\frac{x}{1+e^{-\\\\beta x}}\\n$$\\n\\n<img className={styles.figCenter} src={figSwish} alt=\\"figure1_swish\\" />\\n\\n- Linear Function\uacfc ReLU \uc0ac\uc774\uc5d0\uc11c\uc758 non-linearly interpolated activation\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\\n    - $\u03b2 = 0$ \uc77c \uacbd\uc6b0, Linear function $f(x) = x/2$ \ucc98\ub7fc \uc791\uc6a9\ud558\uac8c \ub429\ub2c8\ub2e4.\\n    - \ubc18\ub300\ub85c $\u03b2 \u2192 \u221e$\uc77c \uacbd\uc6b0, Sigmoid\uc5d0 \ud574\ub2f9\ud558\ub294 \ubd80\ubd84\uc774 0-1 activation\ucc98\ub7fc \uc791\uc6a9\ud558\uac8c \ub418\uc5b4, Swish\uac00 ReLU\ucc98\ub7fc \uc791\uc6a9\ud558\uac8c \ub429\ub2c8\ub2e4.\\n    - $\u03b2 = 1$\uc77c \uacbd\uc6b0, \uac15\ud654\ud559\uc2b5\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 Sigmoid-weighted Linear Unit (SiL) function\ucc98\ub7fc \uc791\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\n    - $\u03b2$\ub294 \uc704\uc5d0\uc11c \ubcf4\uc2e0 \uac83\ucc98\ub7fc \uc5b4\ub5a4 \uc0c1\uc218\uc77c \uc218\ub3c4 \uc788\uace0, \ubaa8\ub378\uc5d0 \ub530\ub77c\uc11c\ub294 \ud6c8\ub828 \uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130\uac00 \ub420 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\\n- \ube0c\ub808\uc778\ud300 AI Scientist\ubd84\ub4e4\uc774 \uc790\uc8fc \uc0ac\uc6a9\ud558\uc2dc\ub294 Activation Function\uc774\uae30\ub3c4 \ud558\uc8e0 \ud83d\ude42\\n- Generative Model\uc5d0\uc11c\ub3c4 ReLU \ub300\uc2e0 \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc774 \uc788\uc2b5\ub2c8\ub2e4.\\n- \ucd5c\uadfc\uc5d0\ub294 Implicit Representation Network \uc0c1\uc5d0\uc11c\ub3c4 Swish\uac00 \ub2e4\uc2dc\uae08 \uc8fc\ubaa9\uc744 \ubc1b\uace0 \uc788\uc2b5\ub2c8\ub2e4.\\n    - SIREN\uc5d0\uc11c \uc5b8\uae09\ud558\ub294 periodic function activation (Sine \ud568\uc218 \ub4f1) \ubcf4\ub2e4 Swish\uac00 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 Task\uac00 \uc788\uc2b5\ub2c8\ub2e4.\\n\\n### Sigmoid\\n\\n$$\\n\\\\sigma(x)=\\\\frac{1}{1+e^{-x}}\\n$$\\n\\n- \uc5ec\uae30\uc11c\ub294 Activation\uc73c\ub85c \uc2dc\uc0ac\ud558\uae30\ubcf4\ub2e4\ub294 \uc218\uc2dd \ud45c\ud604 \uc2dc\uc5d0 sigmoid\ub85c \ubb36\uc5b4 \ud45c\ud604\ud558\uae30 \uc704\ud574 \ud655\uc778\ud558\uace0 \ub118\uc5b4\uac00\uc57c \ud569\ub2c8\ub2e4.\\n- Swish\uac00 \uacb0\uad6d **input \uac12\uc5d0 sigmoid\ud55c \uac83\uacfc input \uac12\uc758 \uacf1\uc73c\ub85c \ud45c\ud604\ub41c\ub2e4**(\u03b2 \ub97c \uacf1\ud558\uae30\ub294 \ud558\uaca0\uc9c0\ub9cc)\ub294 \uac83\ub3c4 \ub2e4\uc2dc \ud55c\ubc88 \ub9ac\ub9c8\uc778\ub4dc\ud558\uace0 \ub118\uc5b4\uac11\uc2dc\ub2e4 \ud83d\ude0e\\n\\n### Maxout Family\\n\\n- ReLU\uc640 \uac19\uc740 Activation Function\uc758 \ucd9c\ubc1c\uc810\uc5d0 \ud574\ub2f9\ud558\ub294 \uac1c\ub150 \uc911 \ud558\ub098\uc785\ub2c8\ub2e4.\\n- Goodfellow\uc640 Bengio\uc758 \ub17c\ubb38[<sup>[2]</sup>](#r2) \uc73c\ub85c, Maximum\uc744 \uc120\ud0dd\ud558\ub294 \uac83\uc73c\ub85c\ub3c4 \uc784\uc758\uc758 Convex Function\uc5d0 \ub300\ud574 \ub450\ub8e8 \uadfc\uc0ac\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud569\ub2c8\ub2e4.\\n\\n## Main Idea\\n\\n### ACON(**Ac**tivationOrNot) Activation Function\\n\\n$$\\n\\\\operatorname{ACON-C}(x) := \\\\left(p_{1}-p_{2}\\\\right) x \\\\cdot \\\\sigma\\\\left(\\\\beta\\\\left(p_{1}-p_{2}\\\\right) x\\\\right)+p_{2} x\\n$$\\n\\n<img className={styles.figCenter} src={figAcon} alt=\\"figure4_acon\\" />\\n\\n*ACON Activation\uc744 \uc0ac\uc6a9\ud558\uc600\uc744 \ub54c, \ud2b9\uc815 Layer\uc758 Activation\uc774 Linear \ud558\uac8c pass\uc218\ub3c4, Non-linear Activation\uc73c\ub85c \ud65c\uc131\ub420 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4*\\n\\n\uc800\uc790\ub294 ACON(\ub354 \ub098\uc544\uac00\uc11c Meta-ACON)\uc774\ub77c\uace0 \ud558\ub294 Activation Function\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4. ACON activation\uc740 trainable\ud55c activation\uc73c\ub85c Neuron\uc744 Activation\ud560 \uc9c0 \uc548 \ud560\uc9c0\ub97c \uac01 Layer\uc758 \ud2b9\uc131\uc5d0 \ub9de\uac8c \uacb0\uc815\ud569\ub2c8\ub2e4.\\n\\n\\n\\n### \uc5b4\ub5bb\uac8c \ud574\uc11c ACON \uc2dd\uc744 \ub3c4\ucd9c\ud560 \uc218 \uc788\uac8c \ub418\uc5c8\uc744\uae4c\uc694?\\n\\n\uba3c\uc800 Maximum Function $max(x1, ..., xn)$ \uc5d0 \ub300\ud574 smooth\ub41c \ubc84\uc804\uc744 \ubcf4\uc544\uc57c \ud569\ub2c8\ub2e4. Maximum\uc744 \uad6c\ud55c\ub2e4\ub294 \uac83\uc740 \uc77c\ubc18\uc801\uc73c\ub85c differentiable\ud558\uc9c0 \uc54a\uc9c0\ub9cc, \uc774\ub97c smooth\ud55c \ud568\uc218\ub294 differentiable\ud558\uac8c \ub429\ub2c8\ub2e4.\\n\\n\ubcf4\ud1b5 \uc544\ub798\uc758 \uc2dd\ucc98\ub7fc \ud45c\ud604\ud569\ub2c8\ub2e4.\\n\\n$$\\nS_{\\\\beta}\\\\left(x_{1}, \\\\ldots, x_{n}\\\\right)=\\\\frac{\\\\sum_{i=1}^{n} x_{i} e^{\\\\beta x_{i}}}{\\\\sum_{i=1}^{n} e^{\\\\beta x_{i}}}\\n$$\\n\\n\uc774 \ub54c, $\u03b2$ \ub294 switching factor\ub85c\uc11c\\n\\n- $\u03b2 \u2192 \u221e$\uc77c \ub54c, \uc8fc\uc5b4\uc9c4 \ud568\uc218\ub294 Maximum Function \uc758 \uc5ed\ud560\uc744 \ud558\uac8c \ub429\ub2c8\ub2e4.\\n- $\u03b2 \u2192 0$\uc77c \ub54c, \uc8fc\uc5b4\uc9c4 \ud568\uc218\ub294 \uc0b0\uc220\ud3c9\uade0(Arithmetic Mean, \uc6b0\ub9ac\uac00 \uc77c\ubc18\uc801\uc73c\ub85c \uc544\ub294 \ud3c9\uade0)\ucc98\ub7fc \uc791\ub3d9\ud569\ub2c8\ub2e4.\\n\\n\uc77c\ubc18\uc801\uc73c\ub85c Neural Network\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ud558\ub294 Activation Function\ub4e4\uc740 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c Maxout\uc5d0 \uc900\ud558\ub294 \uac83\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\n\\n$$\\nmax( \u03b7a(x), \u03b7b(x))\\n$$\\n\\n\uc608\ub97c \ub4e4\uc5b4, ReLU\ub294 $\u03b7a(x)=x$, $\u03b7b(x)=0$\uc778 \uac83\uc73c\ub85c \uc0dd\uac01\ud558\uba74, \uc774 \uc5ed\uc2dc Maxout Family\uc5d0 \uc18d\ud55c\ub2e4\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.  \\nLeaky ReLU, FReLU \ub4f1\ub3c4 \uc774\ub7ec\ud55c \ubc29\uc2dd\uc73c\ub85c \uc811\uadfc\ud574\ubcf4\uba74 \ubaa8\ub450 Maxout Family\uc5d0 \uc18d\ud558\uac8c \ub429\ub2c8\ub2e4.\\n\\n\ubcf8 \ub17c\ubb38\uc5d0\uc11c\uc758 \ubaa9\ud45c\ub294 Maximum Function\uacfc \uc704 Maxout Family\ub97c \ud568\uaed8 \uc0ac\uc6a9\ud558\uc5ec, Maxout Family \uac01\uac01\uc5d0 \uc0c1\uc751\ud558\ub294 activation function\ub4e4\uc744 smooth\ud55c \ud568\uc218\ub85c \uadfc\uc0ac\ud574\ubcf4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc704\uc5d0\uc11c Smooth\ub41c Maximum Function\uc744 \uc791\uc131\ud560 \ub54c, \uc785\ub825 \uac12\uc758 \uac1c\uc218\ub97c 2\uac1c\ub85c\ub9cc \ud55c\uc815\ud574\uc11c \uc2dd\uc744 \uc804\uac1c\ud558\uba74 \ub531\uc774\uaca0\ub124\uc694!\\n\\n$$\\n\\\\begin{array}{l}S_{\\\\beta}\\\\left(\\\\eta_{a}(x), \\\\eta_{b}(x)\\\\right) \\\\\\\\=\\\\eta_{a}(x) \\\\cdot \\\\frac{e^{\\\\beta \\\\eta_{a}(x)}}{e^{\\\\beta \\\\eta_{a}(x)}+e^{\\\\beta \\\\eta_{b}(x)}}+\\\\eta_{b}(x) \\\\cdot \\\\frac{e^{\\\\beta \\\\eta_{b}(x)}}{e^{\\\\beta \\\\eta_{a}(x)}+e^{\\\\beta \\\\eta_{b}(x)}} \\\\\\\\=\\\\eta_{a}(x) \\\\cdot \\\\frac{1}{1+e^{-\\\\beta\\\\left(\\\\eta_{a}(x)-\\\\eta_{b}(x)\\\\right)}}+\\\\eta_{b}(x) \\\\cdot \\\\frac{1}{1+e^{-\\\\beta\\\\left(\\\\eta_{b}(x)-\\\\eta_{a}(x)\\\\right)}} \\\\\\\\=\\\\eta_{a}(x) \\\\cdot \\\\sigma\\\\left[\\\\beta\\\\left(\\\\eta_{a}(x)-\\\\eta_{b}(x)\\\\right)\\\\right]+\\\\eta_{b}(x) \\\\cdot \\\\sigma\\\\left[\\\\beta\\\\left(\\\\eta_{b}(x)-\\\\eta_{a}(x)\\\\right)\\\\right] \\\\\\\\=\\\\left(\\\\eta_{a}(x)-\\\\eta_{b}(x)\\\\right) \\\\cdot \\\\sigma\\\\left[\\\\beta\\\\left(\\\\eta_{a}(x)-\\\\eta_{b}(x)\\\\right)\\\\right]+\\\\eta_{b}(x)\\\\end{array}\\n$$\\n\\n\uc989, Smooth\ub41c Maximum Function\uc5d0 \ub300\uc785\ud574\uc11c \uc804\uac1c\ud574\ubcf4\uba74\\n\\n$$\\n\\\\begin{array}{l}S_{\\\\beta}\\\\left(\\\\eta_{a}(x), \\\\eta_{b}(x)\\\\right) =\\\\left(\\\\eta_{a}(x)-\\\\eta_{b}(x)\\\\right) \\\\cdot \\\\sigma\\\\left[\\\\beta\\\\left(\\\\eta_{a}(x)-\\\\eta_{b}(x)\\\\right)\\\\right]+\\\\eta_{b}(x)\\\\end{array}\\n$$\\n\\n\uc774 \ub420 \uac83\uc785\ub2c8\ub2e4!\\n\\n### ACON \uc548\uc5d0 Swish \uc788\ub2e4 \ud83d\ude09\\n\\n\uc790, \uadf8\ub7fc \uc544\uae4c \uc5b8\uae09\ud55c Maxout Family\uc5d0 \uc900\ud558\uc5ec \uc5ec\ub7ec Activation\uc744 \ud45c\ud604\ud560 \uc218 \uc788\uc5c8\ub2e4\uba74, \uac01\uac01\uc744 Smooth\ub41c Maximum Function\uc5d0 \ud574\ub2f9\ud558\ub3c4\ub85d \uc804\uac1c\ub97c \ud574\ubcfc\uae4c\uc694?\\n\\n<img className={styles.figCenter} src={figFamily} alt=\\"figure5_maxout_family_acon_family\\" />\\n\\n\\nReLU\uc758 smooth\ub418\ub294 \ubc84\uc804\uc774 Swish\ub77c\ub294 \uac74 \uc9c1\uad00\uc73c\ub85c\ub3c4 \ub9ce\uc774\ub4e4 \uc774\ud574\ud558\uace0 \uc788\uc5c8\ub294\ub370\uc694. \uc774 \uc2dd\uc5d0\uc11c \ubcf4\ub4ef\uc774 Smooth\ub41c Maximum Function\uc5d0 \ub300\uc785\ud574\uc11c \uc804\uac1c\ud574\ubcf4\uba74, \ubc14\ub85c Swish \uc2dd\uc774 \ub098\uc624\uac8c \ub429\ub2c8\ub2e4. \uc800\uc790\ub294 \uc774\ub97c \ud1b5\ud574 Swish\uac00 ReLU\uc758 Smooth Approximation\uc784\uc744 \ud45c\ud604\ud560 \uc218 \uc788\uac8c \ub41c\ub2e4\uace0 \ub9d0\ud569\ub2c8\ub2e4.\\n\\n\ub610\ud55c, Leaky ReLU\uc758 \uc0c1\uc704 \ud638\ud658\uc774\uae30\ub3c4 \ud55c PReLU(Parametric ReLU, \uc74c\uc218 \ubd80\ubd84\uc758 \uae30\uc6b8\uae30 \uac12\uc774 learnable\ud568)\ub3c4 \uc0b4\ud3b4\ubcf4\uba74, \uc5ed\uc2dc Smooth\ub418\ub294 \ud568\uc218\ub85c \ub300\uc751\ud558\ub294 \uac83\uc744 \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc544 \uc774 \ub54c PReLU\uc5d0 \ub300\uc751\ud558\ub824\uba74, p < 1 \uc778 \uac78\ub85c \ud55c\uc815\ud574\uc11c \uc0dd\uac01\ud574\ubd10\uc694 \uc6b0\ub9ac \ud83d\ude42)\\n\\n\uadf8\ub9ac\uace0 \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uac01 \uc120\ud615 \ud568\uc218\uc758 \uac00\uc911\uce58(Cartesian \uc88c\ud45c\uacc4 \uc0c1\uc740 \uae30\uc6b8\uae30\uaca0\uc8e0?)\uac00 p1, p2\ub85c \ud45c\ud604\ud558\uba74 \uac00\uc7a5 \uc77c\ubc18\ud654\ub41c \ud45c\ud604\uc77c\ud150\ub370\uc694 (p1 \u2260 p2). \uc5ec\uae30\uc5d0 \uac01\uac01 Maxout Family, ACON Family\ub97c \ub300\uc751\ud574\ubcf4\uba74 \uc77c\ubc18\ud654\ub41c \uc2dd\uc774 \ub098\uc635\ub2c8\ub2e4. \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c\\n\\n$$\\n\\\\operatorname{ACON-C}(x):=\\\\left(p_{1}-p_{2}\\\\right) x \\\\cdot \\\\sigma\\\\left(\\\\beta\\\\left(p_{1}-p_{2}\\\\right) x\\\\right)+p_{2} x\\n$$\\n\\n\uc774 \uc774\ub807\uac8c \uc720\ub3c4\ub418\uac8c \ub418\ub294 \uac83\uc774\uc8e0!\\n\\n\uc0ac\uc2e4 Maxout Family\uc5d0\uc11c \ube44\uad50\ud558\uac8c \ub418\ub294 \ub450 \ud568\uc218\ub294 \uc704\uc5d0\uc11c\ucc98\ub7fc \ub2e8\uc21c\ud558\uc9c0 \uc54a\uc744 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uac01\uac01\uc774 \ubcf5\uc7a1\ud574\uc9c8\uc218\ub85d \ub354 \ub9ce\uc740 \ud568\uc218\ub4e4\uc744 \ud45c\ud604\ud560 \uc218 \uc788\uac8c \ub418\uc8e0. \ub2e4\ub9cc, \uc800\uc790\ub294 \uc774 Maxout Family\ub97c ACON Family\ub85c \ubc14\uafe8\uc744 \ub54c(\uc989, Smooth Maximum Function\uc73c\ub85c \uadfc\uc0ac\ud588\uc744 \ub54c)\uc758 \ud6a8\uacfc\ub97c \ubcf4\ub294 \ub370\uc5d0 \uc5f0\uad6c\ub97c \uc9d1\uc911\ud588\ub2e4\uace0 \ud574\uc694. \ud5a5\ud6c4 \uc5f0\uad6c\uc5d0\uc11c \ub354 \uc804\uccb4\uc801\uc778 Scope\uc5d0\uc11c\uc758 \ube44\uad50\uac00 \uc788\uae30\ub97c \uae30\ub300\ud574\ubd05\ub2c8\ub2e4!\\n\\n### ACON\uc758 \ud2b9\uc131\\n\\nACON\uc5d0 \ud2b9\uc815 \uac12\uc744 \ub300\uc785\ud574\uc11c \ud55c\ubc88 \uc0b4\ud3b4\ubcfc\uae4c\uc694?\\n\\n<img className={styles.figCenter} src={figExample} alt=\\"figure6_acon_example\\" />\\n\\np1=1.2, p2=-0.8\uc77c \ub54c ACON-C\uc5d0 \ub300\uc751\ud558\ub294 \uc2dd\uc744 \uc5ec\ub7ec \u03b2\uac12\uc5d0 \ub300\ud574 \ud45c\ud604\ud55c graph\uc785\ub2c8\ub2e4.\\n\\n- \u03b2\uac00 \ud074 \ub54c\ub294, maximum function\ucc98\ub7fc \ubc18\uc751\ud558\uc5ec \ube44\uc120\ud615\uc801\uc778 \ud2b9\uc131\uc744 \uac16\uac8c \ub418\uace0\uc694.\\n- \u03b2\uac00 0\uc5d0 \uac00\uae4c\uc6b8 \ub54c\ub294 mean function\uc5d0 \uadfc\uc0ac\ub418\uc5b4 \uc120\ud615\uc801\uc778 \ud2b9\uc131\uc744 \uac16\ub124\uc694.\\n\\n<img className={styles.figCenter} src={figProperty} alt=\\"figure7_acon_property\\" />\\n\\nACON Activation\uacfc \uc774\uc5d0 \ub300\ud55c \ub3c4\ud568\uc218(derivative)\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc\uc785\ub2c8\ub2e4.\\n\\n- \uc67c\ucabd: \u03b2\uac00 fixed \ub418\uc5b4 \uc788\uc744 \ub54c, p1, p2 \uacc4\uc218\uc5d0 \ub530\ub77c \uc5b4\ub5bb\uac8c Activation function\uc774 \ub2ec\ub77c\uc9c0\ub294 \uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\\n- \uac00\uc6b4\ub370: \u03b2 \uac12\uc774 \ub2ec\ub77c\uc9d0\uc5d0 \ub530\ub77c ACON\uc758 \ub3c4\ud568\uc218\uac00 \ubcc0\ud654\ud558\uac8c \ub418\uace0 \uc774\ub97c \ud1b5\ud574 \u03b2\uc758 \uc5ed\ud560\uc744 \uc9d0\uc791\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\n- \uc624\ub978\ucabd: \u03b2\uac00 fixed \ub418\uc5b4 \uc788\uc744 \ub54c, p1, p2 \uacc4\uc218\uc5d0 \ub530\ub77c ACON\uc758 \ub3c4\ud568\uc218\uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294 \uc9c0\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\\n\\nACON\uc758 \ub3c4\ud568\uc218\ub97c \ubcf4\uba74\uc11c \uc544\ub798\uc640 \uac19\uc740 \uc0ac\uc2e4\uc744 \uc54c \uc218 \uc788\uc5b4\uc694.\\n\\n- p1, p2\ub294 \uac01\uac01 Upper/Lower Bound\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc744 \uacb0\uc815\ud558\uac8c \ub429\ub2c8\ub2e4.\\n- \u03b2 \uac12\uc740 \ub3c4\ud568\uc218 \uc0c1\uc5d0\uc11c p1, p2\uc5d0 \uc758\ud574 \uacb0\uc815\ub41c Upper/Lower Bound\uc5d0 \uc5bc\ub9c8\ub098 \ube60\ub974\uac8c \uadfc\uc0ac\ub418\ub294 \uc9c0\ub97c \uacb0\uc815\ud558\uac8c \ub429\ub2c8\ub2e4.\\n\\nSwish\uc5d0\uc11c\ub294 Hyperparameter \u03b2\ub9cc\uc774 Upper/Lower Bound\uc5d0 \uc5bc\ub9c8\ub098 \ube68\ub9ac \uadfc\uc0ac\ub418\ub294 \uc9c0\ub97c \uacb0\uc815\ud558\uac8c \ub418\ub294\ub370\uc694. ACON\uc5d0\uc11c\ub294 p1, p2\uac00 \uc774 Bound \uac12\uc744 \uacb0\uc815\ud558\uac8c \ub418\uace0, \uc774 \uc5ed\uc2dc learnable\ud574\uc9c8 \uc218 \uc788\ub2e4\ub294 \ud2b9\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub807\uac8c boundary\uac00 learnable\ud558\ub2e4\ub294 \uac83\uc740 optimization\uc744 \uc27d\uac8c \ud558\ub294 \ub370\uc5d0 \ud544\uc218\uc801\uc778 \ud2b9\uc131\uc774\uace0, \uc800\uc790\ub294 \uc774 \uc7a5\uc810\uc744 \uc2e4\ud5d8 \uacb0\uacfc\ub97c \ud1b5\ud574 \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc2b5\ub2c8\ub2e4.\\n\\n### \ud559\uc2b5\uc5d0 \ubaa8\ub450 \ub9e1\uaca8\ubc84\ub9ac\uc790! Meta-ACON\\n\\nMeta-ACON\uc740 \u03b2 \uc790\uccb4\ub97c Learnable\ud55c parameter\ub85c \ub194\ub450\ub294 \uac83\uc5d0\uc11c \ub354 \ub098\uc544\uac00, Layer\uc5d0 \uc785\ub825\ub418\ub294 feature map\uc73c\ub85c\ubd80\ud130 FC Layers\ub97c \uac70\uccd0 estimation \ub418\ub3c4\ub85d \ub9cc\ub4e0 \uac83\uc785\ub2c8\ub2e4.\\n\\n<img className={styles.figCenter} src={figDistribution} alt=\\"figure8_meta_acon_distribution\\" />\\n\\nACON\uacfc meta-ACON\uc744 \ube44\uad50\ud55c \ub3c4\uc2dd\uc785\ub2c8\ub2e4. ResNet50\uc758 \ub9c8\uc9c0\ub9c9 BottleNeck Layer\uc5d0\uc11c\uc758 activation\uc744 \ube44\uad50\ud55c \uac83\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c 7\uac1c\uc758 sample\uc744 \uc784\uc758\ub85c \ucd94\ucd9c\ud574\ubd24\uc2b5\ub2c8\ub2e4.\\n\\n- ACON\uc5d0\uc11c \ucd94\ucd9c\ud560 \uacbd\uc6b0, \ud30c\ub780 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc5d0 \ud574\ub2f9\ud558\ub294\ub370\uc694. 7\uac1c\uc758 sample\uc774 \ub3d9\uc77c\ud55c \u03b2 distribution\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\\n- Meta-ACON\uc5d0\uc11c\ub294 7\uac1c\uc758 sample\uc774 \uc11c\ub85c \ub2e4\ub978 \u03b2 distribution\uc744 \ubcf4\uc5ec\uc8fc\uac8c \ub429\ub2c8\ub2e4. \uc5ec\uae30\uc11c \u03b2 \uac12\uc774 \uc791\uc744\uc218\ub85d, \uc120\ud615\uc801\uc73c\ub85c(linear) \ubc18\uc751\ud558\ub294 \uac83\uc774\uace0, \u03b2 \uac12\uc774 \ud074 \uc218\ub85d \ube44\uc120\ud615\uc801(non-linear)\uc73c\ub85c \ubc18\uc751\ud558\uace0 \uc788\ub294 \uac83\uc785\ub2c8\ub2e4.\\n\\nCode Snippet\uc73c\ub85c \ubcf4\uba74 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4. \ubcf8 Snippet\uc740 \uc800\uc790\uc758 [official github](https://github.com/nmaac/acon)\uc5d0\uc11c \ubc1c\ucdcc\ud588\uc73c\uba70, \ud574\ub2f9 Repository\uc5d0\uc11c \uc790\uc138\ud55c \ucf54\ub4dc\ub97c \ud655\uc778\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\\n\\n<script src=\\"https://gist.github.com/deepkyu/1616637a06e1b00534a7557c35ad2209.js\\"><\/script>\\n<script src=\\"https://gist.github.com/deepkyu/77b2e5acd98969fdb21ea22198954ad5.js\\"><\/script>\\n\\n### \uacb0\uacfc\\n\\n| ImageNet Classification Result                      | Accuracy Improvements                                 |\\n| --------------------------------------------------- | ----------------------------------------------------- |\\n| ![figure9_result1.png](./image/figure9_result1.png) | ![figure10_result2.png](./image/figure10_result2.png) |\\n\\nImageNet Classification\uc5d0 \ub300\ud55c ShuffleNetV2 \uae30\uc900 \uacb0\uacfc\ub97c \uc0b4\ud3b4\ubcf4\uba74, \ud559\uc2b5 \uc18d\ub3c4\ub3c4 \ube60\ub97c \ubfd0\ub354\ub7ec, Meta-ACON\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c Error rate\uac00 \ub0ae\uc544\uc9c0\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc804\ubc18\uc801\uc73c\ub85c \ubaa8\ub378 \uc0ac\uc774\uc988\uac00 \ucee4\uc9c8 \uc218\ub85d, Meta-ACON\uc744 \uc0ac\uc6a9\ud560 \uc218\ub85d Accuracy \ud5a5\uc0c1\uc774 \ud07d\ub2c8\ub2e4. (Swish \ub300\uccb4, SENet Novelty \ucd94\uac00 \ub4f1 \ub300\ube44)\\n\\n<img className={clsx(styles.figCenter, styles.medium)} src={figResultFour} alt=\\"figure12_result4\\" />\\n<img className={clsx(styles.figCenter, styles.medium)} src={figResultFive} alt=\\"figure13_result5\\" />\\n\\n\uc774\ub807\uac8c Meta-ACON\uc740 \ub2e4\ub978 activation \ub300\ube44 ImageNet Classification\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc81c\ud55c\uc801\uc774\uae30\ub294 \ud558\ub098, \ud2b9\uc815 backbone\uc5d0 \ub300\ud574\uc11c Object Detection \ubc0f Semantic Segmentation\uc5d0 \uc788\uc5b4\uc11c\ub3c4 \ub2e4\ub978 activation function\uc744 \uc0ac\uc6a9\ud560 \ub54c\ubcf4\ub2e4 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\\n\\n### \ub9c8\ubb34\ub9ac\\n\\n\uc774\ub807\uac8c \uc624\ub298\uc740 ReLU\uc640 Swish \uac04\uc758 \uad00\uacc4\ub97c \ud1b5\ud574 \uc0c8\ub85c\uc6b4 Activation Function\ub4e4\uc774 \ud3ec\uc9c4\ub418\uc5b4 \uc788\uc744\ub9cc\ud55c \uc77c\ubc18\ud654\ub41c \uc2dd\uc744 \ucc3e\uace0(ACON Family), \uc774\ub97c \uae30\ubc18\uc73c\ub85c Trainable\ud55c Activation Function\uc744 \uc0c8\ub85c \ub9cc\ub098\ubcfc \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.  \\n\uc0ac\uc2e4 \uc774\ub807\uac8c \ud6c8\ub828 \uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c4 Activation Function\uc774 ACON\ub9cc \ucc98\uc74c\uc778 \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4. \ub610\ud55c, \uc5ec\ub7ec Sub-task\uc5d0 \ub300\ud574 \ubc94\uc6a9\uc801\uc73c\ub85c \uc0ac\uc6a9\ub420 \uc218 \uc788\ub294 Activation Function\uc77c\uc9c0\ub294 \ubbf8\uc9c0\uc218\uc774\uae30\ub3c4 \ud558\uace0\uc694. \ud2b9\ud788 \ubaa8\ub378 \uacbd\ub7c9\ud654 \ub4f1 \uc5b4\ub290 \ud55c\ud3b8\uc5d0\uc11c\ub294 Non-linear Activation Function\ub9c8\uc800 Bottleneck\uc73c\ub85c \uc9da\uace0 \ub118\uc5b4\uac00\ub294 \uc2e4\uc815\uc774\uae30\uc5d0[<sup>[3]</sup>](#r3), \ubaa8\ub4e0 \ubaa9\uc801\uc744 \ub9cc\uc871\uc2dc\ud0ac\ub9cc\ud55c \uc0c8\ub85c\uc6b4 \ud65c\uc131 \ud568\uc218\ub97c \ucc3e\uc740 \uc5f0\uad6c\ub294 \uc544\ub2d9\ub2c8\ub2e4. \ub2e4\ub9cc, \uc2dd\uc5d0 \ub300\ud55c \uac04\ub2e8\ud55c \uc815\ub9ac\ub85c ReLU\uc640 Swish \uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc784\uacfc \ub3d9\uc2dc\uc5d0, \uc0c8\ub85c\uc6b4 Activation Family\ub97c \uc81c\uc2dc\ud588\ub2e4\ub294 \ub370\uc5d0 \uc758\uc758\uac00 \uc788\ub294 \ub17c\ubb38\uc774\uc5c8\uc2b5\ub2c8\ub2e4.\\n\\nCVPR 2021\uc5d0\uc11c \uc774\ub7ec\ud55c \ub17c\ubb38\ub3c4 \ubc1c\ud45c\ub41c\ub2e4\ub294 \uac83\uc744 \ud568\uaed8 \uacf5\uc720\ud558\uace0 \uc2f6\uc5b4 \uac04\ub7b5\ud558\uac8c\ub098\ub9c8 \ub9ac\ubdf0\ub97c \uc9c4\ud589\ud574\ubd24\uc2b5\ub2c8\ub2e4 :+1:\\n\\n\\n### References (+ \ud568\uaed8 \uc77d\uc73c\uba74 \uc88b\uc740 \ub17c\ubb38\ub4e4)\\n\\n<a name=\\"r1\\"></a>\\n\\n1. Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \\"Searching for activation functions.\\" arXiv preprint arXiv:1710.05941 (2017). [[paper]](https://arxiv.org/abs/1710.05941) \\n\\n<a name=\\"r2\\"></a>\\n\\n2. Goodfellow, Ian, et al. \\"Maxout networks.\\" International conference on machine learning. PMLR, 2013. [[paper]](http://proceedings.mlr.press/v28/goodfellow13.html)\\n\\n<a name=\\"r3\\"></a>\\n\\n3. Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. \\"TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning\\n.\\" Part of Advances in Neural Information Processing Systems 33 (NeurIPS 2020) [[paper]](https://proceedings.neurips.cc//paper_files/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html)"},{"id":"anycost","metadata":{"permalink":"/papers/anycost","source":"@site/papers/2021-05-14-paper-review-anycostgan/index.mdx","title":"Anycost GANs for Interactive Image Synthesis and Editing","description":"\ud558\ub098\uc758 GAN Training\uc73c\ub85c \uc5ec\ub7ec \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uac8c \ud55c \uc5f0\uad6c\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4.","date":"2021-05-14T00:00:00.000Z","formattedDate":"May 14, 2021","tags":[{"label":"paper-review","permalink":"/papers/tags/paper-review"},{"label":"gan","permalink":"/papers/tags/gan"},{"label":"compression","permalink":"/papers/tags/compression"}],"readingTime":4.22,"truncated":true,"authors":[{"name":"Hyoung-Kyu Song","title":"AI Researcher (Vision)","url":"https://github.com/deepkyu","imageURL":"https://github.com/deepkyu.png","key":"hkyu"}],"frontMatter":{"slug":"anycost","title":"Anycost GANs for Interactive Image Synthesis and Editing","description":"\ud558\ub098\uc758 GAN Training\uc73c\ub85c \uc5ec\ub7ec \ud574\uc0c1\ub3c4\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uac8c \ud55c \uc5f0\uad6c\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4.","image":"img/default.png","authors":["hkyu"],"tags":["paper-review","gan","compression"]},"prevItem":{"title":"Activate or Not: Learning Customized Activation","permalink":"/papers/acon"},"nextItem":{"title":"Differentiable Augmentation for Data-Efficient GAN Training","permalink":"/papers/diffaugment"}},"content":"import clsx from \'clsx\';\\nimport styles from \'../blog.module.css\';\\n\\nimport figGeneral from \'./image/fig1_model-general.png\';\\nimport figArchi from \'./image/fig2_architecture.png\';\\nimport figTable from \'./image/fig3_table1.png\';\\nimport figAdaptiveCh from \'./image/fig4_adaptive_channel_training.png\';\\nimport figResultGraph from \'./image/fig5_result_graph.png\';\\nimport figChannelDiff from \'./image/fig6_channel_diff_result.png\';\\nimport figLsunResult from \'./image/fig7_LSUN_result.png\';\\nimport figMultiRes from \'./image/fig8_result_multiresolution.png\';\\nimport figLatentSpace from \'./image/fig9_latent_space_result.png\';\\n\\n[![githubio](https://img.shields.io/static/v1?message=Official%20Repo&logo=Github&labelColor=grey&color=blue&logoColor=white&label=%20&style=flat-square)](https://github.com/mit-han-lab/anycost-gan)\\n\\n> Lin, Ji, et al. \\"Anycost gans for interactive image synthesis and editing.\\"\\n> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\\n\\nSong Han (Network Compression \ucd5c\uac15) + Jun-Yan Zhu (GAN \ucd5c\uac15)\\n\\n<br/>\\n\\n\\n## Contribution\\n\\n<img className={styles.figCenter} src={figGeneral} alt=\\"model-general\\" />\\n\\n- Intermediate Layer\ub3c4 Generation\uc758 \uacb0\uacfc\ubb3c\uc774 \ub420 \uc218 \uc788\ub2e4\ub294 \ubd80\ubd84\uc744 \uc2dc\uc0ac\ud55c \uc5f0\uad6c\ub2e4.\\n- Low Resolution Preview\ub97c \ucd94\ucd9c\ud558\ub294 \ub370\uc5d0 \ubcc4\ub3c4\uc758 \ub124\ud2b8\uc6cc\ud06c\ub97c \uad6c\uc131\ud558\uac70\ub098, \uc18d\ub3c4 \uce21\uba74\uc5d0\uc11c \uc190\ud574\ubcf4\ub294 \uac83 \uc5c6\uc774 \uad6c\ud604\uc744 \ud588\ub2e4\ub294 \uc810\uc5d0\uc11c \uc55e\uc73c\ub85c\uc758 \uc4f0\uc784\uc0c8\uac00 \uae30\ub300\ub418\ub294 \ub17c\ubb38\uc774\ub2e4.\\n- Depth Search\ub97c \ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc77c\uc885\uc758 \ub124\ud2b8\uc6cc\ud06c \uacbd\ub7c9\ud654\ub97c \ud574\ub0b8\ub2e4\ub294 \uc544\uc774\ub514\uc5b4\ub294 \uae30\uc874\uc5d0\ub3c4 \ub9ce\uc774 \uc788\uc5c8\ub294\ub370, \uc774\ub97c Image Generation\uc5d0 \uc801\uc6a9\ud55c \uba87 \uc548 \ub418\ub294 \ub17c\ubb38\uc77c \uac83\uc774\uace0 G-conditioned Dsicriminator\ub97c \ud1b5\ud574 Discriminator\ub97c \uc0c1\ud669\uc5d0 \ub9de\uac8c \ud559\uc2b5\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc740 \uc55e\uc73c\ub85c\ub3c4 \ub9ce\uc774 \ucc28\uc6a9\ud560 \uc218 \uc788\ub2e4.\\n\\n\x3c!--truncate--\x3e\\n\\n## Main Method\\n\\n<img className={styles.figCenter} src={figArchi} alt=\\"architecture\\" />\\n\\n**\uc88c\uce21: Generator\uc758 Decoder, \uc6b0\uce21: Discriminator**\\n\\n### Sampling-based Multi-resolution Training\\n\\n<img className={styles.figCenter} src={figTable} alt=\\"table\\" />\\n\\nMSG-GAN\uc740 Generator\ub97c \ud559\uc2b5\ud558\ub294 \ub370 \uc788\uc5b4 \ub2e4\ub978 \ud574\uc0c1\ub3c4\ub97c \uc9c0\uc6d0\ud558\ub3c4\ub85d \ud559\uc2b5\ud558\uae30 \uc704\ud574 \ubaa8\ub4e0 \ud574\uc0c1\ub3c4\uc5d0 \ub300\ud55c Discriminator\ub97c \uac01\uac01 \ub9cc\ub4e4\uc5b4\uc11c \uc774\ub97c \ubaa8\ub450 \uc0ac\uc6a9\ud588\ub2e4. \ud558\uc9c0\ub9cc, \uc774\ub7ec\ud55c All-resolution training \ubc29\uc2dd\uc740 FFHQ \ucc98\ub7fc \ud070 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \uac01\uac01\uc744 single-resolution\uc73c\ub85c \ud559\uc2b5\ud558\ub294 \uac83\ubcf4\ub2e4 \uc88b\uc9c0 \uc54a\uc740 \uacb0\uacfc\ub97c \ub9cc\ub4e4\uc5c8\ub2e4.\\n\\n\ubcf8 \ubaa8\ub378\uc740 \uc774\ub97c Multi-resolution\uc73c\ub85c \ud559\uc2b5\ud560 \uc218 \uc788\uac8c sampling \ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \uc9c4\ud589\ud55c\ub2e4.\\n\\n### Adaptive Channel Training\\n\\n<img className={styles.figCenter} src={figAdaptiveCh} alt=\\"adaptive_channel\\" />\\n\\n\ubc14\ub77c\ub294 \uac83(\uc804\uc81c): Important Channel\ub4e4\uc758 \uacbd\uc6b0 sampling \ud558\ub294 \ub3d9\uc548 \ubcf4\uc874\ub420 \uac83\uc774\ub2e4.\\n\\n\ubc29\uc2dd\\n\\n- \uc774\uc804 stage\ub85c\ubd80\ud130 \ubaa8\ub378 Initialize\ub97c \uc9c4\ud589\ud55c\ub2e4.\\n- \uc774 \ub54c, Kernel Magnitude\ub97c \uae30\uc900\uc73c\ub85c High to Low\ub85c sorting\\n- 0.25, 0.5, 0.75, 1 x\uc758 \ube44\uc728 \uc911 \ud558\ub098\ub85c random\ud558\uac8c sampling \ud558\ub3c4\ub85d \ud568.\\n\\n\uc774\ub97c \ud1b5\ud574 \ubaa8\ub4e0 sub-network\uac00 fewer channel\ub9cc \uac00\uc9c0\uace0 \uc788\uc5b4\ub3c4 \uc2e4\uc7ac\uc801\uc778 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub3c4\ub85d \ud588\ub2e4. \ub2e4\ub9cc, \uc774\uac8c \uc2e4\uc81c full network\ub85c \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0\uc640\ub294 \ub2e4\ub97c \uc218 \ubc16\uc5d0 \uc5c6\uc5c8\ub294\ub370 \uc774\ub97c \uc704\ud574 consistent\ub97c \uc720\uc9c0\ud574\uc8fc\ub294 loss\ub97c \ucd94\uac00\ud588\ub2e4. (MSE (L2), LPIPS (Perceptual) Loss \uae30\ubc18)\\n\\n### Generator-conditioned discriminator\\n\\n\uac00\uc7a5 \uc26c\uc6b4 \ubc29\ubc95\uc740 Discriminator\uc758 channel \uc218\ub97c Generator\uc5d0 \ub9de\ucdb0 \uac00\uba70 \uc904\uc774\ub294 \ubc29\uc2dd\uc77c \uac83\uc774\ub2e4. \ub2e4\ub9cc, \uc774\ub7ec\ud55c \ubc29\uc2dd\uc740 Uniform\ud55c channel ratio\uc5d0\ub9cc \uc801\uc6a9\ub418\uace0, channel \uc218\uac00 \uc791\uc544\uc9c8 \ub54c\uc5d0\ub9cc \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \ub3c4\uc6c0\uc774 \ub418\ub294 \ubc29\ubc95\ub4e4\uc774\uc5c8\ub2e4.\\n\\n\uadf8\ub798\uc11c \uc774\ub97c \ub300\uc2e0\ud574\uc11c FC Layer\ub97c \ucd94\uac00\ud558\uc5ec learnable\ud558\uac8c \uad6c\uc870\ub97c \ub9cc\ub4e4\uc5c8\ub2e4.\\n\\n\uc6b0\uc120 Channel Configuration\uc744 Encoding \ud558\ub3c4\ub85d \ud55c\ub2e4. One-hot Encoding\uc73c\ub85c \uc704\uc5d0 0.25, 0.5, 0.75, 1 \uc911 \uac01 resolution\uc5d0 \ud574\ub2f9\ud558\ub294 layer\uac00 \ubb34\uc5c7\uc744 \uc120\ud0dd\ud588\ub294 \uc9c0\ub97c \ub098\ud0c0\ub0b4\ub294 \ubc29\uc2dd\uc774\ub2e4. \uc5ec\uae30\uc11c \uc774\ub97c G Architecture Vector (g_arch)\ub77c\uace0 \ubd80\ub978\ub2e4. \uadf8\ub9ac\uace0 \uc774\ub97c FC\uc5d0 \uc2e4\uc5b4 per-channel modulation\uc744 \ud560 \uc218 \uc788\ub3c4\ub85d Discrminator\uc5d0 \ub118\uaca8 \uc900\ub2e4. Real Image\ub97c Discriminator\uc5d0 \ub123\uc744 \ub54c\uc5d0\ub294 g_arch\ub97c random\ud558\uac8c \ubd80\uc5ec\ud55c\ub2e4.\\n\\nStablized Training \ud558\uae30 \uc704\ud574 \uc774\ub97c \uc804\uccb4 Discriminator\uc5d0 \uc801\uc6a9\ud558\ub294 \uac83\uc740 \uc544\ub2c8\uace0, Discriminator\uc758 \ub9c8\uc9c0\ub9c9 \ub450 Block\uc5d0\ub9cc \uc801\uc6a9\ud558\ub3c4\ub85d \ud588\ub2e4.\\n\\n## Result\\n\\n<img className={styles.figCenter} src={figResultGraph} alt=\\"fig5_result_graph\\" />\\n<img className={styles.figCenter} src={figChannelDiff} alt=\\"fig6_channel_diff_result\\" />\\n<img className={styles.figCenter} src={figLsunResult} alt=\\"fig7_LSUN_result\\" />\\n<img className={styles.figCenter} src={figMultiRes} alt=\\"fig8_result_multiresolution\\" />\\n<img className={styles.figCenter} src={figLatentSpace} alt=\\"fig9_latent_space_result\\" />"},{"id":"diffaugment","metadata":{"permalink":"/papers/diffaugment","source":"@site/papers/2021-05-03-paper-review-diffaugment/index.mdx","title":"Differentiable Augmentation for Data-Efficient GAN Training","description":"\uc801\uc740 \ub370\uc774\ud130\ub85c \ud6a8\uc728\uc801\uc73c\ub85c GAN \ud559\uc2b5\ud558\ub294 \ubc29\ubc95\uc744 \ud655\uc778\ud574\ubd05\ub2c8\ub2e4.","date":"2021-05-03T00:00:00.000Z","formattedDate":"May 3, 2021","tags":[{"label":"paper-review","permalink":"/papers/tags/paper-review"},{"label":"gan","permalink":"/papers/tags/gan"}],"readingTime":6.86,"truncated":true,"authors":[{"name":"Hyoung-Kyu Song","title":"AI Researcher (Vision)","url":"https://github.com/deepkyu","imageURL":"https://github.com/deepkyu.png","key":"hkyu"}],"frontMatter":{"slug":"diffaugment","title":"Differentiable Augmentation for Data-Efficient GAN Training","description":"\uc801\uc740 \ub370\uc774\ud130\ub85c \ud6a8\uc728\uc801\uc73c\ub85c GAN \ud559\uc2b5\ud558\ub294 \ubc29\ubc95\uc744 \ud655\uc778\ud574\ubd05\ub2c8\ub2e4.","image":"img/default.png","authors":["hkyu"],"tags":["paper-review","gan"]},"prevItem":{"title":"Anycost GANs for Interactive Image Synthesis and Editing","permalink":"/papers/anycost"},"nextItem":{"title":"FreezeD: a Simple Baseline for Fine-tuning GANs","permalink":"/papers/freezed"}},"content":"import clsx from \'clsx\';\\nimport styles from \'../blog.module.css\';\\n\\nimport figAblation from \'./image/ablation_augmentation.png\';\\nimport figAugResult from \'./image/augmentation_type_result.png\';\\nimport figAugType from \'./image/augmentation_type.png\';\\nimport figOverfitting from \'./image/d_overfitting.png\';\\nimport figInterpolation from \'./image/interpolation.png\';\\nimport figModelSize from \'./image/model_size_fid.png\';\\nimport figReg from \'./image/r1_regularization.png\';\\nimport figResultCompare from \'./image/result_compare.png\';\\nimport figTrainingMethod from \'./image/training_method.png\';\\nimport figVsStylegan from \'./image/vs_stylegan2.png\';\\n\\n[![githubio](https://img.shields.io/static/v1?message=Official%20Repo&logo=Github&labelColor=grey&color=blue&logoColor=white&label=%20&style=flat-square)](https://github.com/mit-han-lab/data-efficient-gans)\\n\\n> Zhao, Shengyu, et al. \\"Differentiable augmentation for data-efficient gan training.\\"  \\n> Advances in Neural Information Processing Systems 33 (2020): 7559-7570.\\n\\n\ucc38\uace0\ub85c Song Han \uc5f0\uad6c\uc2e4\uc740 Neural Network Compression \ubd84\uc57c\uc5d0 \uc788\uc5b4 Top\uc744 \ub2ec\ub9ac\uace0 \uc788\ub294 \uc5f0\uad6c\uc2e4\uc774\ub2e4. \ud6c4\uc5d0 \uc18c\uac1c\ud560 AnycostGAN\uc5d0\uc11c\ub3c4 DiffAugment\uac00 \uc5b8\uae09\uc774 \ub41c\ub2e4.\\n\\nHan Lab\uc740 \uae30\uc874\uc5d0\ub294 \ubc29\ubc95\ub860\uc5d0 \uc788\uc5b4 Network Pruning, KD(Knowledge Distillation) \ub4f1\uc5d0 \uc9d1\uc911\uc744 \ud588\uace0, TinyTL \ub4f1 Activation\uc5d0 \ub300\ud55c \uacbd\ub7c9\ud654\ub3c4 \uc5f0\uad6c\ub97c \uc9c4\ud589\ud588\ub2e4. \ub2e4\ub9cc, \ub300\ubd80\ubd84 Task\uac00 Image Recognition\uc5d0 \uad6d\ud55c\ub418\uc5b4 \uc788\uc5c8\ub2e4. Song Han\uc740 CVPR2020\uc5d0\uc11c GAN Compression\uc774\ub77c\ub294 \ub17c\ubb38\uc744 \ud1b5\ud574 Image Generation\uc5d0 \ub300\ud55c Compression \ub17c\ubb38\uc744 \uc2dc\uc791\uc73c\ub85c, GAN \uad00\ub828 \uc5f0\uad6c\ub97c \uc9c0\uc18d\ud574\uc11c \uc774\uc5b4\uc624\uace0 \uc788\ub2e4. \\n\\n\x3c!--truncate--\x3e\\n\\n## \ubc30\uacbd \uc9c0\uc2dd\\n\\n### \uae30\uc874\uc758 Regularization\\n\\nGAN Training \uc790\uccb4\uac00 \uad49\uc7a5\ud788 Unstable\ud55c Process\uc774\uae30 \ub54c\ubb38\uc5d0, \ucd94\uac00\uc801\uc778 Regularization\uc774 \ub9ce\uc774 \ud544\uc694\ud558\ub2e4. \uc9c0\uae08\uae4c\uc9c0 \uc5ec\ub7ec\uac00\uc9c0 Regularization \ubc29\uc2dd\ub4e4\uc774 \ub4f1\uc7a5\ud588\ub2e4.\\n\\n- Instance Noise\\n- Jensen-Shannon Regularization\\n- Gradient Penalty\\n- Spectral Normalization\\n- Adversarial Defense Regularization\\n- Consistency Regularization\\n\\n\uc774\ub7ec\ud55c Regularization Method\ub4e4\uc740 Input \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \ub300\uc751\ud558\ub294 \uac83\uc774\uc9c0, augmentation\uc5d0 \ub300\ud574 \uc18c\ud654\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\ub4e4\uc774 \uc544\ub2c8\ub2e4.\\n\\n\uc800\uc790\ub294 \uc5ec\ub7ec \uac00\uc9c0 Augmentation\uc5d0 \ub300\ud574\uc11c\ub3c4 \uc798 working \ud558\ub294 Discriminator\ub97c \uad6c\ucd95\ud558\uace0\uc790 \ud588\ub2e4.\\n\\n### D\ub294 \uc9c0\uae08\uae4c\uc9c0 Overfitting \ud574\uc654\ub2e4\\n\\n<img className={styles.figCenter} src={figOverfitting} alt=\\"d_overfitting\\" />\\n\\nBigGAN\uc744 \uc801\uc740 \ub370\uc774\ud130\uc5d0 \ub300\ud574 \ud559\uc2b5\ud588\uc744 \ub54c, \ud559\uc2b5\uc744 \ud558\ub2e4\uac00 collapse\ud558\ub294 \uac78 \ubcf4\uc5ec\uc8fc\ub294 Figure\ub2e4. \uc67c\ucabd\uc5d0\uc11c \ubcf4\uba74 \ube68\uac04\uc0c9(CIFAR-10 10%\ub9cc \uac00\uc9c0\uace0 \ud559\uc2b5) \uadf8\ub798\ud504\uac00 \ud280\uc5b4\ubc84\ub9ac\ub294 \uac78 \ubcfc \uc218 \uc788\ub2e4. \uc774\uac8c \uc65c \uadf8\ub7f4\uae4c \ud558\uace0 Discriminator\ub97c \ubcf4\uba74, \uc544\ub2c8\ub098 \ub2e4\ub97c\uae4c Training Accuracy\uac00 \ube60\ub974\uac8c \ud559\uc2b5\uc774 \ub3fc\uc11c \uadf8\ub807\ub2e4.\\n\\n\uadf8\ub7f0\ub370 D\uc5d0 \ub300\ud574 \uac01 Iteration\uc5d0 \ub300\ud55c Validation Acc.\ub97c \uce21\uc815\ud574\ubcf4\uba74, mode collapse\uac00 \ub418\uc5c8\ub2e4\ub294 \uac78 \ubcfc \uc218 \uc788\ub2e4. \uc989, D\uac00 training set \uc744 memorize \ud574\uc654\uace0, \uc774\ub85c \uc778\ud574 generalize\uac00 \uc548\ub410\ub2e4\ub294 \uac78 \ubcf4\uc5ec\uc900\ub2e4.\\n\\n## Training Method\\n\\n<img className={clsx(styles.figCenter, styles.medium)} src={figTrainingMethod} alt=\\"training_method\\" />\\n\\nDiffAugment\uc758 \ud559\uc2b5 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uadf8\ub9bc.\\n\\n\uc6d0\ubcf8 \uc774\ubbf8\uc9c0(x), \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0(G(z)) \ubaa8\ub450\uc5d0 augmentation(T)\uc744 \uc801\uc6a9\ud55c\ub2e4.\\n\\nAugmentation Senario\uc5d0 \ub530\ub77c\uc11c \uc5ec\ub7ec \uac00\uc9c0 Case\ub85c \ub098\ub20c \uc218 \uc788\ub2e4.\\n\\nAugment Reals Only: Real \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574\ub9cc Augmentation\uc744 \uc9c4\ud589\ud568 (i\ub9cc \uc9c4\ud589.)\\n\u2192 Augmentation \ud55c \uac78 \uadf8\ub300\ub85c \ubaa8\ubc29\ud558\uc5ec \uc0dd\uc131\ud55c\ub2e4.\\n\\nAugment D Only: Discriminator\uc5d0 \ub123\ub294 Input\ub4e4\uc5d0 \ub300\ud574 \uc9c4\ud589\ud568 (i, ii \ub9cc \uc9c4\ud589. iii\ub294 \uadf8\ub300\ub85c)\\n\u2192 Unbalanced Optimization \uc5d0 \uc758\ud574 Diverge \ud574\ubc84\ub9b0\ub2e4.\\n\\nD perfectly classifies the augmented images (both T(x) and T(G(z)) but barely\\nrecognizes G(z) (i.e., fake images without augmentation)\\n\uc774 \ub54c\ubb38\uc5d0 G\uc758 gradient update\ub97c \ud560 \ub54c, Discriminator\uac00 \uc798 working \ud558\uc9c0 \ubabb\ud558\uba74\uc11c, G\uc5d0 \ub300\ud55c \ud559\uc2b5\uc5d0 \ubc29\ud574\uac00 \ub428.\\n\\n## Result\\n\\n### StyleGAN2\uc640 \ube44\uad50\\n\\n<img className={styles.figCenter} src={figVsStylegan} alt=\\"vs_stylegan2\\" />\\n\\n\uae30\uc874 StyleGAN2\uacfc \ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc81c\uc2dc\ud558\ub294 DiffAugment\ub97c \uc801\uc6a9\ud588\uc744 \ub54c\ub97c \ube44\uad50\ud558\ub294 Figure.  \\nStyleGAN2\ub294 \ub370\uc774\ud130\uac00 \uc791\uc544\uc9c8\uc218\ub85d, FID, IS \uac12\uc758 \ubcc0\ud654\uac00 dramatic\ud55c\ub370, DiffAugment\ub294 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\uc740 \ub370\uc774\ud130\uc5d0 \ub300\ud574\uc11c\ub3c4 generalize\ub418\uc5b4 \uc788\ub2e4\ub294 \uac78 \ubcfc \uc218 \uc788\ub2e4. StyleGAN2\uac00 \uc791\uc740 \ub370\uc774\ud130\uc5d0 generalize\uac00 \uc548\ub41c\ub2e4\ub294 \uac74 \uc880 \uc54c\ub824\uc9c4 \uc0ac\uc2e4\uc774\uc5c8\uc73c\ub2c8, Discriminator Training Method\uc5d0 \uc9d1\uc911\ud55c ADA\ub098 Freeze D\uc640 \ube44\uad50\ud558\ub294 Figure\ub97c \uc790\uc5f0\uc2a4\ub7fd\uac8c \uae30\ub300\ud558\uac8c \ub41c\ub2e4.\\n\\n### Low-Shot Generation\\n\\n<img className={styles.figCenter} src={figResultCompare} alt=\\"result_compare\\" />\\n\\nLow-shot generation without pre-training.\\n\\n\uac01\uac01 \uc624\ubc14\ub9c8 100\uc7a5, \uace0\uc591\uc774 160\uc7a5, \uac15\uc544\uc9c0 389\uc7a5\ub9cc\uc744 \uac00\uc9c0\uace0 \ud559\uc2b5\uc744 \ud558\uc5ec \uc0dd\uc131\ud574\ub0b8 \uc774\ubbf8\uc9c0\ub4e4\uc774\ub2e4. (w/o pre-training)\\n\\n## Result\\n\\n### Training Performance\\n\\n<img className={styles.figCenter} src={figAugResult} alt=\\"augmentation_type_result\\" />\\n\\nAugmentation \uc870\ud569\uc5d0 \ub530\ub77c \ub2ec\ub77c\uc9c0\ub294 DiffAugment Training Performance (CIFAR-10 Data 100% \uc0ac\uc6a9)\\n\\nBigGAN \ub300\ube44 Discriminator\uac00 Validation\uc5d0 \uc788\uc5b4\uc11c\ub3c4 generalize \ud558\uc5ec \ud559\uc2b5\ub418\uc5c8\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4. DiffAugment \uc790\uccb4\ub294 Low Dataset\uc5d0 \ub300\ud574\uc11c\ub3c4 Generation\uc744 \uc548\uc815\uc801\uc73c\ub85c \ud560 \uc218 \uc788\ub2e4\ub294 Novelty\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \ubcf8 Figure\ub294 BigGAN\ub3c4 \ud559\uc2b5\ud588\ub2e4\uace0 \ud558\ub294 \ub370\uc774\ud130\uac00 CIFAR-10 \uc804\uccb4\ub97c \uc0ac\uc6a9\ud55c \uac83\uc774\uc5c8\uc73c\ub2c8, \ud574\ub2f9 \uc870\uac74\uc73c\ub85c Comparison\uc744 \ud55c \uac83\uc73c\ub85c \ud574\uc11d\ud560 \uc218 \uc788\ub2e4.\\n\\n### Interpolation\ub3c4 \uac00\ub2a5\ud558\ub2e4\\n\\n<img className={styles.figCenter} src={figInterpolation} alt=\\"interpolation\\" />\\n\\nStyle Space\uc5d0\uc11c Interpolation\uc774 \uac00\ub2a5\ud568\uc744 \ubcf4\uc5ec\uc8fc\ub294 Figure.\\n\\n\uc801\uc740 \ub370\uc774\ud130\uc600\uc9c0\ub9cc (\uc624\ubc14\ub9c8, \uace0\uc591\uc774, \uac15\uc544\uc9c0 \ucc38\uc870), Interpolation\uc774 \uac00\ub2a5\ud560 \uc815\ub3c4\ub85c generalize\ud558\uac8c \ud559\uc2b5\uc774 \ub418\uc5c8\ub2e4.\\n\\n### Model Size\uc5d0 \ub300\ud55c \ubd84\uc11d\\n\\n<img className={styles.figCenter} src={figModelSize} alt=\\"model_size_fid\\" />\\n\\nModel size\uac00 FID\uc5d0 \uc5bc\ub9c8\ub098 \uc601\ud5a5\uc744 \uc8fc\ub294\uac00\uc5d0 \ub300\ud55c Figure\\n\\nCIFAR-10 \ub370\uc774\ud130 10%\uc5d0 \ub300\ud574\uc11c\ub9cc \ud559\uc2b5\uc744 \ud588\uc744 \ub54c, generalize\uac00 \ub420 \uc218 \uc788\ub294 \uac00\ub97c \ub2e4\ub8ec \ubd80\ubd84\uc774\ub2e4. BigGAN \ub300\ube44 channel size\ub97c \uc904\uc5ec\uac00\uba70 \ube44\uad50\ub97c \ud574\ubd10\ub3c4, \uc5b4\ub5a4 capacity\ub4e0 \uc0c1\uad00\uc5c6\uc774 \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4. StyleGAN2\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 R1 Regularization \uad00\ub828\ud574\uc11c\ub3c4, hyperparameter\uac00 \ub3d9\uc77c\ud55c \uc138\ud305\uc77c \ub54c \ub354 \ub098\uc740 FID\ub97c \ubcf4\uc5ec\uc92c\ub2e4.\\n\\n### Regularization\\n\\n:::tip\\n\\n**R1 Regularization**  \\nGradient Penalty\ub85c regularize \ud558\ub294 \ubc29\uc2dd\uc73c\ub85c, Discriminator\uc758 Real Data\uc5d0 \ub300\ud574 Penalize\ub97c \uc900\ub2e4.\\n\\n$$\\nR_{1}(\\\\psi):=\\\\frac{\\\\gamma}{2} \\\\mathrm{E}_{p_{\\\\mathcal{D}}(x)}\\\\left[\\\\left\\\\|\\\\nabla D_{\\\\psi}(x)\\\\right\\\\|^{2}\\\\right]\\n$$\\n\\n> when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the GAN game.\\n\\n:::\\n\\n### Ablation for Augmentation\\n\\n<img className={styles.figCenter} src={figAblation} alt=\\"ablation_augmentation\\" />\\n\\n\uc5ec\ub7ec \uac00\uc9c0 Augmentation\uc5d0 \ub300\ud574 FID\ub97c \ud655\uc778\ud574\ubcf8 \uacb0\uacfc\ub2e4.\\nAblation\uc744 \uc9c4\ud589\ud55c \ub05d\uc5d0, color distortion, translation, cutout\ub9cc \uc900 \uac83\uc73c\ub85c \ud655\uc778\ud588\ub2e4. (\uba3c\uc800 \uc0dd\uac01\ud558\uace0 \uc900 \uac74 \uc544\ub2cc \uac83\uc73c\ub85c \ud655\uc778\ud588\ub2e4.)\\nStyleGAN2\uc758 FID\uc774\uace0, CIFAR-10 10% \ub370\uc774\ud130\ub9cc\uc744 \uac00\uc9c0\uace0 Training \ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub2e4."},{"id":"freezed","metadata":{"permalink":"/papers/freezed","source":"@site/papers/2020-09-24-paper-review-freezed/index.mdx","title":"FreezeD: a Simple Baseline for Fine-tuning GANs","description":"GAN Finetuning \uc2dc\uc5d0 Discriminator\ub97c Freezing \ud558\uba74 \uc5b4\ub5bb\uac8c \ub420\uae4c\uc694?","date":"2020-09-24T00:00:00.000Z","formattedDate":"September 24, 2020","tags":[{"label":"paper-review","permalink":"/papers/tags/paper-review"},{"label":"gan","permalink":"/papers/tags/gan"}],"readingTime":7.665,"truncated":true,"authors":[{"name":"Hyoung-Kyu Song","title":"AI Researcher (Vision)","url":"https://github.com/deepkyu","imageURL":"https://github.com/deepkyu.png","key":"hkyu"}],"frontMatter":{"slug":"freezed","title":"FreezeD: a Simple Baseline for Fine-tuning GANs","description":"GAN Finetuning \uc2dc\uc5d0 Discriminator\ub97c Freezing \ud558\uba74 \uc5b4\ub5bb\uac8c \ub420\uae4c\uc694?","image":"img/default.png","authors":["hkyu"],"tags":["paper-review","gan"]},"prevItem":{"title":"Differentiable Augmentation for Data-Efficient GAN Training","permalink":"/papers/diffaugment"}},"content":"import clsx from \'clsx\';\\nimport styles from \'../blog.module.css\';\\n\\nimport figGraph from \'./image/result_graph.png\';\\nimport figTable from \'./image/table.png\';\\n\\n[![githubio](https://img.shields.io/static/v1?message=Official%20Repo&logo=Github&labelColor=grey&color=blue&logoColor=white&label=%20&style=flat-square)](https://github.com/sangwoomo/FreezeD)\\n\\n> Mo, Sangwoo, et al. \\"Freeze the discriminator: a simple baseline for fine-tuning gans.\\"  \\n> CVPR AI for Content Creation Workshop (2020).\\n\\n\\n## Contribution\\n\\nGAN\uc744\xa0fine-tuning\ud558\ub294\xa0\ub370\uc5d0\xa0\uc788\uc5b4, discriminator\uc758\xa0lower layer\ub4e4(classifier\xa0\ub9d0\uace0\xa0visul repr. (=visual feature) extractor)\uc5d0 \ub300\ud574\xa0freeze\ud558\uace0\xa0G/D\uc5d0\xa0\ub300\ud574\xa0fine-tuning\uc744\xa0\uc9c4\ud589\ud558\uba74, limited data\ub85c\xa0\ud6a8\uacfc\uc801\uc778\xa0transfer learning\uc744\xa0\ud560\xa0\uc218\xa0\uc788\ub2e4.\\n\\n\uc989, image classification \ub4f1 recognition(\ub610\ub294 understanding)\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 fine-tuning \ubc29\uc2dd (visual representation\uc740 freeze, classifier\ub9cc use_gradient)\uc774 GAN \ubaa8\ub378\uc5d0\uc11c\ub3c4 \uc720\uc6a9\ud558\uac8c \uc0ac\uc6a9\ub420 \uc218 \uc788\ub2e4.\\n\\n<img className={styles.figCenter} src={figGraph} alt=\\"result_graph\\" />\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\n\ucd5c\uadfc\xa0SotA GAN\ub4e4\uc740\xa0\ub9ce\uc740\xa0\uc591\uc758\xa0training data\ub97c\xa0\uc694\uad6c\ud560\xa0\ubfd0\ub9cc\xa0\uc544\ub2c8\ub77c,\xa0\ub9e4\uc6b0\xa0\ud070 computational resource\ub97c\xa0\uc694\uad6c\ud558\ub294\xa0\uacbd\uc6b0\uac00\xa0\ub9ce\uc544,\xa0\uc2e4\uc9c8\uc801\uc778\xa0senario\uc5d0\xa0\ub300\uc785\ud558\ub294\xa0\ub370\uc5d0\xa0\uc5b4\ub824\uc6c0\uc774\xa0\ub9ce\ub2e4.\\n\\n\uadf8\ub798\uc11c\xa0\uc774\ub97c\xa0\ud574\uacb0\ud558\uace0\uc790\xa0\ub9ce\uc740\xa0\uc811\uadfc\ubc95\ub4e4\uc774\xa0recogntion task\uc5d0\uc11c\uc758\xa0\uc131\uacf5\uc801\uc778\xa0\uc0ac\ub840\ub4e4\uc744\xa0\ubc14\ud0d5\uc73c\ub85c\xa0GAN\uc5d0\uc11c\uc758\xa0transfer learning\uc744\xa0\uc2dc\ub3c4\ud558\uace0\xa0\uc788\uace0,\xa0\uc774\ub97c\xa0\ud1b5\ud574\xa0\ud55c\uc815\ub41c\xa0\ub370\uc774\ud130\ub97c\xa0\uac00\uc9c0\uace0 image generation\xa0\ud558\ub294\xa0\ubc29\ubc95\ub4e4\uc744\xa0\uc5f0\uad6c\ud558\uace0\xa0\uc788\ub2e4.\\n\\n\ud558\uc9c0\ub9cc\xa0\ud604\uc7ac\uae4c\uc9c0\xa0Transfer Learning\uc744\xa0\uc9c4\ud589\ud55c\xa0\ub9ce\uc740\xa0\uc0ac\ub840\ub4e4\uc774\xa0\ud55c\uc815\ub41c\xa0training data\ub85c\xa0\uc9c4\ud589\ud588\uc744\xa0\ub54c, overfitting\uc774\xa0\ub418\ub294\xa0\uacbd\uc6b0\uac00\ub9ce\uc558\uace0,\xa0\ud2b9\ud788\xa0dataset\uc758\xa0distribution\uc774\xa0pre-training\uc744\xa0\ud560\xa0\ub54c\xa0\uc0ac\uc6a9\ud55c\xa0large dataset\uacfc\xa0\ub9ce\uc774\xa0\ub2e4\ub97c\xa0\uacbd\uc6b0, robust\ud558\uc9c0\xa0\ubabb\ud558\ub294\xa0\uc77c\uc774\xa0\ub9ce\uc558\ub2e4 (not robust in learning a significant distribution shift).\\n\\n### Previous Methods for GAN transfer learning\\n\\n\uc6b0\uc120 \uae30\uc874\uc758 GAN Finetuning\uc73c\ub85c \uc2dc\ub3c4\ub41c \ubc29\uc2dd\uc740 \uc544\ub798\uc640 \uac19\uc774 \ub098\uc5f4\ud560 \uc218 \uc788\ub2e4.\\n\\n#### Fine-tuning\\n\\nTarget Model(=limited data\uc5d0 \ub300\ud55c model)\uc758 generator\uc640 discriminator \uac01\uac01\uc5d0 \ub300\ud574 Source Model(=trained with large dataset)\uc758 pre-trained weight\ub97c load\ud558\uc5ec, \ud574\ub2f9 checkpoint\ubd80\ud130 training\uc744 \uc9c4\ud589\ud55c\ub2e4.\\n\\n\ud558\uc9c0\ub9cc, \uc774\ub7ec\ud55c fine-tuning \ubc29\uc2dd\uc740 overfit\ub418\ub294 \ubb38\uc81c\uc810\uc744 \ud56d\uc0c1 \uac00\uc9c0\uace0 \uc788\uace0, \uc774\ub97c \uc704\ud574 \uc801\uc808\ud55c regularization\uc774 \ub9e4\ubc88 \ud544\uc694\ud558\ub2e4.\\n\\n#### Scale/Shift\\n\\n\uc704\uc640 \uac19\uc740 naive fine-tuning\uc774 overfit\uc5d0 \ube60\uc9c0\uae30 \uc27d\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\uae30\uc5d0, scale + shift\ub294 \ub2e4\ub978 weight\ub294 \uadf8\ub300\ub85c \ub194\ub450\uace0 batchnorm \ub4f1 normalization layer\ub9cc update\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc774\ub97c \uc9c4\ud589\ud558\uace0\uc790 \uc2dc\ub3c4\ud588\ub2e4. \ud558\uc9c0\ub9cc, \uc774\ub294 restriction\uc774 \uba85\ud655\ud574\uc11c \uadf8\ub9ac \uc88b\uc740 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uc9c0 \uc54a\uc558\uace0, \ud2b9\ud788 source dataset(=large dataset)\uacfc target dataset(=limited dataset) \uac04 distribution shift\uac00 \uadf9\uba85\ud560 \uacbd\uc6b0, \uacb0\uacfc\uac00 \ub354\uc6b1 \uc548 \uc88b\uac8c \ub098\uc654\ub2e4.\\n\\n#### Generative latent optimization (GLO)\\n\\nGAN loss\ub294 discriminator\ub85c\ubd80\ud130 \uc8fc\uc5b4\uc9c0\ub294 loss\uc774\ub2e4\ubcf4\ub2c8, limited data\uc5d0 reliable \ud558\uc9c0 \uc54a\ub2e4. GLO\ub294 \uc774\ub85c\ubd80\ud130 source data\ub85c \ud559\uc2b5\ud55c generator\ub9cc \ub5bc\uc5b4\ub2e4\uac00 L1 + perceptual loss\uc758 \uc870\ud569\uc73c\ub85c supervised learning \ud558\ub294 \ubc29\uc2dd\uc744 \ub9d0\ud55c\ub2e4. \uc774 \ub54c, GLO\ub294 overfitting\uc744 \ubc29\uc9c0\ud558\uae30 \uc704\ud574 generator\uc640 latent codes\ub97c \ub3d9\uc2dc\uc5d0 optimize\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ud6c8\ub828\ud55c\ub2e4. \uc989, latent vector\uc640 sample data\uac00 \ud56d\uc0c1 1:1\ub85c \ub418\ub3c4\ub85d \ud559\uc2b5\ud55c\ub2e4.  \uc774\ub97c \ud1b5\ud574 generator\uac00 sample\ub4e4\uc5d0 \ub300\ud574 generalize \ub418\ub3c4\ub85d \ud558\ub294 \ubc29\ubc95\uc774\ub2e4.\\n\\nGLO\uac00 \uc774\ub7f0 \ubc29\ud5a5\uc73c\ub85c \ud559\uc2b5\uc744 \ud558\ub2e4 \ubcf4\ub2c8 stability\ub294 \ubcf4\uc7a5\uc774 \ub418\uc9c0\ub9cc, \uc544\ubb34\ub798\ub3c4 adversarial loss\uac00 \uc5c6\ub2e4\ubcf4\ub2c8 image generation \uacb0\uacfc\uac00 \uacb0\uad6d\uc5d0\ub294 blurry \ud574\uc9c4\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \uadf8\ub807\ub2e4\uace0 Adversarial \ud558\uac8c \ubcc4\ub3c4\ub85c discrimator\ub97c \ubd99\uc77c\uc218\ub3c4 \uc788\uaca0\uc9c0\ub9cc, \uc774\ub7ec\ud55c \ubc29\ud5a5\uc73c\ub85c \uc9c4\ud589\ub420 \ub54c \uae30\ubcf8\uc801\uc73c\ub85c source data\uc5d0 \ub300\ud55c discriminator\uc758 prior knowledge\uac00 \uc544\uc608 \uc99d\ubc1c\ud574\ubc84\ub9ac\ubbc0\ub85c, \uc774 \uc5ed\uc2dc \uadf8\ub807\uac8c \ud6a8\uc728\uc801\uc774\ub77c\uace0\ub294 \ubcfc \uc218 \uc5c6\ub2e4.\\n\\n#### MineGAN\\n\\nMineGAN\uc740 generator\uc758 overfitting\uc744 \ud53c\ud558\uae30 \uc704\ud574, generator\ub97c \uc218\uc815\ud558\uc5ec latent code\ub97c \uc218\uc815\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4. MineGAN\uc740 latent code \uac04 transfer\ub97c \ub2f4\ub2f9\ud558\ub294 miner(\ucc44\uad74) network\ub97c \ud559\uc2b5\uc2dc\ud0a4\ub294\ub370, \uc774\ub7ec\ud55c \ubc29\uc2dd\uc740 source\uc640 target distribution\uc774 \uacf5\uc720\ud558\uace0 \uc788\ub294 \ubc14\uac00 \uc788\uc744 \ub54c \ud6a8\uacfc\uc801\uc774\uaca0\uc9c0\ub9cc, \ub450 dataset\uac00 disjoint \ud55c \uc0c1\ud669\uc5d0\uc11c\ub294 generalize \ub420 \uc218 \uc5c6\ub2e4\ub294 \ub2e8\uc810\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4.\\n\\n### Suggested Methods\\n\\n#### FreezeD\\n\\nDiscriminator\uc758 lower layer(=visual representation)\ub9cc freeze, upper layer\ub294 fine-tune. \uc800\uc790\uac00 \uc131\ub2a5\ud55c \uac83 \uc911\uc5d0 \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc74c.\\n\\n#### L2-SP\\n\\nFine-tuning\ud558\ub294 \ubc29\uc2dd\uc778\ub370, source model\uc758 parameter\uc640 target model\uc758 parameter\ub97c L2-norm regularize \ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc9c4\ud589\ud558\uc5ec target model\uc774 source model\uc758 knowledge\ub85c\ubd80\ud130 \ub108\ubb34 \uba40\uc5b4\uc9c0\uc9c0 \uc54a\uac8c \ud55c\ub2e4.\\n\\n\ud558\uc9c0\ub9cc, \uc774\ub294 \uadf8\ub807\uac8c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uc9c0 \ubabb\ud588\ub2e4. L2-norm \ud55c\ub2e4\uace0 \ud558\uba74 \uc774 \ubc29\uc2dd\uc774 \uacb0\uad6d freezing layer\ub85c \uc120\ud0dd\ud55c layer\uc5d0\ub294 infinite weight\ub97c \uc8fc\uace0, \ub2e4\ub978 layer\uc5d0\ub294 weight\ub97c 0\uc73c\ub85c \uc8fc\ub294 \ubc29\uc2dd\uc73c\ub85c \uad73\uc5b4\uc9c0\ub294\ub370, \uc774\uac83\ubcf4\ub2e4\ub294 \uac01 Layer \uc5d0 \uc801\uc808\ud55c weight \ub97c \uc8fc\ub294 \uac83\uc774 \ub2f9\uc5f0 \uc88b\uc740 \uacb0\uacfc\ub97c \ub0bc \uac83\uc774\ub2e4.\\n\\n#### Feature Distillation\\n\\nClassifier\uc758 transfer learning\uc744 \ud560 \ub54c, \uc694\uc998 \uc81c\uc77c \ub9ce\uc774 \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc774\ub2e4.\\n\\n\uc800\uc790\ub294 source model\uacfc target model\uc744 distill \ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc9c4\ud589\ud588\ub2e4. Computation\uc73c\ub85c \ubcf4\uba74 FreezeD\ubcf4\ub2e4 2\ubc30 \uac00\ub7c9 \uc5f0\uc0b0\ub7c9\uc774 \ub9ce\uc740\ub370, FreezeD\ub791 \ube44\uad50\ud588\uc744 \ub54c \uc6b0\uc704\ub97c \uc810\ud558\ub294 \uacbd\uc6b0\ub3c4 \uc788\uc5c8\ub2e4. \uc774 \ub17c\ubb38\uc740 FreezeD\uac00 \ucd5c\uc885\uc801\uc73c\ub85c \uc81c\uc548\ud558\ub294 method\uc774\uae30\ub294 \ud558\uc9c0\ub9cc, Feature Distillation\uc774 \ubbf8\ub798\uc5d0 \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \ubc29\ud5a5\uc77c \uac83\uc774\ub77c\uace0 \uc774\uc57c\uae30 \ud55c\ub2e4.\\n\\n<img className={styles.figCenter} src={figTable} alt=\\"table\\" />\\n\\n## Result\\n\\n\uc800\uc790\ub294 Unconditional GAN\uacfc Conditional GAN\uc5d0 \ubaa8\ub450 generic \ud558\uac8c \uc801\uc6a9\ub420 \uc218 \uc788\ub294 \ubc29\ubc95\uc778\uc9c0\ub97c \ud30c\uc545\ud558\uae30 \uc704\ud574, \ub450 \uac00\uc9c0 adversarial model\uc744 \ub4e4\uace0 \uc654\ub2e4. Unconditional GAN\uc73c\ub85c\ub294 stylegan, conditional GAN\uc73c\ub85c\ub294 SNGAN-projection \uc744 \uc0ac\uc6a9\ud588\ub2e4. \uadf8\ub9ac\uace0 \uce21\uc815 metric \uc73c\ub85c\ub294 FID(Frechet Inception Distance)\ub9cc\uc744 \uc0ac\uc6a9\ud588\ub2e4.\\n\\n\uac01 Dataset class \ubcc4\ub85c FID\ub97c \uac01\uac01 \uacc4\uc0b0\ud588\uace0, Fine-tuning \ud55c \uac83\uacfc FreezeD \ub450 case\uc5d0 \ub300\ud55c \uacb0\uacfc\ub9cc\uc744 \ube44\uad50\ud588\ub2e4.\\n\\nFreezeD \uac00 FID\uc5d0 \uc788\uc5b4\uc11c \ud56d\uc0c1 \uc55e\uc11c\ub294 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc92c\ub2e4.\\n\\n## \ud2b9\uc774\uc810\\n\\nStyleGAN\uc744 \uc2e4\ud5d8\ud560 \ub54c, tensorflow\ub85c \ub418\uc5b4 \uc788\ub294 \uc800\uc790 repo\uac00 \uc544\ub2c8\ub77c, unofficial pytorch repo\ub97c \uc0ac\uc6a9\ud588\ub2e4.\\n\\npytorch repo\ub3c4 \ub9ce\uc740 \uc720\uc800\ub4e4\uc774 \uc548\uc815\uc131 \uac80\uc99d\uc744 \ud558\uae30\ub294 \ud588\uc5c8\ub294\ub370, \uc774\uac78 \uc0ac\uc6a9\ud558\uc5ec \ub17c\ubb38\uc774 accept\uc774 \ub418\uc5c8\uc73c\ub2c8, \ub098\ub3c4 \uc368\ub3c4 \ub418\uaca0\ub2e4\ub294 \uc0dd\uac01\uc774 \ub4e0\ub2e4. (\uc774\ub807\uac8c guarantee \ubc1b\ub294 \ubc29\ubc95\uc774...!)"}]}')}}]);
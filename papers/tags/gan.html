<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-223362952-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="alternate" type="application/rss+xml" href="/story/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/story/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/papers/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/papers/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="js/scroll.js"></script><title data-rh="true">2 posts tagged with &quot;gan&quot; | Hyoung-Kyu Song</title><meta data-rh="true" property="og:title" content="2 posts tagged with &quot;gan&quot; | Hyoung-Kyu Song"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://deepkyu.github.io//papers/tags/gan"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepkyu.github.io//papers/tags/gan"><link data-rh="true" rel="alternate" href="https://deepkyu.github.io//papers/tags/gan" hreflang="en"><link data-rh="true" rel="alternate" href="https://deepkyu.github.io//papers/tags/gan" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.416ca7f4.css">
<link rel="preload" href="/assets/js/runtime~main.79987285.js" as="script">
<link rel="preload" href="/assets/js/main.dcb62cf8.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Deepkyu" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="Deepkyu" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a class="navbar__item navbar__link" href="/blog">Blog</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers">Paper Reviews</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/cv">CV</a><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://www.linkedin.com/in/deepkyu" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">All Posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/acon">Activate or Not: Learning Customized Activation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/diffaugment">Differentiable Augmentation for Data-Efficient GAN Training</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/freezed">FreezeD: a Simple Baseline for Fine-tuning GANs</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;gan&quot;</h1><a href="/papers/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/papers/diffaugment">Differentiable Augmentation for Data-Efficient GAN Training</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-05-03T00:00:00.000Z" itemprop="datePublished">May 3, 2021</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/deepkyu.png" alt="Hyoung-Kyu Song"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyoung-Kyu Song</span></a></div><small class="avatar__subtitle" itemprop="description">AI Researcher (Vision)</small></div></div></div></div></header><meta itemprop="image" content="https://deepkyu.github.io//img/default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2006.10738" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2006.10738-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/mit-han-lab/data-efficient-gans" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Zhao, Shengyu, et al. &quot;Differentiable augmentation for data-efficient gan training.&quot;<br>
<!-- -->Advances in Neural Information Processing Systems 33 (2020): 7559-7570.</p></blockquote><p>참고로 Song Han 연구실은 Neural Network Compression 분야에 있어 Top을 달리고 있는 연구실이다. 후에 소개할 AnycostGAN에서도 DiffAugment가 언급이 된다.</p><p>Han Lab은 기존에는 방법론에 있어 Network Pruning, KD(Knowledge Distillation) 등에 집중을 했고, TinyTL 등 Activation에 대한 경량화도 연구를 진행했다. 다만, 대부분 Task가 Image Recognition에 국한되어 있었다. Song Han은 CVPR2020에서 GAN Compression이라는 논문을 통해 Image Generation에 대한 Compression 논문을 시작으로, GAN 관련 연구를 지속해서 이어오고 있다. </p><h2 class="anchor anchorWithStickyNavbar_mojV" id="배경-지식">배경 지식<a class="hash-link" href="#배경-지식" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="기존의-regularization">기존의 Regularization<a class="hash-link" href="#기존의-regularization" title="Direct link to heading">​</a></h3><p>GAN Training 자체가 굉장히 Unstable한 Process이기 때문에, 추가적인 Regularization이 많이 필요하다. 지금까지 여러가지 Regularization 방식들이 등장했다.</p><ul><li>Instance Noise</li><li>Jensen-Shannon Regularization</li><li>Gradient Penalty</li><li>Spectral Normalization</li><li>Adversarial Defense Regularization</li><li>Consistency Regularization</li></ul><p>이러한 Regularization Method들은 Input 이미지에 대해 대응하는 것이지, augmentation에 대해 소화할 수 있는 방법들이 아니다.</p><p>저자는 여러 가지 Augmentation에 대해서도 잘 working 하는 Discriminator를 구축하고자 했다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="d는-지금까지-overfitting-해왔다">D는 지금까지 Overfitting 해왔다<a class="hash-link" href="#d는-지금까지-overfitting-해왔다" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/d_overfitting-0fde96f1fa39bd9365ce7230b620fffb.png" alt="d_overfitting"><p>BigGAN을 적은 데이터에 대해 학습했을 때, 학습을 하다가 collapse하는 걸 보여주는 Figure다. 왼쪽에서 보면 빨간색(CIFAR-10 10%만 가지고 학습) 그래프가 튀어버리는 걸 볼 수 있다. 이게 왜 그럴까 하고 Discriminator를 보면, 아니나 다를까 Training Accuracy가 빠르게 학습이 돼서 그렇다.</p><p>그런데 D에 대해 각 Iteration에 대한 Validation Acc.를 측정해보면, mode collapse가 되었다는 걸 볼 수 있다. 즉, D가 training set 을 memorize 해왔고, 이로 인해 generalize가 안됐다는 걸 보여준다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="training-method">Training Method<a class="hash-link" href="#training-method" title="Direct link to heading">​</a></h2><img class="figCenter__7gH medium_sJ4m" src="/assets/images/training_method-182e7e89d5035fa3118b77161271b12b.png" alt="training_method"><p>DiffAugment의 학습 방법을 보여주는 그림.</p><p>원본 이미지(x), 생성한 이미지(G(z)) 모두에 augmentation(T)을 적용한다.</p><p>Augmentation Senario에 따라서 여러 가지 Case로 나눌 수 있다.</p><p>Augment Reals Only: Real 이미지에 대해만 Augmentation을 진행함 (i만 진행.)
→ Augmentation 한 걸 그대로 모방하여 생성한다.</p><p>Augment D Only: Discriminator에 넣는 Input들에 대해 진행함 (i, ii 만 진행. iii는 그대로)
→ Unbalanced Optimization 에 의해 Diverge 해버린다.</p><p>D perfectly classifies the augmented images (both T(x) and T(G(z)) but barely
recognizes G(z) (i.e., fake images without augmentation)
이 때문에 G의 gradient update를 할 때, Discriminator가 잘 working 하지 못하면서, G에 대한 학습에 방해가 됨.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="result">Result<a class="hash-link" href="#result" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="stylegan2와-비교">StyleGAN2와 비교<a class="hash-link" href="#stylegan2와-비교" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/vs_stylegan2-f1b3f845a404ca098b35e023e87f8a99.png" alt="vs_stylegan2"><p>기존 StyleGAN2과 본 논문에서 제시하는 DiffAugment를 적용했을 때를 비교하는 Figure.<br>
<!-- -->StyleGAN2는 데이터가 작아질수록, FID, IS 값의 변화가 dramatic한데, DiffAugment는 상대적으로 적은 데이터에 대해서도 generalize되어 있다는 걸 볼 수 있다. StyleGAN2가 작은 데이터에 generalize가 안된다는 건 좀 알려진 사실이었으니, Discriminator Training Method에 집중한 ADA나 Freeze D와 비교하는 Figure를 자연스럽게 기대하게 된다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="low-shot-generation">Low-Shot Generation<a class="hash-link" href="#low-shot-generation" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/result_compare-acc36d84cc9ea455714866e23cef7a61.png" alt="result_compare"><p>Low-shot generation without pre-training.</p><p>각각 오바마 100장, 고양이 160장, 강아지 389장만을 가지고 학습을 하여 생성해낸 이미지들이다. (w/o pre-training)</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="result-1">Result<a class="hash-link" href="#result-1" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="training-performance">Training Performance<a class="hash-link" href="#training-performance" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/augmentation_type_result-e03bdd14ef6248c21157d69ba3f953af.png" alt="augmentation_type_result"><p>Augmentation 조합에 따라 달라지는 DiffAugment Training Performance (CIFAR-10 Data 100% 사용)</p><p>BigGAN 대비 Discriminator가 Validation에 있어서도 generalize 하여 학습되었다고 볼 수 있다. DiffAugment 자체는 Low Dataset에 대해서도 Generation을 안정적으로 할 수 있다는 Novelty를 가지고 있지만, 본 Figure는 BigGAN도 학습했다고 하는 데이터가 CIFAR-10 전체를 사용한 것이었으니, 해당 조건으로 Comparison을 한 것으로 해석할 수 있다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="interpolation도-가능하다">Interpolation도 가능하다<a class="hash-link" href="#interpolation도-가능하다" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/interpolation-a37d53f43c37be1bde34d1839ecf1837.png" alt="interpolation"><p>Style Space에서 Interpolation이 가능함을 보여주는 Figure.</p><p>적은 데이터였지만 (오바마, 고양이, 강아지 참조), Interpolation이 가능할 정도로 generalize하게 학습이 되었다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="model-size에-대한-분석">Model Size에 대한 분석<a class="hash-link" href="#model-size에-대한-분석" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/model_size_fid-b395637cdd2677201a79958933acd29f.png" alt="model_size_fid"><p>Model size가 FID에 얼마나 영향을 주는가에 대한 Figure</p><p>CIFAR-10 데이터 10%에 대해서만 학습을 했을 때, generalize가 될 수 있는 가를 다룬 부분이다. BigGAN 대비 channel size를 줄여가며 비교를 해봐도, 어떤 capacity든 상관없이 더 좋은 성능을 보여준다. StyleGAN2에서 사용하는 R1 Regularization 관련해서도, hyperparameter가 동일한 세팅일 때 더 나은 FID를 보여줬다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="regularization">Regularization<a class="hash-link" href="#regularization" title="Direct link to heading">​</a></h3><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</h5></div><div class="admonition-content"><p><strong>R1 Regularization</strong><br>
<!-- -->Gradient Penalty로 regularize 하는 방식으로, Discriminator의 Real Data에 대해 Penalize를 준다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>ψ</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mfrac><mi>γ</mi><mn>2</mn></mfrac><msub><mi mathvariant="normal">E</mi><mrow><msub><mi>p</mi><mi mathvariant="script">D</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo fence="true">[</mo><msup><mrow><mo fence="true">∥</mo><mi mathvariant="normal">∇</mi><msub><mi>D</mi><mi>ψ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">∥</mo></mrow><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">R_{1}(\psi):=\frac{\gamma}{2} \mathrm{E}_{p_{\mathcal{D}}(x)}\left[\left\|\nabla D_{\psi}(x)\right\|^{2}\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">ψ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.836em;vertical-align:-0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathrm">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:0.02778em">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size2">[</span></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em">∥</span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">ψ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">∥</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954em"><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size2">]</span></span></span></span></span></span></span></div><blockquote><p>when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the GAN game.</p></blockquote></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="ablation-for-augmentation">Ablation for Augmentation<a class="hash-link" href="#ablation-for-augmentation" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/ablation_augmentation-6fad8d2422af1ae0b528dfa994cf5049.png" alt="ablation_augmentation"><p>여러 가지 Augmentation에 대해 FID를 확인해본 결과다.
Ablation을 진행한 끝에, color distortion, translation, cutout만 준 것으로 확인했다. (먼저 생각하고 준 건 아닌 것으로 확인했다.)
StyleGAN2의 FID이고, CIFAR-10 10% 데이터만을 가지고 Training 했을 때의 결과다.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/papers/tags/paper-review">paper-review</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/papers/tags/gan">gan</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Differentiable Augmentation for Data-Efficient GAN Training" href="/papers/diffaugment"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/papers/freezed">FreezeD: a Simple Baseline for Fine-tuning GANs</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2020-09-24T00:00:00.000Z" itemprop="datePublished">September 24, 2020</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/deepkyu.png" alt="Hyoung-Kyu Song"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyoung-Kyu Song</span></a></div><small class="avatar__subtitle" itemprop="description">AI Researcher (Vision)</small></div></div></div></div></header><meta itemprop="image" content="https://deepkyu.github.io//img/default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2002.10964" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2002.10964-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/sangwoomo/FreezeD" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Mo, Sangwoo, et al. &quot;Freeze the discriminator: a simple baseline for fine-tuning gans.&quot;<br>
<!-- -->CVPR AI for Content Creation Workshop (2020).</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="contribution">Contribution<a class="hash-link" href="#contribution" title="Direct link to heading">​</a></h2><p>GAN을 fine-tuning하는 데에 있어, discriminator의 lower layer들(classifier 말고 visul repr. (=visual feature) extractor)에 대해 freeze하고 G/D에 대해 fine-tuning을 진행하면, limited data로 효과적인 transfer learning을 할 수 있다.</p><p>즉, image classification 등 recognition(또는 understanding)에서 사용하는 fine-tuning 방식 (visual representation은 freeze, classifier만 use_gradient)이 GAN 모델에서도 유용하게 사용될 수 있다.</p><img class="figCenter__7gH" src="/assets/images/result_graph-131a4e1493d0c89f7b52e6d88ab11441.png" alt="result_graph"><h2 class="anchor anchorWithStickyNavbar_mojV" id="background">Background<a class="hash-link" href="#background" title="Direct link to heading">​</a></h2><p>최근 SotA GAN들은 많은 양의 training data를 요구할 뿐만 아니라, 매우 큰 computational resource를 요구하는 경우가 많아, 실질적인 senario에 대입하는 데에 어려움이 많다.</p><p>그래서 이를 해결하고자 많은 접근법들이 recogntion task에서의 성공적인 사례들을 바탕으로 GAN에서의 transfer learning을 시도하고 있고, 이를 통해 한정된 데이터를 가지고 image generation 하는 방법들을 연구하고 있다.</p><p>하지만 현재까지 Transfer Learning을 진행한 많은 사례들이 한정된 training data로 진행했을 때, overfitting이 되는 경우가많았고, 특히 dataset의 distribution이 pre-training을 할 때 사용한 large dataset과 많이 다를 경우, robust하지 못하는 일이 많았다 (not robust in learning a significant distribution shift).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="previous-methods-for-gan-transfer-learning">Previous Methods for GAN transfer learning<a class="hash-link" href="#previous-methods-for-gan-transfer-learning" title="Direct link to heading">​</a></h3><p>우선 기존의 GAN Finetuning으로 시도된 방식은 아래와 같이 나열할 수 있다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="fine-tuning">Fine-tuning<a class="hash-link" href="#fine-tuning" title="Direct link to heading">​</a></h4><p>Target Model(=limited data에 대한 model)의 generator와 discriminator 각각에 대해 Source Model(=trained with large dataset)의 pre-trained weight를 load하여, 해당 checkpoint부터 training을 진행한다.</p><p>하지만, 이러한 fine-tuning 방식은 overfit되는 문제점을 항상 가지고 있고, 이를 위해 적절한 regularization이 매번 필요하다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="scaleshift">Scale/Shift<a class="hash-link" href="#scaleshift" title="Direct link to heading">​</a></h4><p>위와 같은 naive fine-tuning이 overfit에 빠지기 쉽다는 단점이 있기에, scale + shift는 다른 weight는 그대로 놔두고 batchnorm 등 normalization layer만 update하는 방식으로 이를 진행하고자 시도했다. 하지만, 이는 restriction이 명확해서 그리 좋은 결과를 보여주지 않았고, 특히 source dataset(=large dataset)과 target dataset(=limited dataset) 간 distribution shift가 극명할 경우, 결과가 더욱 안 좋게 나왔다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="generative-latent-optimization-glo">Generative latent optimization (GLO)<a class="hash-link" href="#generative-latent-optimization-glo" title="Direct link to heading">​</a></h4><p>GAN loss는 discriminator로부터 주어지는 loss이다보니, limited data에 reliable 하지 않다. GLO는 이로부터 source data로 학습한 generator만 떼어다가 L1 + perceptual loss의 조합으로 supervised learning 하는 방식을 말한다. 이 때, GLO는 overfitting을 방지하기 위해 generator와 latent codes를 동시에 optimize하는 방식으로 훈련한다. 즉, latent vector와 sample data가 항상 1:1로 되도록 학습한다.  이를 통해 generator가 sample들에 대해 generalize 되도록 하는 방법이다.</p><p>GLO가 이런 방향으로 학습을 하다 보니 stability는 보장이 되지만, 아무래도 adversarial loss가 없다보니 image generation 결과가 결국에는 blurry 해진다는 단점이 있다. 그렇다고 Adversarial 하게 별도로 discrimator를 붙일수도 있겠지만, 이러한 방향으로 진행될 때 기본적으로 source data에 대한 discriminator의 prior knowledge가 아예 증발해버리므로, 이 역시 그렇게 효율적이라고는 볼 수 없다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="minegan">MineGAN<a class="hash-link" href="#minegan" title="Direct link to heading">​</a></h4><p>MineGAN은 generator의 overfitting을 피하기 위해, generator를 수정하여 latent code를 수정하는 방법을 제시한다. MineGAN은 latent code 간 transfer를 담당하는 miner(채굴) network를 학습시키는데, 이러한 방식은 source와 target distribution이 공유하고 있는 바가 있을 때 효과적이겠지만, 두 dataset가 disjoint 한 상황에서는 generalize 될 수 없다는 단점을 가지고 있다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="suggested-methods">Suggested Methods<a class="hash-link" href="#suggested-methods" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_mojV" id="freezed">FreezeD<a class="hash-link" href="#freezed" title="Direct link to heading">​</a></h4><p>Discriminator의 lower layer(=visual representation)만 freeze, upper layer는 fine-tune. 저자가 성능한 것 중에 가장 좋은 성능을 보였음.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="l2-sp">L2-SP<a class="hash-link" href="#l2-sp" title="Direct link to heading">​</a></h4><p>Fine-tuning하는 방식인데, source model의 parameter와 target model의 parameter를 L2-norm regularize 하는 방식으로 진행하여 target model이 source model의 knowledge로부터 너무 멀어지지 않게 한다.</p><p>하지만, 이는 그렇게 좋은 성능을 보이지 못했다. L2-norm 한다고 하면 이 방식이 결국 freezing layer로 선택한 layer에는 infinite weight를 주고, 다른 layer에는 weight를 0으로 주는 방식으로 굳어지는데, 이것보다는 각 Layer 에 적절한 weight 를 주는 것이 당연 좋은 결과를 낼 것이다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="feature-distillation">Feature Distillation<a class="hash-link" href="#feature-distillation" title="Direct link to heading">​</a></h4><p>Classifier의 transfer learning을 할 때, 요즘 제일 많이 사용하는 방식이다.</p><p>저자는 source model과 target model을 distill 하는 방식으로 진행했다. Computation으로 보면 FreezeD보다 2배 가량 연산량이 많은데, FreezeD랑 비교했을 때 우위를 점하는 경우도 있었다. 이 논문은 FreezeD가 최종적으로 제안하는 method이기는 하지만, Feature Distillation이 미래에 더 좋은 성능을 보이는 방향일 것이라고 이야기 한다.</p><img class="figCenter__7gH" src="/assets/images/table-8729e8237c9747aac68039866747ac52.png" alt="table"><h2 class="anchor anchorWithStickyNavbar_mojV" id="result">Result<a class="hash-link" href="#result" title="Direct link to heading">​</a></h2><p>저자는 Unconditional GAN과 Conditional GAN에 모두 generic 하게 적용될 수 있는 방법인지를 파악하기 위해, 두 가지 adversarial model을 들고 왔다. Unconditional GAN으로는 stylegan, conditional GAN으로는 SNGAN-projection 을 사용했다. 그리고 측정 metric 으로는 FID(Frechet Inception Distance)만을 사용했다.</p><p>각 Dataset class 별로 FID를 각각 계산했고, Fine-tuning 한 것과 FreezeD 두 case에 대한 결과만을 비교했다.</p><p>FreezeD 가 FID에 있어서 항상 앞서는 결과를 보여줬다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="특이점">특이점<a class="hash-link" href="#특이점" title="Direct link to heading">​</a></h2><p>StyleGAN을 실험할 때, tensorflow로 되어 있는 저자 repo가 아니라, unofficial pytorch repo를 사용했다.</p><p>pytorch repo도 많은 유저들이 안정성 검증을 하기는 했었는데, 이걸 사용하여 논문이 accept이 되었으니, 나도 써도 되겠다는 생각이 든다. (이렇게 guarantee 받는 방법이...!)</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/papers/tags/paper-review">paper-review</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/papers/tags/gan">gan</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about FreezeD: a Simple Baseline for Fine-tuning GANs" href="/papers/freezed"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/papers">Reviews</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/story">Story</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/deepkyu" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Deepkyu. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.79987285.js"></script>
<script src="/assets/js/main.dcb62cf8.js"></script>
</body>
</html>
---
slug: freezed
title: "FreezeD: a Simple Baseline for Fine-tuning GANs"
description: GAN Finetuning 시에 Discriminator를 Freezing 하면 어떻게 될까요?
image: img/default.png
authors: [hkyu]
tags: [paper-review, gan]
---

import clsx from 'clsx';
import styles from '../blog.module.css';

import figGraph from './image/result_graph.png';
import figTable from './image/table.png';

## Contribution

GAN을 fine-tuning하는 데에 있어, discriminator의 lower layer들(classifier 말고 visul repr. (=visual feature) extractor)에 대해 freeze하고 G/D에 대해 fine-tuning을 진행하면, limited data로 효과적인 transfer learning을 할 수 있다.

즉, image classification 등 recognition(또는 understanding)에서 사용하는 fine-tuning 방식 (visual representation은 freeze, classifier만 use_gradient)이 GAN 모델에서도 유용하게 사용될 수 있다.

<img className={styles.figCenter} src={figGraph} alt="result_graph" />

## Background

최근 SotA GAN들은 많은 양의 training data를 요구할 뿐만 아니라, 매우 큰 computational resource를 요구하는 경우가 많아, 실질적인 senario에 대입하는 데에 어려움이 많다.

그래서 이를 해결하고자 많은 접근법들이 recogntion task에서의 성공적인 사례들을 바탕으로 GAN에서의 transfer learning을 시도하고 있고, 이를 통해 한정된 데이터를 가지고 image generation 하는 방법들을 연구하고 있다.

하지만 현재까지 Transfer Learning을 진행한 많은 사례들이 한정된 training data로 진행했을 때, overfitting이 되는 경우가많았고, 특히 dataset의 distribution이 pre-training을 할 때 사용한 large dataset과 많이 다를 경우, robust하지 못하는 일이 많았다 (not robust in learning a significant distribution shift).

### Previous Methods for GAN transfer learning

우선 기존의 GAN Finetuning으로 시도된 방식은 아래와 같이 나열할 수 있다.

#### Fine-tuning

Target Model(=limited data에 대한 model)의 generator와 discriminator 각각에 대해 Source Model(=trained with large dataset)의 pre-trained weight를 load하여, 해당 checkpoint부터 training을 진행한다.

하지만, 이러한 fine-tuning 방식은 overfit되는 문제점을 항상 가지고 있고, 이를 위해 적절한 regularization이 매번 필요하다.

#### Scale/Shift

위와 같은 naive fine-tuning이 overfit에 빠지기 쉽다는 단점이 있기에, scale + shift는 다른 weight는 그대로 놔두고 batchnorm 등 normalization layer만 update하는 방식으로 이를 진행하고자 시도했다. 하지만, 이는 restriction이 명확해서 그리 좋은 결과를 보여주지 않았고, 특히 source dataset(=large dataset)과 target dataset(=limited dataset) 간 distribution shift가 극명할 경우, 결과가 더욱 안 좋게 나왔다.

#### Generative latent optimization (GLO)

GAN loss는 discriminator로부터 주어지는 loss이다보니, limited data에 reliable 하지 않다. GLO는 이로부터 source data로 학습한 generator만 떼어다가 L1 + perceptual loss의 조합으로 supervised learning 하는 방식을 말한다. 이 때, GLO는 overfitting을 방지하기 위해 generator와 latent codes를 동시에 optimize하는 방식으로 훈련한다. 즉, latent vector와 sample data가 항상 1:1로 되도록 학습한다.  이를 통해 generator가 sample들에 대해 generalize 되도록 하는 방법이다.

GLO가 이런 방향으로 학습을 하다 보니 stability는 보장이 되지만, 아무래도 adversarial loss가 없다보니 image generation 결과가 결국에는 blurry 해진다는 단점이 있다. 그렇다고 Adversarial 하게 별도로 discrimator를 붙일수도 있겠지만, 이러한 방향으로 진행될 때 기본적으로 source data에 대한 discriminator의 prior knowledge가 아예 증발해버리므로, 이 역시 그렇게 효율적이라고는 볼 수 없다.

#### MineGAN

MineGAN은 generator의 overfitting을 피하기 위해, generator를 수정하여 latent code를 수정하는 방법을 제시한다. MineGAN은 latent code 간 transfer를 담당하는 miner(채굴) network를 학습시키는데, 이러한 방식은 source와 target distribution이 공유하고 있는 바가 있을 때 효과적이겠지만, 두 dataset가 disjoint 한 상황에서는 generalize 될 수 없다는 단점을 가지고 있다.

### Suggested Methods

#### FreezeD

Discriminator의 lower layer(=visual representation)만 freeze, upper layer는 fine-tune. 저자가 성능한 것 중에 가장 좋은 성능을 보였음.

#### L2-SP

Fine-tuning하는 방식인데, source model의 parameter와 target model의 parameter를 L2-norm regularize 하는 방식으로 진행하여 target model이 source model의 knowledge로부터 너무 멀어지지 않게 한다.

하지만, 이는 그렇게 좋은 성능을 보이지 못했다. L2-norm 한다고 하면 이 방식이 결국 freezing layer로 선택한 layer에는 infinite weight를 주고, 다른 layer에는 weight를 0으로 주는 방식으로 굳어지는데, 이것보다는 각 Layer 에 적절한 weight 를 주는 것이 당연 좋은 결과를 낼 것이다.

#### Feature Distillation

Classifier의 transfer learning을 할 때, 요즘 제일 많이 사용하는 방식이다.

저자는 source model과 target model을 distill 하는 방식으로 진행했다. Computation으로 보면 FreezeD보다 2배 가량 연산량이 많은데, FreezeD랑 비교했을 때 우위를 점하는 경우도 있었다. 이 논문은 FreezeD가 최종적으로 제안하는 method이기는 하지만, Feature Distillation이 미래에 더 좋은 성능을 보이는 방향일 것이라고 이야기 한다.

<img className={styles.figCenter} src={figTable} alt="table" />

## Result

저자는 Unconditional GAN과 Conditional GAN에 모두 generic 하게 적용될 수 있는 방법인지를 파악하기 위해, 두 가지 adversarial model을 들고 왔다. Unconditional GAN으로는 stylegan, conditional GAN으로는 SNGAN-projection 을 사용했다. 그리고 측정 metric 으로는 FID(Frechet Inception Distance)만을 사용했다.

각 Dataset class 별로 FID를 각각 계산했고, Fine-tuning 한 것과 FreezeD 두 case에 대한 결과만을 비교했다.

FreezeD 가 FID에 있어서 항상 앞서는 결과를 보여줬다.

## 특이점

StyleGAN을 실험할 때, tensorflow로 되어 있는 저자 repo가 아니라, unofficial pytorch repo를 사용했다.

pytorch repo도 많은 유저들이 안정성 검증을 하기는 했었는데, 이걸 사용하여 논문이 accept이 되었으니, 나도 써도 되겠다는 생각이 든다. (이렇게 guarantee 받는 방법이...!)
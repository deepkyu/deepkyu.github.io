<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://deepkyu.me/papers</id>
    <title>Hyoung-Kyu Song Blog</title>
    <updated>2022-08-31T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://deepkyu.me/papers"/>
    <subtitle>Hyoung-Kyu Song Blog</subtitle>
    <icon>https://deepkyu.me/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[A ConvNet for the 2020s (ConvNeXt)]]></title>
        <id>convnext</id>
        <link href="https://deepkyu.me/papers/convnext"/>
        <updated>2022-08-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Convolution이 말합니다. 형 아직 죽지 않았어 임마.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/facebookresearch/ConvNeXt" target="_blank" rel="noopener noreferrer">github</a></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="main-idea">Main Idea<a class="hash-link" href="#main-idea" title="Direct link to heading">​</a></h2><p>Vision Task에 Transformer 기반 Architecture를 접목하는 ViT의 등장 이후, classification task에 대해 좋은 성능을 보였습니다. ViT 이후에 Swim Transformers 등의 방법론 등은 segmentation이나 object detection에도 transformer를 적용하기 위해 등장한 방법들이었습니다. Swim Transformer의 경우, 여러 ConvNet을 prior로 삼는 hybrid한 방법으로 이를 해결한 것이 특징입니다. 하지만, hybrid하다고 하기에는 기존 Transformer의 힘을 빌린 것일 뿐, ConvNet 자체가 가지고 있는 inductive bias를 최대한 사용한 방법은 아닙니다.</p><p>저자는 순수한 ConvNet(Transformer 구조를 곁들이지 않은)의 힘을 확인하고자 기존의 standard ResNet(architecture + 학습 방법론)에서 vision Transformer처럼 학습할 수 있게끔 지금까지 등장해온 여러 Novelty들을 접목해보고자 시도합니다. 특히, vision Transformer 모델들이 등장할 때 항상 새로운 학습 방법론을 함께 들고 나와서 성능 향상을 주장하고는 하는데, 그 학습 방법론들을 기존 ConvNet들에 적용해본 사례가 많이 없습니다. 여러 실험을 통해 최근에 등장한 학습 방법론을 적용하고 convolution block design을 새롭게 디자인하면 더 직관적인 구조로 Transformer에 근접한 성능을 보일 수 있음을 보여줍니다.</p><p>가끔 ConvNeXt 논문을 들어보기만 하고 착각하는 것 중 하나가, ConvNeXt의 FLOPs 가 Transformer보다 현저히 작은 채로 성능을 비등하게 낸 것으로 아시는 분들이 있습니다. ConvNeXt는 architecture가 (inductive bias에서 등장했던) ConvNet을 순수하게 기반으로 했을 뿐, 그 모델 사이즈가 Transformers 보다 절대 작지는 않습니다. 다만, 상대적으로 Transformer보다는 convolution layer에 대한 compression 방법론들이 더 많이 고안된만큼, 그 직관적인 구조에 의해 등장했던 compression 방법론들을 두루 적용하여 앞으로 경량화할 수 있는 가능성이 좀 더 있다는 게 제 개인적인 생각입니다.</p><p>주로 ResNet 50 을 기준으로 성능 report를 진행하고, 각 accuracy는 random seed를 다르게 하여 3번씩 실험한 결과입니다. 요즘 논문치고 철저하다고 보는 분도 계시고, 통계적으로 무슨 의미가 있냐고 하시는 분도 있더라고요 ㅎㅎ 다만, publish 이후 학계에서 검증하면서 ConvNeXt 실험 결과가 잘못됐다고 하는 논문은 본 적이 없습니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="background-knowledge">Background Knowledge<a class="hash-link" href="#background-knowledge" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="examples-of-representative-convnet">Examples of Representative ConvNet<a class="hash-link" href="#examples-of-representative-convnet" title="Direct link to heading">​</a></h3><p>VGGNet, Inceptions, ResNe(X)t, DenseNet, MobileNet, EfficientNet and RegNet</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="convnet-의-주요-특징들">ConvNet 의 주요 특징들<a class="hash-link" href="#convnet-의-주요-특징들" title="Direct link to heading">​</a></h3><p>아래는 "sliding window"를 사용하는 convolution에서 고안되었기 때문에 생겨나는 특징들입니다.</p><ul><li>Translation equivariant<ul><li>Object detection 등의 Task에 있어서 특히 유용합니다.</li><li>가끔 equivariance, invariance 헷갈려 하시는 분들 계셔서 말씀드리면, Patch를 이동하든, 그 결과값을 이동한다고 feature vector 값이 바뀌는 건 아니여서 invariant 하다고도 볼 수 있습니다. (g: Identity Mapping)</li></ul></li><li>Weight Sharing</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화1-training-methodology">변화1: Training Methodology<a class="hash-link" href="#변화1-training-methodology" title="Direct link to heading">​</a></h2><p>저자는 ResNet 50 에 DeiT, Swim Transformer와 유사한 training recipe를 적용하여 성능 향상이 얼마나 일어나는 지를 살펴봅니다.</p><ul><li>Training epoch 증가<ul><li>90 -&gt; 300</li></ul></li><li>Optimizer 변경<ul><li>Adam -&gt; AdamW</li></ul></li><li>Data augmentation<ul><li>Mixup, Cutmix, RandAugment, Random Erasing을 추가</li></ul></li><li>Regularization scheme 추가<ul><li>Stochastic depth<ul><li>Depth를 이루는 ResBlocks 중 일부를 random하게 drop하면서 학습하는 방법입니다. 여기서 drop 한다는 의미는 구현에서 봤을 때는 ResBlock과 identity mapping(skip connection) 중 선택하는 것과 같습니다.<ul><li>"... Aims to shrink the depth of a network during training, while keeping it unchanged during testing. This is achieved by randomly dropping entire&nbsp;ResBlocks&nbsp;during training and bypassing their transformations through skip connections."</li></ul></li></ul></li><li>Label smoothing</li></ul></li></ul><p>이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 76.1% -&gt; 78.8% (+2.7%)</li></ul><p>어쩌면 traditional ConvNets와 vision Transformers의 차이는 학습 방법에서 주로 기인한 게 아닐까 싶을 정도의 차이일 수도 있습니다.<br>
<!-- -->저자는 이 파트에서 찾은 training recipe를 hyperparameter와 함께 유지하면서 아래의 design 변경들을 진행합니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화2-큰-틀에서의-구조-변경">변화2: 큰 틀에서의 구조 변경<a class="hash-link" href="#변화2-큰-틀에서의-구조-변경" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="stage-compute-ratio를-3-3-9-3으로-변경">Stage Compute Ratio를 (3, 3, 9, 3)으로 변경<a class="hash-link" href="#stage-compute-ratio를-3-3-9-3으로-변경" title="Direct link to heading">​</a></h3><p>기존 ResNet의 stage별 디자인은 매우 경험적으로 결정되었습니다.</p><img class="figCenter__7gH" src="/assets/images/figure1-imagenet1k-4c92a355f2805984792477b06d558009.png" alt="imagenet1k"><p>4번째 stage에 layer가 많기 때문에(ResNet50 기준 6), object detection 등의 downstream task에 접목되기 위한 backbone으로 많이 쓰일 수 있었고, 특히 이 때의 feature map 사이즈가 14 x 14 이기에 detector head로서의 역할도 겸할 수 있었습니다. Swim Transformer는 이와는 비슷하나 조금 다른 stage ratio를 보여주는데, 작은 모델의 경우 (1, 1, 3, 1), 큰 모델의 경우 (1, 1, 9, 1)의 비율을 보여줍니다.</p><p>저자는 Swim-T와의 FLOPs를 유사한 비율로 가져가기 위해, 기존의 stage ratio (3, 4, 6, 3)을 (3, 3, 9, 3)으로 변경했습니다.</p><p>이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 78.8% -&gt; 79.4 (+0.6%, 누적: +3.3%)</li></ul><p>Network design space에 관한 연구들을 살펴보면, 이보다 충분히 더 좋은 stage compute ratio가 있을 수도 있다고 저자는 말합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="patch를-만드는-첫-stem-layer을-convks4-stride4로-변경-non-overlapping-conv">Patch를 만드는 첫 stem layer을 Conv(ks=4, stride=4)로 변경 (non-overlapping conv.)<a class="hash-link" href="#patch를-만드는-첫-stem-layer을-convks4-stride4로-변경-non-overlapping-conv" title="Direct link to heading">​</a></h3><p>Stem cell design은 아키텍쳐 가장 처음에 input image를 어떻게 처리해줄 지를 담고 있습니다. ResNet에서는 kernel size 7x7, stride 2 의 Conv layer(2x downsample)과 max-pooling(2x downsample)을 통해 4x downsample 시키는 stem cell design을 사용했습니다. vision Transformer 들은 이보다도 더 과감하게 patch를 만드는 전략을 사용하는 데, 14x14 또는 16x16의 아주 큰 kernel size를 가진 Conv layer를 겹치는 부분이 없게끔(kernel size와 stride가 동일) 설정합니다. Swim Transformer의 경우, 이 design에 추가로 multi-stage design을 위해 4x4 patch size의 layer를 별개로 두었습니다.</p><p>저자는 ResNet-style stem cell로서 kernel size 4x4, stride 4로 설정한 conv. layer(4x downsample)를 stem cell design으로 선정했습니다.</p><p>이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 79.4% -&gt; 79.5% (+0.1%, 누적: +3.4%)</li></ul><p>ResNet에서 patch를 만드는 방법에 있어서는 이렇게 ViT 등에서 보여준 더 쉬운 방법들을 사용하는 것으로 교체 가능할 수 있음을 시사합니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화3-resnext-아이디어-적용하기">변화3: ResNeXt 아이디어 적용하기<a class="hash-link" href="#변화3-resnext-아이디어-적용하기" title="Direct link to heading">​</a></h2><p>ResNeXt 에서는 bottleneck block에서 grouped convolution을 사용함으로서, FLOPs를 줄이고 network width(# channel in hidden layers) 를 늘릴 수 있었습니다.</p><p>저자는 grouped convolution의 맥락에서 depthwise convolution을 사용합니다. Depthwise convolution은 per-channel로 self-attention의 weighted sum과 동일한 역할을 하게 되는데요. 즉, spatial dimension으로만 정보를 섞는 역할을 합니다.</p><p>이를 통해 FLOPs는 효과적으로 줄이면서도 정확도는 더욱 얻을 수 있습니다. Width를 확장할 수 있게 된 만큼, width를 Swim Transformer를 따라 64에서 96으로 늘리게 되었다. 그리고 이를 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 79.5% -&gt; 80.5% (+1.0%, 누적: +4.4%)</li></ul><img class="figCenter__7gH" src="/assets/images/figure2-block-0c4373a08e91dae993b0c579624d7da8.png" alt="block"><p>(a): ResNeXt block / (b): inverted bottleneck / (c): position switch of depthwise conv layer</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화4-inverted-bottleneck-적용하기">변화4: Inverted Bottleneck 적용하기<a class="hash-link" href="#변화4-inverted-bottleneck-적용하기" title="Direct link to heading">​</a></h2><p>Transformer에서의 inverted bottleneck은 MLP block 중 hidden layer의 dimension이 input dimensionqhek 4배 크게 디자인 된 것을 말합니다. ConvNet에서도 inverted bottleneck은 MobileNetV2에서 등장한 이후 두루 쓰이고 있는데, layer의 형태만 다를 뿐 Transformer에서 사용하는 inverted block과 유사하다고 볼 수 있습니다. 이를 반영하면, 위 figure 내 (a)에서 (b)로의 변화라고 볼 수 있습니다. Depthwise convolution(보라색)의 FLOPs 는 늘어나지만 전체 네트워크의 FLOPs는 줄어드는데, 이는 downsampling을 진행하는 residual block 내의 1x1 conv layer (shortcut layer) 의 FLOPs가 줄어들기 때문입니다(layer 별: (384 -&gt; 384) 1x1 conv -&gt; (96 -&gt; 96) 1x1 conv).</p><p>저자는 inverted bottleneck 구조를 적용했고, 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 80.5% -&gt; 80.6% (+0.1%, 누적: +4.5%)</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화5-큰-사이즈의-kernel을-이용하기">변화5: 큰 사이즈의 Kernel을 이용하기<a class="hash-link" href="#변화5-큰-사이즈의-kernel을-이용하기" title="Direct link to heading">​</a></h2><p>Vision Transformer에서 사용하는 self-attention은 non-local한 특성을 가지고 있어, 사실상 receptive field가 global하다고 볼 수 있습니다. 이와는 반대로 ConvNet에서는 GPU의 특성을 고려하여 효율적인 연산을 위해 VGG network를 시작으로 3x3의 작은 사이즈의 kernel 사이즈를 사용했습니다. Swim Transformer는 self-attention을 사용하는 데에 있어 local window를 적용하는데, 여기서는 kernel size를 최소 7x7 로 잡아서 사용했었습니다. 저자는 ConvNet에도 large kernel size를 적용해보고자 합니다.</p><p>우선 kernel size를 키우기 위해서는 depthwise conv. 의 위치를 1x1 conv. 앞으로 가져와야 합니다. 이는 Transformer에서 MSA(Multi-head Self Attention)가 MLP layer 앞에 있는 구조와 유사하게 가져가기 위함입니다.</p><p>이 상태에서 저자는 kernel size를 3, 5, 7, 9, 11 등의 크기로 바꿔가면서 실험을 진행했습니다. Kernel size가 달라진다고 FLOPs에 미치는 영향은 미미합니다. 저자들의 실험을 통해 7x7보다 큰 kernel에서는 성능이 더 증가하지 않고 saturate 되는 것을 확인했습니다.</p><p>이를 통해 inverted bottleneck까지 적용했을 때와 동일한 accuracy를 보이면서도 FLOPs는 4.6G에서 4.2G로 낮출 수 있었습니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화6-activation-normalization-layer-변경하기">변화6: Activation, Normalization Layer 변경하기<a class="hash-link" href="#변화6-activation-normalization-layer-변경하기" title="Direct link to heading">​</a></h2><img class="figCenter__7gH" src="/assets/images/figure3-block-design-60ac24931c3b5b08d1bcb0b361c1debf.png" alt="block-design"><p>가장 우측 ConvNeXt block이 저자가 최종적으로 디자인한 형태입니다. 아래 내용을 모두 확인하시고 저 block에 해당하는 내용들이 잘 반영되었는지 확인해보세요😊.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="relu를-gelu로-변경">ReLU를 GELU로 변경<a class="hash-link" href="#relu를-gelu로-변경" title="Direct link to heading">​</a></h3><p>ConvNet에서는 아직도 ReLU가 두루 쓰이고, original Transformer 논문에서도 ReLU를 사용하지만, 그 이후 등장한 NLP Transformer(BERT, GPT-2)와 ViT에서는 GELU(Gaussian Error Linear Unit)을 많이 사용합니다.</p><p>이를 적용했을 때, 별도의 성능 향상은 없었지만, GELU가 ConvNet에도 적용될 수 있음을 보여준 변화점이라고 볼 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="activation-function을-적게-쓰기">Activation Function을 적게 쓰기<a class="hash-link" href="#activation-function을-적게-쓰기" title="Direct link to heading">​</a></h3><p>Transformer block에서도 MLP block 내 activation 1번 쓰이는 것 외로는 쓰이지 않습니다. 이는 1x1 conv. 뒤에까지 activation을 붙이는 ConvNet에서의 행태와는 매우 다릅니다. 저자는 Transformer block design을 따라, residual block 내에 모든 GELU activation을 지우고, 1x1 layer 사이에 activation 하나만 남겨두었습니다. 이와 같은 변화를 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 80.6% -&gt; 81.3% (+0.7%, 누적: +5.2%)</li></ul><p>이 성능은 Swim-T와 동일한 성능입니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="normalization도-적게-쓰기">Normalization도 적게 쓰기<a class="hash-link" href="#normalization도-적게-쓰기" title="Direct link to heading">​</a></h3><p>Transformer에서는 normalization layer도 적게 사용합니다. 저자는 이를 따라 1x1 conv 앞에 하나의 BN(Batch Normalization)만을 놔두고, 나머지 normalization layer를 지웠습니다. 이를 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 81.3% -&gt; 81.4% (+0.1%, 누적: +5.3%)</li></ul><p>재밌는 점은 이러한 변화를 통해 Transfomer보다도 오히려 normalization layer가 적어졌습니다. 이는 Transformer layer는 block 초입에 LN((Layer Normalization) 을 취하는데, 저자는 BN을 붙인다고 성능이 좋아지지 않아서 그냥 지웠다고 합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="batch-normalization-대신-layer-normalization">Batch Normalization 대신 Layer Normalization<a class="hash-link" href="#batch-normalization-대신-layer-normalization" title="Direct link to heading">​</a></h3><p>많은 vision task에서 BN이 사랑받고 있는 것은 사실인데요. 하지만, Transformer에서는 이미 LN을 쓰면서 좋은 성능을 보인 경우가 많았습니다. 저자는 이 상태에서 LN을 적용해서도 학습하는 데 무리가 없음을 확인했고 오히려 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 81.4% -&gt; 81.5% (+0.1%, 누적: +5.4%)</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="separate-downsampling-layer-사용">Separate Downsampling Layer 사용<a class="hash-link" href="#separate-downsampling-layer-사용" title="Direct link to heading">​</a></h3><p>이 내용은 block 내부에 반영된 내용이 아니라, stage 넘어가면서 downsample을 할 때에 해당하는 내용입니다. 위 diagram에는 드러나지 않는 부분이니 참고해주세요~<br>
<!-- -->Swim Transformer가 patch merging을 할 때 2x2 neighborhood patch들의 channel을 concat하여(4C) 이에 대해 2C 의 channel size가 되도록 내뱉습니다. 이를 Conv. 로 표현하면 channel size가 2배가 되고, kernel size는 2x2, stride 2인 것입니다. 저자는 ResNet에서 kernel size 3x3, stride 2, padding 1 로 마치 공식처럼 쓰이던 downsample하는 conv. layer 대신에 Swim Transformer에서의 downsample 방식을 사용합니다. 그냥 이를 대치시키면 학습이 diverge 되는데, block-level에서 spatial resolution이 변경되는 지점에 LN을 적당히 넣어주면 이에 대해 학습이 stabilize 되는 것을 찾아냅니다. 이는 Swim Transformer도 적용된 방식입니다.</p><p>그리고 이를 통해 최종적으로 ConvNeXt는 아래의 성능 향상을 보였습니다.</p><ul><li>ResNet-50: 81.5% -&gt; 82.0% (+0.5%, 누적: +5.9%)</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="training-details">Training Details<a class="hash-link" href="#training-details" title="Direct link to heading">​</a></h2><p>저자는 본 모델을 어떻게 학습했는 지 Hyperparameter와 학습 세팅도 함께 보여주고 있습니다.<br>
<!-- -->Augmentation, optimizer 등 여러 부분에서 등장한 최신의 학습 방법론들을 한 곳에 모아둔 느낌입니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="pre-training-settings">(Pre-)Training Settings<a class="hash-link" href="#pre-training-settings" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/figure4-pretrain-abd6f5f77e98f8c2066000329b278825.png" alt="pretrain"><h3 class="anchor anchorWithStickyNavbar_mojV" id="finetuning-settings">Finetuning Settings<a class="hash-link" href="#finetuning-settings" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/figure5-finetune-062a17c7c12e0d895436abd6ecdcb8a2.png" alt="finetune">]]></content>
        <author>
            <name>Hyoung-Kyu Song</name>
            <uri>https://github.com/deepkyu</uri>
        </author>
        <category label="paper-review" term="paper-review"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Activate or Not: Learning Customized Activation]]></title>
        <id>acon</id>
        <link href="https://deepkyu.me/papers/acon"/>
        <updated>2021-07-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Swish와 ReLU와의 관계를 설명하고, 학습이 가능한 활성함수를 소개합니다.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2009.04759" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2009.04759-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/nmaac/acon" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Ma, Ningning, et al. "Activate or Not: Learning Customized Activation."<br>
<!-- -->Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</p></blockquote><br><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</h5></div><div class="admonition-content"><p>업무 중에 진행된 논문 리뷰로 <a href="https://mindslab-ai.github.io" target="_blank" rel="noopener noreferrer">마인즈랩 Brain팀 Tech Blog</a>에서도 확인하실 수 있습니다.</p></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="contribution">Contribution<a class="hash-link" href="#contribution" title="Direct link to heading">​</a></h2><ul><li>Activation function들에 대해 기존 Maxout family에 해당하는 일반화를 넘어 <strong>ACON Family</strong>라는 개념으로 확장하여 일반화를 합니다.</li><li>이를 통해 ACON Family에서 각 activation을 결정 짓는 parameter 자체를 learnable하게 하여 <strong>acon</strong> 이라는 activation을 새롭게 제시합니다.</li><li>기존 Swish 는 NAS로 찾은 activation으로서, 더 좋다는 것만 알 뿐, 왜 좋은지를 몰랐는데, <strong>ACON Family</strong>에 대응하여 봤을 때, 이를 어느정도 설명할 수 있게 됩니다.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="먼저-알면-좋은-것들">먼저 알면 좋은 것들<a class="hash-link" href="#먼저-알면-좋은-것들" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="swish-activation-function-sup1sup">Swish Activation Function <a href="#r1"><sup>[1]</sup></a><a class="hash-link" href="#swish-activation-function-sup1sup" title="Direct link to heading">​</a></h3><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">swish</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mi>x</mi><mo>×</mo><mi>σ</mi><mo stretchy="false">(</mo><mi>β</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>β</mi><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\operatorname{swish}(x):=x \times \sigma(\beta x)=\frac{x}{1+e^{-\beta x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mord mathrm">swish</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.8769em;vertical-align:-0.7693em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><img class="figCenter__7gH" src="/assets/images/figure1_swish-c594f427f7c7dacfa81c1f6d97e10eb5.png" alt="figure1_swish"><ul><li>Linear Function과 ReLU 사이에서의 non-linearly interpolated activation을 보여줍니다.<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">β = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span> 일 경우, Linear function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">f(x) = x/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">x</span><span class="mord">/2</span></span></span></span></span> 처럼 작용하게 됩니다.</li><li>반대로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">β → ∞</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span>일 경우, Sigmoid에 해당하는 부분이 0-1 activation처럼 작용하게 되어, Swish가 ReLU처럼 작용하게 됩니다.</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">β = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>일 경우, 강화학습에서 사용되는 Sigmoid-weighted Linear Unit (SiL) function처럼 작용할 수 있습니다.</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">β</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>는 위에서 보신 것처럼 어떤 상수일 수도 있고, 모델에 따라서는 훈련 가능한 파라미터가 될 수도 있습니다.</li></ul></li><li>브레인팀 AI Scientist분들이 자주 사용하시는 Activation Function이기도 하죠 🙂</li><li>Generative Model에서도 ReLU 대신 사용하는 경우가 많이 있습니다.</li><li>최근에는 Implicit Representation Network 상에서도 Swish가 다시금 주목을 받고 있습니다.<ul><li>SIREN에서 언급하는 periodic function activation (Sine 함수 등) 보다 Swish가 더 나은 성능을 보이는 Task가 있습니다.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="sigmoid">Sigmoid<a class="hash-link" href="#sigmoid" title="Direct link to heading">​</a></h3><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x)=\frac{1}{1+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.0908em;vertical-align:-0.7693em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><ul><li>여기서는 Activation으로 시사하기보다는 수식 표현 시에 sigmoid로 묶어 표현하기 위해 확인하고 넘어가야 합니다.</li><li>Swish가 결국 <strong>input 값에 sigmoid한 것과 input 값의 곱으로 표현된다</strong>(β 를 곱하기는 하겠지만)는 것도 다시 한번 리마인드하고 넘어갑시다 😎</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="maxout-family">Maxout Family<a class="hash-link" href="#maxout-family" title="Direct link to heading">​</a></h3><ul><li>ReLU와 같은 Activation Function의 출발점에 해당하는 개념 중 하나입니다.</li><li>Goodfellow와 Bengio의 논문<a href="#r2"><sup>[2]</sup></a> 으로, Maximum을 선택하는 것으로도 임의의 Convex Function에 대해 두루 근사할 수 있음을 시사합니다.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="main-idea">Main Idea<a class="hash-link" href="#main-idea" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="aconactivationornot-activation-function">ACON(<strong>Ac</strong>tivationOrNot) Activation Function<a class="hash-link" href="#aconactivationornot-activation-function" title="Direct link to heading">​</a></h3><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">ACON-C</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>−</mo><msub><mi>p</mi><mn>2</mn></msub><mo fence="true">)</mo></mrow><mi>x</mi><mo>⋅</mo><mi>σ</mi><mrow><mo fence="true">(</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>−</mo><msub><mi>p</mi><mn>2</mn></msub><mo fence="true">)</mo></mrow><mi>x</mi><mo fence="true">)</mo></mrow><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">\operatorname{ACON-C}(x) := \left(p_{1}-p_{2}\right) x \cdot \sigma\left(\beta\left(p_{1}-p_{2}\right) x\right)+p_{2} x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mord mathrm">ACON-C</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span></span></span></span></span></div><img class="figCenter__7gH" src="/assets/images/figure4_acon-52933c29b2e15e7c34c558bf5b507adf.png" alt="figure4_acon"><p><em>ACON Activation을 사용하였을 때, 특정 Layer의 Activation이 Linear 하게 pass수도, Non-linear Activation으로 활성될 수도 있습니다</em></p><p>저자는 ACON(더 나아가서 Meta-ACON)이라고 하는 Activation Function을 제안합니다. ACON activation은 trainable한 activation으로 Neuron을 Activation할 지 안 할지를 각 Layer의 특성에 맞게 결정합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="어떻게-해서-acon-식을-도출할-수-있게-되었을까요">어떻게 해서 ACON 식을 도출할 수 있게 되었을까요?<a class="hash-link" href="#어떻게-해서-acon-식을-도출할-수-있게-되었을까요" title="Direct link to heading">​</a></h3><p>먼저 Maximum Function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>x</mi><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">max(x1, ..., xn)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span> 에 대해 smooth된 버전을 보아야 합니다. Maximum을 구한다는 것은 일반적으로 differentiable하지 않지만, 이를 smooth한 함수는 differentiable하게 됩니다.</p><p>보통 아래의 식처럼 표현합니다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>S</mi><mi>β</mi></msub><mrow><mo fence="true">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo fence="true">)</mo></mrow><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>x</mi><mi>i</mi></msub><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></msup></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">S_{\beta}\left(x_{1}, \ldots, x_{n}\right)=\frac{\sum_{i=1}^{n} x_{i} e^{\beta x_{i}}}{\sum_{i=1}^{n} e^{\beta x_{i}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.5328em;vertical-align:-0.994em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5388em"><span style="top:-2.3057em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.6897em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.994em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>이 때, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">β</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 는 switching factor로서</p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">β → ∞</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span>일 때, 주어진 함수는 Maximum Function 의 역할을 하게 됩니다.</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">β → 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span>일 때, 주어진 함수는 산술평균(Arithmetic Mean, 우리가 일반적으로 아는 평균)처럼 작동합니다.</li></ul><p>일반적으로 Neural Network에서 많이 사용하는 Activation Function들은 위에서 언급한 Maxout에 준하는 것으로 표현할 수 있습니다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>η</mi><mi>a</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>η</mi><mi>b</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">max( ηa(x), ηb(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="mord mathnormal">b</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></div><p>예를 들어, ReLU는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mi>a</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">ηa(x)=x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mi>b</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">ηb(x)=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="mord mathnormal">b</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span>인 것으로 생각하면, 이 역시 Maxout Family에 속한다고 볼 수 있습니다.<br>
<!-- -->Leaky ReLU, FReLU 등도 이러한 방식으로 접근해보면 모두 Maxout Family에 속하게 됩니다.</p><p>본 논문에서의 목표는 Maximum Function과 위 Maxout Family를 함께 사용하여, Maxout Family 각각에 상응하는 activation function들을 smooth한 함수로 근사해보는 것입니다. 위에서 Smooth된 Maximum Function을 작성할 때, 입력 값의 개수를 2개로만 한정해서 식을 전개하면 딱이겠네요!</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>S</mi><mi>β</mi></msub><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>=</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mfrac><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mrow><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>+</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mfrac><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mrow><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>β</mi><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>=</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow></msup></mrow></mfrac><mo>+</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow></msup></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>=</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>σ</mi><mrow><mo fence="true">[</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow><mo>+</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>σ</mi><mrow><mo fence="true">[</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo>⋅</mo><mi>σ</mi><mrow><mo fence="true">[</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow><mo>+</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{l}S_{\beta}\left(\eta_{a}(x), \eta_{b}(x)\right) \\=\eta_{a}(x) \cdot \frac{e^{\beta \eta_{a}(x)}}{e^{\beta \eta_{a}(x)}+e^{\beta \eta_{b}(x)}}+\eta_{b}(x) \cdot \frac{e^{\beta \eta_{b}(x)}}{e^{\beta \eta_{a}(x)}+e^{\beta \eta_{b}(x)}} \\=\eta_{a}(x) \cdot \frac{1}{1+e^{-\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)}}+\eta_{b}(x) \cdot \frac{1}{1+e^{-\beta\left(\eta_{b}(x)-\eta_{a}(x)\right)}} \\=\eta_{a}(x) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x) \cdot \sigma\left[\beta\left(\eta_{b}(x)-\eta_{a}(x)\right)\right] \\=\left(\eta_{a}(x)-\eta_{b}(x)\right) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x)\end{array}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6.6531em;vertical-align:-3.0766em"></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.5766em"><span style="top:-5.8073em"><span class="pstrut" style="height:3.0707em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span></span></span><span style="top:-4.3766em"><span class="pstrut" style="height:3.0707em"></span><span class="mord"><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0707em"><span style="top:-2.5648em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.822em"><span style="top:-2.822em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2306em"><span style="top:-2.3em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.5em"></span><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8932em"><span style="top:-2.8932em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.6944em"></span><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3496em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em"><span style="top:-2.9667em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2306em"><span style="top:-2.3em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.5em"></span><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0707em"><span style="top:-2.5648em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.822em"><span style="top:-2.822em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2306em"><span style="top:-2.3em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.5em"></span><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8932em"><span style="top:-2.8932em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.6944em"></span><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3496em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9667em"><span style="top:-2.9667em;margin-right:0.0714em"><span class="pstrut" style="height:2.5357em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.6944em"></span><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3496em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.0379em"><span class="pstrut" style="height:3.0707em"></span><span class="mord"><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.4146em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1077em"><span style="top:-3.1088em;margin-right:0.0714em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="minner mtight"><span class="mopen sizing reset-size1 size3 mtight delimcenter" style="top:0.0714em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2306em"><span style="top:-2.3em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.5em"></span><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.6944em"></span><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3496em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mclose sizing reset-size1 size3 mtight delimcenter" style="top:0.0714em"><span class="mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6438em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.4146em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1077em"><span style="top:-3.1088em;margin-right:0.0714em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span><span class="minner mtight"><span class="mopen sizing reset-size1 size3 mtight delimcenter" style="top:0.0714em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.6944em"></span><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3496em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2306em"><span style="top:-2.3em;margin-left:-0.0359em;margin-right:0.1em"><span class="pstrut" style="height:2.5em"></span><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mclose sizing reset-size1 size3 mtight delimcenter" style="top:0.0714em"><span class="mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6438em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-1.5541em"><span class="pstrut" style="height:3.0707em"></span><span class="mord"><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mclose delimcenter" style="top:0em">]</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mclose delimcenter" style="top:0em">]</span></span></span></span><span style="top:-0.3541em"><span class="pstrut" style="height:3.0707em"></span><span class="mord"><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mclose delimcenter" style="top:0em">]</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.0766em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span></span></span></span></span></span></span></div><p>즉, Smooth된 Maximum Function에 대입해서 전개해보면</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnalign="left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>S</mi><mi>β</mi></msub><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo>⋅</mo><mi>σ</mi><mrow><mo fence="true">[</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>η</mi><mi>a</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow><mo>+</mo><msub><mi>η</mi><mi>b</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{l}S_{\beta}\left(\eta_{a}(x), \eta_{b}(x)\right) =\left(\eta_{a}(x)-\eta_{b}(x)\right) \cdot \sigma\left[\beta\left(\eta_{a}(x)-\eta_{b}(x)\right)\right]+\eta_{b}(x)\end{array}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em">β</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">[</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mclose delimcenter" style="top:0em">]</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span></span></span></span></span></span></span></div><p>이 될 것입니다!</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="acon-안에-swish-있다-">ACON 안에 Swish 있다 😉<a class="hash-link" href="#acon-안에-swish-있다-" title="Direct link to heading">​</a></h3><p>자, 그럼 아까 언급한 Maxout Family에 준하여 여러 Activation을 표현할 수 있었다면, 각각을 Smooth된 Maximum Function에 해당하도록 전개를 해볼까요?</p><img class="figCenter__7gH" src="/assets/images/figure5_maxout_family_acon_family-6bbc5639cceda0ba510cd7bbb7336690.png" alt="figure5_maxout_family_acon_family"><p>ReLU의 smooth되는 버전이 Swish라는 건 직관으로도 많이들 이해하고 있었는데요. 이 식에서 보듯이 Smooth된 Maximum Function에 대입해서 전개해보면, 바로 Swish 식이 나오게 됩니다. 저자는 이를 통해 Swish가 ReLU의 Smooth Approximation임을 표현할 수 있게 된다고 말합니다.</p><p>또한, Leaky ReLU의 상위 호환이기도 한 PReLU(Parametric ReLU, 음수 부분의 기울기 값이 learnable함)도 살펴보면, 역시 Smooth되는 함수로 대응하는 것을 찾을 수 있습니다. (아 이 때 PReLU에 대응하려면, p &lt; 1 인 걸로 한정해서 생각해봐요 우리 🙂)</p><p>그리고 마지막으로 각 선형 함수의 가중치(Cartesian 좌표계 상은 기울기겠죠?)가 p1, p2로 표현하면 가장 일반화된 표현일텐데요 (p1 ≠ p2). 여기에 각각 Maxout Family, ACON Family를 대응해보면 일반화된 식이 나옵니다. 위에서 언급한</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">ACON-C</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>−</mo><msub><mi>p</mi><mn>2</mn></msub><mo fence="true">)</mo></mrow><mi>x</mi><mo>⋅</mo><mi>σ</mi><mrow><mo fence="true">(</mo><mi>β</mi><mrow><mo fence="true">(</mo><msub><mi>p</mi><mn>1</mn></msub><mo>−</mo><msub><mi>p</mi><mn>2</mn></msub><mo fence="true">)</mo></mrow><mi>x</mi><mo fence="true">)</mo></mrow><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">\operatorname{ACON-C}(x):=\left(p_{1}-p_{2}\right) x \cdot \sigma\left(\beta\left(p_{1}-p_{2}\right) x\right)+p_{2} x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop"><span class="mord mathrm">ACON-C</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose delimcenter" style="top:0em">)</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span></span></span></span></span></div><p>이 이렇게 유도되게 되는 것이죠!</p><p>사실 Maxout Family에서 비교하게 되는 두 함수는 위에서처럼 단순하지 않을 수도 있습니다. 각각이 복잡해질수록 더 많은 함수들을 표현할 수 있게 되죠. 다만, 저자는 이 Maxout Family를 ACON Family로 바꿨을 때(즉, Smooth Maximum Function으로 근사했을 때)의 효과를 보는 데에 연구를 집중했다고 해요. 향후 연구에서 더 전체적인 Scope에서의 비교가 있기를 기대해봅니다!</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="acon의-특성">ACON의 특성<a class="hash-link" href="#acon의-특성" title="Direct link to heading">​</a></h3><p>ACON에 특정 값을 대입해서 한번 살펴볼까요?</p><img class="figCenter__7gH" src="/assets/images/figure6_acon_example-6ae8fd9579ecab389a590921d8fe9baf.png" alt="figure6_acon_example"><p>p1=1.2, p2=-0.8일 때 ACON-C에 대응하는 식을 여러 β값에 대해 표현한 graph입니다.</p><ul><li>β가 클 때는, maximum function처럼 반응하여 비선형적인 특성을 갖게 되고요.</li><li>β가 0에 가까울 때는 mean function에 근사되어 선형적인 특성을 갖네요.</li></ul><img class="figCenter__7gH" src="/assets/images/figure7_acon_property-3dfc87db450c4d8bc7d4737d32d37e6a.png" alt="figure7_acon_property"><p>ACON Activation과 이에 대한 도함수(derivative)를 보여주는 그림입니다.</p><ul><li>왼쪽: β가 fixed 되어 있을 때, p1, p2 계수에 따라 어떻게 Activation function이 달라지는 지를 보여줍니다.</li><li>가운데: β 값이 달라짐에 따라 ACON의 도함수가 변화하게 되고 이를 통해 β의 역할을 짐작해볼 수 있습니다.</li><li>오른쪽: β가 fixed 되어 있을 때, p1, p2 계수에 따라 ACON의 도함수가 어떻게 변하는 지를 보여줍니다.</li></ul><p>ACON의 도함수를 보면서 아래와 같은 사실을 알 수 있어요.</p><ul><li>p1, p2는 각각 Upper/Lower Bound에 해당하는 값을 결정하게 됩니다.</li><li>β 값은 도함수 상에서 p1, p2에 의해 결정된 Upper/Lower Bound에 얼마나 빠르게 근사되는 지를 결정하게 됩니다.</li></ul><p>Swish에서는 Hyperparameter β만이 Upper/Lower Bound에 얼마나 빨리 근사되는 지를 결정하게 되는데요. ACON에서는 p1, p2가 이 Bound 값을 결정하게 되고, 이 역시 learnable해질 수 있다는 특성이 있습니다. 이렇게 boundary가 learnable하다는 것은 optimization을 쉽게 하는 데에 필수적인 특성이고, 저자는 이 장점을 실험 결과를 통해 보여주고 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="학습에-모두-맡겨버리자-meta-acon">학습에 모두 맡겨버리자! Meta-ACON<a class="hash-link" href="#학습에-모두-맡겨버리자-meta-acon" title="Direct link to heading">​</a></h3><p>Meta-ACON은 β 자체를 Learnable한 parameter로 놔두는 것에서 더 나아가, Layer에 입력되는 feature map으로부터 FC Layers를 거쳐 estimation 되도록 만든 것입니다.</p><img class="figCenter__7gH" src="/assets/images/figure8_meta_acon_distribution-9280a99c296055eadbabf5fa63432552.png" alt="figure8_meta_acon_distribution"><p>ACON과 meta-ACON을 비교한 도식입니다. ResNet50의 마지막 BottleNeck Layer에서의 activation을 비교한 것입니다. 여기에서 7개의 sample을 임의로 추출해봤습니다.</p><ul><li>ACON에서 추출할 경우, 파란 히스토그램에 해당하는데요. 7개의 sample이 동일한 β distribution을 나타냅니다.</li><li>Meta-ACON에서는 7개의 sample이 서로 다른 β distribution을 보여주게 됩니다. 여기서 β 값이 작을수록, 선형적으로(linear) 반응하는 것이고, β 값이 클 수록 비선형적(non-linear)으로 반응하고 있는 것입니다.</li></ul><p>Code Snippet으로 보면 아래와 같습니다. 본 Snippet은 저자의 <a href="https://github.com/nmaac/acon" target="_blank" rel="noopener noreferrer">official github</a>에서 발췌했으며, 해당 Repository에서 자세한 코드를 확인하실 수 있습니다.</p><script src="https://gist.github.com/deepkyu/1616637a06e1b00534a7557c35ad2209.js"></script><script src="https://gist.github.com/deepkyu/77b2e5acd98969fdb21ea22198954ad5.js"></script><h3 class="anchor anchorWithStickyNavbar_mojV" id="결과">결과<a class="hash-link" href="#결과" title="Direct link to heading">​</a></h3><table><thead><tr><th>ImageNet Classification Result</th><th>Accuracy Improvements</th></tr></thead><tbody><tr><td><img loading="lazy" alt="figure9_result1.png" src="/assets/images/figure9_result1-de49dfef49f0671fc762fa4e8d1941ff.png" width="1036" height="672"></td><td><img loading="lazy" alt="figure10_result2.png" src="/assets/images/figure10_result2-b99da14b48ff976af5527bf034f3f47f.png" width="1266" height="692"></td></tr></tbody></table><p>ImageNet Classification에 대한 ShuffleNetV2 기준 결과를 살펴보면, 학습 속도도 빠를 뿐더러, Meta-ACON을 사용했을 때 Error rate가 낮아지는 것을 확인할 수 있습니다. 또한, 전반적으로 모델 사이즈가 커질 수록, Meta-ACON을 사용할 수록 Accuracy 향상이 큽니다. (Swish 대체, SENet Novelty 추가 등 대비)</p><img class="figCenter__7gH medium_sJ4m" src="/assets/images/figure12_result4-abb7730a898219a4a559abca35cdd27e.png" alt="figure12_result4"><img class="figCenter__7gH medium_sJ4m" src="/assets/images/figure13_result5-d40da392ea4fe2155921a2b1e1544ad5.png" alt="figure13_result5"><p>이렇게 Meta-ACON은 다른 activation 대비 ImageNet Classification에서 좋은 성능을 보여주고 있습니다. 또한, 제한적이기는 하나, 특정 backbone에 대해서 Object Detection 및 Semantic Segmentation에 있어서도 다른 activation function을 사용할 때보다 좋은 성능을 보여줍니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="마무리">마무리<a class="hash-link" href="#마무리" title="Direct link to heading">​</a></h3><p>이렇게 오늘은 ReLU와 Swish 간의 관계를 통해 새로운 Activation Function들이 포진되어 있을만한 일반화된 식을 찾고(ACON Family), 이를 기반으로 Trainable한 Activation Function을 새로 만나볼 수 있었습니다.<br>
<!-- -->사실 이렇게 훈련 가능한 파라미터를 가진 Activation Function이 ACON만 처음인 것은 아닙니다. 또한, 여러 Sub-task에 대해 범용적으로 사용될 수 있는 Activation Function일지는 미지수이기도 하고요. 특히 모델 경량화 등 어느 한편에서는 Non-linear Activation Function마저 Bottleneck으로 짚고 넘어가는 실정이기에<a href="#r3"><sup>[3]</sup></a>, 모든 목적을 만족시킬만한 새로운 활성 함수를 찾은 연구는 아닙니다. 다만, 식에 대한 간단한 정리로 ReLU와 Swish 간의 관계를 보임과 동시에, 새로운 Activation Family를 제시했다는 데에 의의가 있는 논문이었습니다.</p><p>CVPR 2021에서 이러한 논문도 발표된다는 것을 함께 공유하고 싶어 간략하게나마 리뷰를 진행해봤습니다 👍</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="references--함께-읽으면-좋은-논문들">References (+ 함께 읽으면 좋은 논문들)<a class="hash-link" href="#references--함께-읽으면-좋은-논문들" title="Direct link to heading">​</a></h3><a name="r1"></a><ol><li>Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. "Searching for activation functions." arXiv preprint arXiv:1710.05941 (2017). <a href="https://arxiv.org/abs/1710.05941" target="_blank" rel="noopener noreferrer">[paper]</a> </li></ol><a name="r2"></a><ol start="2"><li>Goodfellow, Ian, et al. "Maxout networks." International conference on machine learning. PMLR, 2013. <a href="http://proceedings.mlr.press/v28/goodfellow13.html" target="_blank" rel="noopener noreferrer">[paper]</a></li></ol><a name="r3"></a><ol start="3"><li>Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. "TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning
." Part of Advances in Neural Information Processing Systems 33 (NeurIPS 2020) <a href="https://proceedings.neurips.cc//paper_files/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html" target="_blank" rel="noopener noreferrer">[paper]</a></li></ol>]]></content>
        <author>
            <name>Hyoung-Kyu Song</name>
            <uri>https://github.com/deepkyu</uri>
        </author>
        <category label="paper-review" term="paper-review"/>
        <category label="activation" term="activation"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anycost GANs for Interactive Image Synthesis and Editing]]></title>
        <id>anycost</id>
        <link href="https://deepkyu.me/papers/anycost"/>
        <updated>2021-05-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[하나의 GAN Training으로 여러 해상도의 이미지를 생성할 수 있게 한 연구를 소개합니다.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/mit-han-lab/anycost-gan" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Lin, Ji, et al. "Anycost gans for interactive image synthesis and editing."
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.</p></blockquote><p>Song Han (Network Compression 최강) + Jun-Yan Zhu (GAN 최강)</p><br><h2 class="anchor anchorWithStickyNavbar_mojV" id="contribution">Contribution<a class="hash-link" href="#contribution" title="Direct link to heading">​</a></h2><img class="figCenter__7gH" src="/assets/images/fig1_model-general-62740d0c269966dfdc9a85ef63ebe71e.png" alt="model-general"><ul><li>Intermediate Layer도 Generation의 결과물이 될 수 있다는 부분을 시사한 연구다.</li><li>Low Resolution Preview를 추출하는 데에 별도의 네트워크를 구성하거나, 속도 측면에서 손해보는 것 없이 구현을 했다는 점에서 앞으로의 쓰임새가 기대되는 논문이다.</li><li>Depth Search를 하는 방식으로 일종의 네트워크 경량화를 해낸다는 아이디어는 기존에도 많이 있었는데, 이를 Image Generation에 적용한 몇 안 되는 논문일 것이고 G-conditioned Dsicriminator를 통해 Discriminator를 상황에 맞게 학습시키는 방법은 앞으로도 많이 차용할 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="main-method">Main Method<a class="hash-link" href="#main-method" title="Direct link to heading">​</a></h2><img class="figCenter__7gH" src="/assets/images/fig2_architecture-314f2800e56a423ec81bd3fea41efc25.png" alt="architecture"><p><strong>좌측: Generator의 Decoder, 우측: Discriminator</strong></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="sampling-based-multi-resolution-training">Sampling-based Multi-resolution Training<a class="hash-link" href="#sampling-based-multi-resolution-training" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/fig3_table1-1b32ebd7dec52b2a39f48b5fce944053.png" alt="table"><p>MSG-GAN은 Generator를 학습하는 데 있어 다른 해상도를 지원하도록 학습하기 위해 모든 해상도에 대한 Discriminator를 각각 만들어서 이를 모두 사용했다. 하지만, 이러한 All-resolution training 방식은 FFHQ 처럼 큰 데이터셋에 대해 각각을 single-resolution으로 학습하는 것보다 좋지 않은 결과를 만들었다.</p><p>본 모델은 이를 Multi-resolution으로 학습할 수 있게 sampling 하는 방향으로 진행한다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="adaptive-channel-training">Adaptive Channel Training<a class="hash-link" href="#adaptive-channel-training" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/fig4_adaptive_channel_training-294e78a2a6b7aa246008ca798a35f33d.png" alt="adaptive_channel"><p>바라는 것(전제): Important Channel들의 경우 sampling 하는 동안 보존될 것이다.</p><p>방식</p><ul><li>이전 stage로부터 모델 Initialize를 진행한다.</li><li>이 때, Kernel Magnitude를 기준으로 High to Low로 sorting</li><li>0.25, 0.5, 0.75, 1 x의 비율 중 하나로 random하게 sampling 하도록 함.</li></ul><p>이를 통해 모든 sub-network가 fewer channel만 가지고 있어도 실재적인 이미지를 생성하도록 했다. 다만, 이게 실제 full network로 생성한 이미지와는 다를 수 밖에 없었는데 이를 위해 consistent를 유지해주는 loss를 추가했다. (MSE (L2), LPIPS (Perceptual) Loss 기반)</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="generator-conditioned-discriminator">Generator-conditioned discriminator<a class="hash-link" href="#generator-conditioned-discriminator" title="Direct link to heading">​</a></h3><p>가장 쉬운 방법은 Discriminator의 channel 수를 Generator에 맞춰 가며 줄이는 방식일 것이다. 다만, 이러한 방식은 Uniform한 channel ratio에만 적용되고, channel 수가 작아질 때에만 성능 향상에 도움이 되는 방법들이었다.</p><p>그래서 이를 대신해서 FC Layer를 추가하여 learnable하게 구조를 만들었다.</p><p>우선 Channel Configuration을 Encoding 하도록 한다. One-hot Encoding으로 위에 0.25, 0.5, 0.75, 1 중 각 resolution에 해당하는 layer가 무엇을 선택했는 지를 나타내는 방식이다. 여기서 이를 G Architecture Vector (g_arch)라고 부른다. 그리고 이를 FC에 실어 per-channel modulation을 할 수 있도록 Discrminator에 넘겨 준다. Real Image를 Discriminator에 넣을 때에는 g_arch를 random하게 부여한다.</p><p>Stablized Training 하기 위해 이를 전체 Discriminator에 적용하는 것은 아니고, Discriminator의 마지막 두 Block에만 적용하도록 했다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="result">Result<a class="hash-link" href="#result" title="Direct link to heading">​</a></h2><img class="figCenter__7gH" src="/assets/images/fig5_result_graph-e5be35f19bf280b9fb03dcb0be25c9f5.png" alt="fig5_result_graph"><img class="figCenter__7gH" src="/assets/images/fig6_channel_diff_result-6813eba3a496f2451b7e3f0ea187e767.png" alt="fig6_channel_diff_result"><img class="figCenter__7gH" src="/assets/images/fig7_LSUN_result-c4596f6685b2f5030854437d9d36c5a1.png" alt="fig7_LSUN_result"><img class="figCenter__7gH" src="/assets/images/fig8_result_multiresolution-9f60088fd608b5b216ad3ea11462eaeb.png" alt="fig8_result_multiresolution"><img class="figCenter__7gH" src="/assets/images/fig9_latent_space_result-3ffc95163028fe2cab41765ba082a20c.png" alt="fig9_latent_space_result">]]></content>
        <author>
            <name>Hyoung-Kyu Song</name>
            <uri>https://github.com/deepkyu</uri>
        </author>
        <category label="paper-review" term="paper-review"/>
        <category label="gan" term="gan"/>
        <category label="compression" term="compression"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Augmentation for Data-Efficient GAN Training]]></title>
        <id>diffaugment</id>
        <link href="https://deepkyu.me/papers/diffaugment"/>
        <updated>2021-05-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[적은 데이터로 효율적으로 GAN 학습하는 방법을 확인해봅니다.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/mit-han-lab/data-efficient-gans" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Zhao, Shengyu, et al. "Differentiable augmentation for data-efficient gan training."<br>
<!-- -->Advances in Neural Information Processing Systems 33 (2020): 7559-7570.</p></blockquote><p>참고로 Song Han 연구실은 Neural Network Compression 분야에 있어 Top을 달리고 있는 연구실이다. 후에 소개할 AnycostGAN에서도 DiffAugment가 언급이 된다.</p><p>Han Lab은 기존에는 방법론에 있어 Network Pruning, KD(Knowledge Distillation) 등에 집중을 했고, TinyTL 등 Activation에 대한 경량화도 연구를 진행했다. 다만, 대부분 Task가 Image Recognition에 국한되어 있었다. Song Han은 CVPR2020에서 GAN Compression이라는 논문을 통해 Image Generation에 대한 Compression 논문을 시작으로, GAN 관련 연구를 지속해서 이어오고 있다. </p><h2 class="anchor anchorWithStickyNavbar_mojV" id="배경-지식">배경 지식<a class="hash-link" href="#배경-지식" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="기존의-regularization">기존의 Regularization<a class="hash-link" href="#기존의-regularization" title="Direct link to heading">​</a></h3><p>GAN Training 자체가 굉장히 Unstable한 Process이기 때문에, 추가적인 Regularization이 많이 필요하다. 지금까지 여러가지 Regularization 방식들이 등장했다.</p><ul><li>Instance Noise</li><li>Jensen-Shannon Regularization</li><li>Gradient Penalty</li><li>Spectral Normalization</li><li>Adversarial Defense Regularization</li><li>Consistency Regularization</li></ul><p>이러한 Regularization Method들은 Input 이미지에 대해 대응하는 것이지, augmentation에 대해 소화할 수 있는 방법들이 아니다.</p><p>저자는 여러 가지 Augmentation에 대해서도 잘 working 하는 Discriminator를 구축하고자 했다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="d는-지금까지-overfitting-해왔다">D는 지금까지 Overfitting 해왔다<a class="hash-link" href="#d는-지금까지-overfitting-해왔다" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/d_overfitting-0fde96f1fa39bd9365ce7230b620fffb.png" alt="d_overfitting"><p>BigGAN을 적은 데이터에 대해 학습했을 때, 학습을 하다가 collapse하는 걸 보여주는 Figure다. 왼쪽에서 보면 빨간색(CIFAR-10 10%만 가지고 학습) 그래프가 튀어버리는 걸 볼 수 있다. 이게 왜 그럴까 하고 Discriminator를 보면, 아니나 다를까 Training Accuracy가 빠르게 학습이 돼서 그렇다.</p><p>그런데 D에 대해 각 Iteration에 대한 Validation Acc.를 측정해보면, mode collapse가 되었다는 걸 볼 수 있다. 즉, D가 training set 을 memorize 해왔고, 이로 인해 generalize가 안됐다는 걸 보여준다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="training-method">Training Method<a class="hash-link" href="#training-method" title="Direct link to heading">​</a></h2><img class="figCenter__7gH medium_sJ4m" src="/assets/images/training_method-182e7e89d5035fa3118b77161271b12b.png" alt="training_method"><p>DiffAugment의 학습 방법을 보여주는 그림.</p><p>원본 이미지(x), 생성한 이미지(G(z)) 모두에 augmentation(T)을 적용한다.</p><p>Augmentation Senario에 따라서 여러 가지 Case로 나눌 수 있다.</p><p>Augment Reals Only: Real 이미지에 대해만 Augmentation을 진행함 (i만 진행.)
→ Augmentation 한 걸 그대로 모방하여 생성한다.</p><p>Augment D Only: Discriminator에 넣는 Input들에 대해 진행함 (i, ii 만 진행. iii는 그대로)
→ Unbalanced Optimization 에 의해 Diverge 해버린다.</p><p>D perfectly classifies the augmented images (both T(x) and T(G(z)) but barely
recognizes G(z) (i.e., fake images without augmentation)
이 때문에 G의 gradient update를 할 때, Discriminator가 잘 working 하지 못하면서, G에 대한 학습에 방해가 됨.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="result">Result<a class="hash-link" href="#result" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="stylegan2와-비교">StyleGAN2와 비교<a class="hash-link" href="#stylegan2와-비교" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/vs_stylegan2-f1b3f845a404ca098b35e023e87f8a99.png" alt="vs_stylegan2"><p>기존 StyleGAN2과 본 논문에서 제시하는 DiffAugment를 적용했을 때를 비교하는 Figure.<br>
<!-- -->StyleGAN2는 데이터가 작아질수록, FID, IS 값의 변화가 dramatic한데, DiffAugment는 상대적으로 적은 데이터에 대해서도 generalize되어 있다는 걸 볼 수 있다. StyleGAN2가 작은 데이터에 generalize가 안된다는 건 좀 알려진 사실이었으니, Discriminator Training Method에 집중한 ADA나 Freeze D와 비교하는 Figure를 자연스럽게 기대하게 된다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="low-shot-generation">Low-Shot Generation<a class="hash-link" href="#low-shot-generation" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/result_compare-acc36d84cc9ea455714866e23cef7a61.png" alt="result_compare"><p>Low-shot generation without pre-training.</p><p>각각 오바마 100장, 고양이 160장, 강아지 389장만을 가지고 학습을 하여 생성해낸 이미지들이다. (w/o pre-training)</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="result-1">Result<a class="hash-link" href="#result-1" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="training-performance">Training Performance<a class="hash-link" href="#training-performance" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/augmentation_type_result-e03bdd14ef6248c21157d69ba3f953af.png" alt="augmentation_type_result"><p>Augmentation 조합에 따라 달라지는 DiffAugment Training Performance (CIFAR-10 Data 100% 사용)</p><p>BigGAN 대비 Discriminator가 Validation에 있어서도 generalize 하여 학습되었다고 볼 수 있다. DiffAugment 자체는 Low Dataset에 대해서도 Generation을 안정적으로 할 수 있다는 Novelty를 가지고 있지만, 본 Figure는 BigGAN도 학습했다고 하는 데이터가 CIFAR-10 전체를 사용한 것이었으니, 해당 조건으로 Comparison을 한 것으로 해석할 수 있다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="interpolation도-가능하다">Interpolation도 가능하다<a class="hash-link" href="#interpolation도-가능하다" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/interpolation-a37d53f43c37be1bde34d1839ecf1837.png" alt="interpolation"><p>Style Space에서 Interpolation이 가능함을 보여주는 Figure.</p><p>적은 데이터였지만 (오바마, 고양이, 강아지 참조), Interpolation이 가능할 정도로 generalize하게 학습이 되었다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="model-size에-대한-분석">Model Size에 대한 분석<a class="hash-link" href="#model-size에-대한-분석" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/model_size_fid-b395637cdd2677201a79958933acd29f.png" alt="model_size_fid"><p>Model size가 FID에 얼마나 영향을 주는가에 대한 Figure</p><p>CIFAR-10 데이터 10%에 대해서만 학습을 했을 때, generalize가 될 수 있는 가를 다룬 부분이다. BigGAN 대비 channel size를 줄여가며 비교를 해봐도, 어떤 capacity든 상관없이 더 좋은 성능을 보여준다. StyleGAN2에서 사용하는 R1 Regularization 관련해서도, hyperparameter가 동일한 세팅일 때 더 나은 FID를 보여줬다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="regularization">Regularization<a class="hash-link" href="#regularization" title="Direct link to heading">​</a></h3><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</h5></div><div class="admonition-content"><p><strong>R1 Regularization</strong><br>
<!-- -->Gradient Penalty로 regularize 하는 방식으로, Discriminator의 Real Data에 대해 Penalize를 준다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>ψ</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mfrac><mi>γ</mi><mn>2</mn></mfrac><msub><mi mathvariant="normal">E</mi><mrow><msub><mi>p</mi><mi mathvariant="script">D</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo fence="true">[</mo><msup><mrow><mo fence="true">∥</mo><mi mathvariant="normal">∇</mi><msub><mi>D</mi><mi>ψ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">∥</mo></mrow><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">R_{1}(\psi):=\frac{\gamma}{2} \mathrm{E}_{p_{\mathcal{D}}(x)}\left[\left\|\nabla D_{\psi}(x)\right\|^{2}\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">ψ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.836em;vertical-align:-0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathrm">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:0.02778em">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size2">[</span></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em">∥</span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">ψ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">∥</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954em"><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size2">]</span></span></span></span></span></span></span></div><blockquote><p>when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the GAN game.</p></blockquote></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="ablation-for-augmentation">Ablation for Augmentation<a class="hash-link" href="#ablation-for-augmentation" title="Direct link to heading">​</a></h3><img class="figCenter__7gH" src="/assets/images/ablation_augmentation-6fad8d2422af1ae0b528dfa994cf5049.png" alt="ablation_augmentation"><p>여러 가지 Augmentation에 대해 FID를 확인해본 결과다.
Ablation을 진행한 끝에, color distortion, translation, cutout만 준 것으로 확인했다. (먼저 생각하고 준 건 아닌 것으로 확인했다.)
StyleGAN2의 FID이고, CIFAR-10 10% 데이터만을 가지고 Training 했을 때의 결과다.</p>]]></content>
        <author>
            <name>Hyoung-Kyu Song</name>
            <uri>https://github.com/deepkyu</uri>
        </author>
        <category label="paper-review" term="paper-review"/>
        <category label="gan" term="gan"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[FreezeD: a Simple Baseline for Fine-tuning GANs]]></title>
        <id>freezed</id>
        <link href="https://deepkyu.me/papers/freezed"/>
        <updated>2020-09-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[GAN Finetuning 시에 Discriminator를 Freezing 하면 어떻게 될까요?]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/sangwoomo/FreezeD" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Mo, Sangwoo, et al. "Freeze the discriminator: a simple baseline for fine-tuning gans."<br>
<!-- -->CVPR AI for Content Creation Workshop (2020).</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="contribution">Contribution<a class="hash-link" href="#contribution" title="Direct link to heading">​</a></h2><p>GAN을&nbsp;fine-tuning하는&nbsp;데에&nbsp;있어, discriminator의&nbsp;lower layer들(classifier&nbsp;말고&nbsp;visul repr. (=visual feature) extractor)에 대해&nbsp;freeze하고&nbsp;G/D에&nbsp;대해&nbsp;fine-tuning을&nbsp;진행하면, limited data로&nbsp;효과적인&nbsp;transfer learning을&nbsp;할&nbsp;수&nbsp;있다.</p><p>즉, image classification 등 recognition(또는 understanding)에서 사용하는 fine-tuning 방식 (visual representation은 freeze, classifier만 use_gradient)이 GAN 모델에서도 유용하게 사용될 수 있다.</p><img class="figCenter__7gH" src="/assets/images/result_graph-131a4e1493d0c89f7b52e6d88ab11441.png" alt="result_graph"><h2 class="anchor anchorWithStickyNavbar_mojV" id="background">Background<a class="hash-link" href="#background" title="Direct link to heading">​</a></h2><p>최근&nbsp;SotA GAN들은&nbsp;많은&nbsp;양의&nbsp;training data를&nbsp;요구할&nbsp;뿐만&nbsp;아니라,&nbsp;매우&nbsp;큰 computational resource를&nbsp;요구하는&nbsp;경우가&nbsp;많아,&nbsp;실질적인&nbsp;senario에&nbsp;대입하는&nbsp;데에&nbsp;어려움이&nbsp;많다.</p><p>그래서&nbsp;이를&nbsp;해결하고자&nbsp;많은&nbsp;접근법들이&nbsp;recogntion task에서의&nbsp;성공적인&nbsp;사례들을&nbsp;바탕으로&nbsp;GAN에서의&nbsp;transfer learning을&nbsp;시도하고&nbsp;있고,&nbsp;이를&nbsp;통해&nbsp;한정된&nbsp;데이터를&nbsp;가지고 image generation&nbsp;하는&nbsp;방법들을&nbsp;연구하고&nbsp;있다.</p><p>하지만&nbsp;현재까지&nbsp;Transfer Learning을&nbsp;진행한&nbsp;많은&nbsp;사례들이&nbsp;한정된&nbsp;training data로&nbsp;진행했을&nbsp;때, overfitting이&nbsp;되는&nbsp;경우가많았고,&nbsp;특히&nbsp;dataset의&nbsp;distribution이&nbsp;pre-training을&nbsp;할&nbsp;때&nbsp;사용한&nbsp;large dataset과&nbsp;많이&nbsp;다를&nbsp;경우, robust하지&nbsp;못하는&nbsp;일이&nbsp;많았다 (not robust in learning a significant distribution shift).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="previous-methods-for-gan-transfer-learning">Previous Methods for GAN transfer learning<a class="hash-link" href="#previous-methods-for-gan-transfer-learning" title="Direct link to heading">​</a></h3><p>우선 기존의 GAN Finetuning으로 시도된 방식은 아래와 같이 나열할 수 있다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="fine-tuning">Fine-tuning<a class="hash-link" href="#fine-tuning" title="Direct link to heading">​</a></h4><p>Target Model(=limited data에 대한 model)의 generator와 discriminator 각각에 대해 Source Model(=trained with large dataset)의 pre-trained weight를 load하여, 해당 checkpoint부터 training을 진행한다.</p><p>하지만, 이러한 fine-tuning 방식은 overfit되는 문제점을 항상 가지고 있고, 이를 위해 적절한 regularization이 매번 필요하다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="scaleshift">Scale/Shift<a class="hash-link" href="#scaleshift" title="Direct link to heading">​</a></h4><p>위와 같은 naive fine-tuning이 overfit에 빠지기 쉽다는 단점이 있기에, scale + shift는 다른 weight는 그대로 놔두고 batchnorm 등 normalization layer만 update하는 방식으로 이를 진행하고자 시도했다. 하지만, 이는 restriction이 명확해서 그리 좋은 결과를 보여주지 않았고, 특히 source dataset(=large dataset)과 target dataset(=limited dataset) 간 distribution shift가 극명할 경우, 결과가 더욱 안 좋게 나왔다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="generative-latent-optimization-glo">Generative latent optimization (GLO)<a class="hash-link" href="#generative-latent-optimization-glo" title="Direct link to heading">​</a></h4><p>GAN loss는 discriminator로부터 주어지는 loss이다보니, limited data에 reliable 하지 않다. GLO는 이로부터 source data로 학습한 generator만 떼어다가 L1 + perceptual loss의 조합으로 supervised learning 하는 방식을 말한다. 이 때, GLO는 overfitting을 방지하기 위해 generator와 latent codes를 동시에 optimize하는 방식으로 훈련한다. 즉, latent vector와 sample data가 항상 1:1로 되도록 학습한다.  이를 통해 generator가 sample들에 대해 generalize 되도록 하는 방법이다.</p><p>GLO가 이런 방향으로 학습을 하다 보니 stability는 보장이 되지만, 아무래도 adversarial loss가 없다보니 image generation 결과가 결국에는 blurry 해진다는 단점이 있다. 그렇다고 Adversarial 하게 별도로 discrimator를 붙일수도 있겠지만, 이러한 방향으로 진행될 때 기본적으로 source data에 대한 discriminator의 prior knowledge가 아예 증발해버리므로, 이 역시 그렇게 효율적이라고는 볼 수 없다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="minegan">MineGAN<a class="hash-link" href="#minegan" title="Direct link to heading">​</a></h4><p>MineGAN은 generator의 overfitting을 피하기 위해, generator를 수정하여 latent code를 수정하는 방법을 제시한다. MineGAN은 latent code 간 transfer를 담당하는 miner(채굴) network를 학습시키는데, 이러한 방식은 source와 target distribution이 공유하고 있는 바가 있을 때 효과적이겠지만, 두 dataset가 disjoint 한 상황에서는 generalize 될 수 없다는 단점을 가지고 있다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="suggested-methods">Suggested Methods<a class="hash-link" href="#suggested-methods" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_mojV" id="freezed">FreezeD<a class="hash-link" href="#freezed" title="Direct link to heading">​</a></h4><p>Discriminator의 lower layer(=visual representation)만 freeze, upper layer는 fine-tune. 저자가 성능한 것 중에 가장 좋은 성능을 보였음.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="l2-sp">L2-SP<a class="hash-link" href="#l2-sp" title="Direct link to heading">​</a></h4><p>Fine-tuning하는 방식인데, source model의 parameter와 target model의 parameter를 L2-norm regularize 하는 방식으로 진행하여 target model이 source model의 knowledge로부터 너무 멀어지지 않게 한다.</p><p>하지만, 이는 그렇게 좋은 성능을 보이지 못했다. L2-norm 한다고 하면 이 방식이 결국 freezing layer로 선택한 layer에는 infinite weight를 주고, 다른 layer에는 weight를 0으로 주는 방식으로 굳어지는데, 이것보다는 각 Layer 에 적절한 weight 를 주는 것이 당연 좋은 결과를 낼 것이다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="feature-distillation">Feature Distillation<a class="hash-link" href="#feature-distillation" title="Direct link to heading">​</a></h4><p>Classifier의 transfer learning을 할 때, 요즘 제일 많이 사용하는 방식이다.</p><p>저자는 source model과 target model을 distill 하는 방식으로 진행했다. Computation으로 보면 FreezeD보다 2배 가량 연산량이 많은데, FreezeD랑 비교했을 때 우위를 점하는 경우도 있었다. 이 논문은 FreezeD가 최종적으로 제안하는 method이기는 하지만, Feature Distillation이 미래에 더 좋은 성능을 보이는 방향일 것이라고 이야기 한다.</p><img class="figCenter__7gH" src="/assets/images/table-8729e8237c9747aac68039866747ac52.png" alt="table"><h2 class="anchor anchorWithStickyNavbar_mojV" id="result">Result<a class="hash-link" href="#result" title="Direct link to heading">​</a></h2><p>저자는 Unconditional GAN과 Conditional GAN에 모두 generic 하게 적용될 수 있는 방법인지를 파악하기 위해, 두 가지 adversarial model을 들고 왔다. Unconditional GAN으로는 stylegan, conditional GAN으로는 SNGAN-projection 을 사용했다. 그리고 측정 metric 으로는 FID(Frechet Inception Distance)만을 사용했다.</p><p>각 Dataset class 별로 FID를 각각 계산했고, Fine-tuning 한 것과 FreezeD 두 case에 대한 결과만을 비교했다.</p><p>FreezeD 가 FID에 있어서 항상 앞서는 결과를 보여줬다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="특이점">특이점<a class="hash-link" href="#특이점" title="Direct link to heading">​</a></h2><p>StyleGAN을 실험할 때, tensorflow로 되어 있는 저자 repo가 아니라, unofficial pytorch repo를 사용했다.</p><p>pytorch repo도 많은 유저들이 안정성 검증을 하기는 했었는데, 이걸 사용하여 논문이 accept이 되었으니, 나도 써도 되겠다는 생각이 든다. (이렇게 guarantee 받는 방법이...!)</p>]]></content>
        <author>
            <name>Hyoung-Kyu Song</name>
            <uri>https://github.com/deepkyu</uri>
        </author>
        <category label="paper-review" term="paper-review"/>
        <category label="gan" term="gan"/>
    </entry>
</feed>
<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-223362952-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="alternate" type="application/rss+xml" href="/story/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/story/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/papers/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/papers/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous">
<script src="js/scroll.js"></script><title data-rh="true">A ConvNet for the 2020s (ConvNeXt) | Hyoung-Kyu Song</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://deepkyu.me//papers/convnext"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="A ConvNet for the 2020s (ConvNeXt) | Hyoung-Kyu Song"><meta data-rh="true" name="description" content="Convolution이 말합니다. 형 아직 죽지 않았어 임마."><meta data-rh="true" property="og:description" content="Convolution이 말합니다. 형 아직 죽지 않았어 임마."><meta data-rh="true" property="og:image" content="https://deepkyu.me//img/default.png"><meta data-rh="true" name="twitter:image" content="https://deepkyu.me//img/default.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2022-08-31T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/deepkyu"><meta data-rh="true" property="article:tag" content="paper-review"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepkyu.me//papers/convnext"><link data-rh="true" rel="alternate" href="https://deepkyu.me//papers/convnext" hreflang="en"><link data-rh="true" rel="alternate" href="https://deepkyu.me//papers/convnext" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.944d9ea4.css">
<link rel="preload" href="/assets/js/runtime~main.401e42fa.js" as="script">
<link rel="preload" href="/assets/js/main.79fa2e00.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Deepkyu" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="Deepkyu" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a class="navbar__item navbar__link" href="/blog">Blog</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers">Paper Reviews</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/cv">CV</a><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://www.linkedin.com/in/deepkyu" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">All Posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a aria-current="page" class="sidebarItemLink_miNk sidebarItemLinkActive_RRTD" href="/papers/convnext">A ConvNet for the 2020s (ConvNeXt)</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/acon">Activate or Not: Learning Customized Activation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/anycost">Anycost GANs for Interactive Image Synthesis and Editing</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/diffaugment">Differentiable Augmentation for Data-Efficient GAN Training</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/freezed">FreezeD: a Simple Baseline for Fine-tuning GANs</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_rzP5" itemprop="headline">A ConvNet for the 2020s (ConvNeXt)</h1><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-08-31T00:00:00.000Z" itemprop="datePublished">August 31, 2022</time> · <!-- -->18 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/deepkyu.png" alt="Hyoung-Kyu Song"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyoung-Kyu Song</span></a></div><small class="avatar__subtitle" itemprop="description">AI Researcher (Vision)</small></div></div></div></div></header><meta itemprop="image" content="https://deepkyu.me//img/default.png"><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2201.03545" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2201.03545-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/facebookresearch/ConvNeXt" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="main-idea">Main Idea<a class="hash-link" href="#main-idea" title="Direct link to heading">​</a></h2><p>Vision Task에 Transformer 기반 Architecture를 접목하는 ViT의 등장 이후, classification task에 대해 좋은 성능을 보였습니다. ViT 이후에 Swim Transformers 등의 방법론 등은 segmentation이나 object detection에도 transformer를 적용하기 위해 등장한 방법들이었습니다. Swim Transformer의 경우, 여러 ConvNet을 prior로 삼는 hybrid한 방법으로 이를 해결한 것이 특징입니다. 하지만, hybrid하다고 하기에는 기존 Transformer의 힘을 빌린 것일 뿐, ConvNet 자체가 가지고 있는 inductive bias를 최대한 사용한 방법은 아닙니다.</p><p>저자는 순수한 ConvNet(Transformer 구조를 곁들이지 않은)의 힘을 확인하고자 기존의 standard ResNet(architecture + 학습 방법론)에서 vision Transformer처럼 학습할 수 있게끔 지금까지 등장해온 여러 Novelty들을 접목해보고자 시도합니다. 특히, vision Transformer 모델들이 등장할 때 항상 새로운 학습 방법론을 함께 들고 나와서 성능 향상을 주장하고는 하는데, 그 학습 방법론들을 기존 ConvNet들에 적용해본 사례가 많이 없습니다. 여러 실험을 통해 최근에 등장한 학습 방법론을 적용하고 convolution block design을 새롭게 디자인하면 더 직관적인 구조로 Transformer에 근접한 성능을 보일 수 있음을 보여줍니다.</p><p>가끔 ConvNeXt 논문을 들어보기만 하고 착각하는 것 중 하나가, ConvNeXt의 FLOPs 가 Transformer보다 현저히 작은 채로 성능을 비등하게 낸 것으로 아시는 분들이 있습니다. ConvNeXt는 architecture가 (inductive bias에서 등장했던) ConvNet을 순수하게 기반으로 했을 뿐, 그 모델 사이즈가 Transformers 보다 절대 작지는 않습니다. 다만, 상대적으로 Transformer보다는 convolution layer에 대한 compression 방법론들이 더 많이 고안된만큼, 그 직관적인 구조에 의해 등장했던 compression 방법론들을 두루 적용하여 앞으로 경량화할 수 있는 가능성이 좀 더 있다는 게 제 개인적인 생각입니다.</p><p>주로 ResNet 50 을 기준으로 성능 report를 진행하고, 각 accuracy는 random seed를 다르게 하여 3번씩 실험한 결과입니다. 요즘 논문치고 철저하다고 보는 분도 계시고, 통계적으로 무슨 의미가 있냐고 하시는 분도 있더라고요 ㅎㅎ 다만, publish 이후 학계에서 검증하면서 ConvNeXt 실험 결과가 잘못됐다고 하는 논문은 본 적이 없습니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="background-knowledge">Background Knowledge<a class="hash-link" href="#background-knowledge" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="examples-of-representative-convnet">Examples of Representative ConvNet<a class="hash-link" href="#examples-of-representative-convnet" title="Direct link to heading">​</a></h3><p>VGGNet, Inceptions, ResNe(X)t, DenseNet, MobileNet, EfficientNet and RegNet</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="convnet-의-주요-특징들">ConvNet 의 주요 특징들<a class="hash-link" href="#convnet-의-주요-특징들" title="Direct link to heading">​</a></h3><p>아래는 &quot;sliding window&quot;를 사용하는 convolution에서 고안되었기 때문에 생겨나는 특징들입니다.</p><ul><li>Translation equivariant<ul><li>Object detection 등의 Task에 있어서 특히 유용합니다.</li><li>가끔 equivariance, invariance 헷갈려 하시는 분들 계셔서 말씀드리면, Patch를 이동하든, 그 결과값을 이동한다고 feature vector 값이 바뀌는 건 아니여서 invariant 하다고도 볼 수 있습니다. (g: Identity Mapping)</li></ul></li><li>Weight Sharing</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화1-training-methodology">변화1: Training Methodology<a class="hash-link" href="#변화1-training-methodology" title="Direct link to heading">​</a></h2><p>저자는 ResNet 50 에 DeiT, Swim Transformer와 유사한 training recipe를 적용하여 성능 향상이 얼마나 일어나는 지를 살펴봅니다.</p><ul><li>Training epoch 증가<ul><li>90 -&gt; 300</li></ul></li><li>Optimizer 변경<ul><li>Adam -&gt; AdamW</li></ul></li><li>Data augmentation<ul><li>Mixup, Cutmix, RandAugment, Random Erasing을 추가</li></ul></li><li>Regularization scheme 추가<ul><li>Stochastic depth<ul><li>Depth를 이루는 ResBlocks 중 일부를 random하게 drop하면서 학습하는 방법입니다. 여기서 drop 한다는 의미는 구현에서 봤을 때는 ResBlock과 identity mapping(skip connection) 중 선택하는 것과 같습니다.<ul><li>&quot;... Aims to shrink the depth of a network during training, while keeping it unchanged during testing. This is achieved by randomly dropping entire ResBlocks during training and bypassing their transformations through skip connections.&quot;</li></ul></li></ul></li><li>Label smoothing</li></ul></li></ul><p>이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 76.1% -&gt; 78.8% (+2.7%)</li></ul><p>어쩌면 traditional ConvNets와 vision Transformers의 차이는 학습 방법에서 주로 기인한 게 아닐까 싶을 정도의 차이일 수도 있습니다.<br>
<!-- -->저자는 이 파트에서 찾은 training recipe를 hyperparameter와 함께 유지하면서 아래의 design 변경들을 진행합니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화2-큰-틀에서의-구조-변경">변화2: 큰 틀에서의 구조 변경<a class="hash-link" href="#변화2-큰-틀에서의-구조-변경" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="stage-compute-ratio를-3-3-9-3으로-변경">Stage Compute Ratio를 (3, 3, 9, 3)으로 변경<a class="hash-link" href="#stage-compute-ratio를-3-3-9-3으로-변경" title="Direct link to heading">​</a></h3><p>기존 ResNet의 stage별 디자인은 매우 경험적으로 결정되었습니다.</p><img class="figCenter__7gH" src="/assets/images/figure1-imagenet1k-4c92a355f2805984792477b06d558009.png" alt="imagenet1k"><p>4번째 stage에 layer가 많기 때문에(ResNet50 기준 6), object detection 등의 downstream task에 접목되기 위한 backbone으로 많이 쓰일 수 있었고, 특히 이 때의 feature map 사이즈가 14 x 14 이기에 detector head로서의 역할도 겸할 수 있었습니다. Swim Transformer는 이와는 비슷하나 조금 다른 stage ratio를 보여주는데, 작은 모델의 경우 (1, 1, 3, 1), 큰 모델의 경우 (1, 1, 9, 1)의 비율을 보여줍니다.</p><p>저자는 Swim-T와의 FLOPs를 유사한 비율로 가져가기 위해, 기존의 stage ratio (3, 4, 6, 3)을 (3, 3, 9, 3)으로 변경했습니다.</p><p>이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 78.8% -&gt; 79.4 (+0.6%, 누적: +3.3%)</li></ul><p>Network design space에 관한 연구들을 살펴보면, 이보다 충분히 더 좋은 stage compute ratio가 있을 수도 있다고 저자는 말합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="patch를-만드는-첫-stem-layer을-convks4-stride4로-변경-non-overlapping-conv">Patch를 만드는 첫 stem layer을 Conv(ks=4, stride=4)로 변경 (non-overlapping conv.)<a class="hash-link" href="#patch를-만드는-첫-stem-layer을-convks4-stride4로-변경-non-overlapping-conv" title="Direct link to heading">​</a></h3><p>Stem cell design은 아키텍쳐 가장 처음에 input image를 어떻게 처리해줄 지를 담고 있습니다. ResNet에서는 kernel size 7x7, stride 2 의 Conv layer(2x downsample)과 max-pooling(2x downsample)을 통해 4x downsample 시키는 stem cell design을 사용했습니다. vision Transformer 들은 이보다도 더 과감하게 patch를 만드는 전략을 사용하는 데, 14x14 또는 16x16의 아주 큰 kernel size를 가진 Conv layer를 겹치는 부분이 없게끔(kernel size와 stride가 동일) 설정합니다. Swim Transformer의 경우, 이 design에 추가로 multi-stage design을 위해 4x4 patch size의 layer를 별개로 두었습니다.</p><p>저자는 ResNet-style stem cell로서 kernel size 4x4, stride 4로 설정한 conv. layer(4x downsample)를 stem cell design으로 선정했습니다.</p><p>이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 79.4% -&gt; 79.5% (+0.1%, 누적: +3.4%)</li></ul><p>ResNet에서 patch를 만드는 방법에 있어서는 이렇게 ViT 등에서 보여준 더 쉬운 방법들을 사용하는 것으로 교체 가능할 수 있음을 시사합니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화3-resnext-아이디어-적용하기">변화3: ResNeXt 아이디어 적용하기<a class="hash-link" href="#변화3-resnext-아이디어-적용하기" title="Direct link to heading">​</a></h2><p>ResNeXt 에서는 bottleneck block에서 grouped convolution을 사용함으로서, FLOPs를 줄이고 network width(# channel in hidden layers) 를 늘릴 수 있었습니다.</p><p>저자는 grouped convolution의 맥락에서 depthwise convolution을 사용합니다. Depthwise convolution은 per-channel로 self-attention의 weighted sum과 동일한 역할을 하게 되는데요. 즉, spatial dimension으로만 정보를 섞는 역할을 합니다.</p><p>이를 통해 FLOPs는 효과적으로 줄이면서도 정확도는 더욱 얻을 수 있습니다. Width를 확장할 수 있게 된 만큼, width를 Swim Transformer를 따라 64에서 96으로 늘리게 되었다. 그리고 이를 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 79.5% -&gt; 80.5% (+1.0%, 누적: +4.4%)</li></ul><img class="figCenter__7gH medium_sJ4m" src="/assets/images/figure2-block-0c4373a08e91dae993b0c579624d7da8.png" alt="block"><p>(a): ResNeXt block / (b): inverted bottleneck / (c): position switch of depthwise conv layer</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화4-inverted-bottleneck-적용하기">변화4: Inverted Bottleneck 적용하기<a class="hash-link" href="#변화4-inverted-bottleneck-적용하기" title="Direct link to heading">​</a></h2><p>Transformer에서의 inverted bottleneck은 MLP block 중 hidden layer의 dimension이 input dimensionqhek 4배 크게 디자인 된 것을 말합니다. ConvNet에서도 inverted bottleneck은 MobileNetV2에서 등장한 이후 두루 쓰이고 있는데, layer의 형태만 다를 뿐 Transformer에서 사용하는 inverted block과 유사하다고 볼 수 있습니다. 이를 반영하면, 위 figure 내 (a)에서 (b)로의 변화라고 볼 수 있습니다. Depthwise convolution(보라색)의 FLOPs 는 늘어나지만 전체 네트워크의 FLOPs는 줄어드는데, 이는 downsampling을 진행하는 residual block 내의 1x1 conv layer (shortcut layer) 의 FLOPs가 줄어들기 때문입니다(layer 별: (384 -&gt; 384) 1x1 conv -&gt; (96 -&gt; 96) 1x1 conv).</p><p>저자는 inverted bottleneck 구조를 적용했고, 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 80.5% -&gt; 80.6% (+0.1%, 누적: +4.5%)</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화5-큰-사이즈의-kernel을-이용하기">변화5: 큰 사이즈의 Kernel을 이용하기<a class="hash-link" href="#변화5-큰-사이즈의-kernel을-이용하기" title="Direct link to heading">​</a></h2><p>Vision Transformer에서 사용하는 self-attention은 non-local한 특성을 가지고 있어, 사실상 receptive field가 global하다고 볼 수 있습니다. 이와는 반대로 ConvNet에서는 GPU의 특성을 고려하여 효율적인 연산을 위해 VGG network를 시작으로 3x3의 작은 사이즈의 kernel 사이즈를 사용했습니다. Swim Transformer는 self-attention을 사용하는 데에 있어 local window를 적용하는데, 여기서는 kernel size를 최소 7x7 로 잡아서 사용했었습니다. 저자는 ConvNet에도 large kernel size를 적용해보고자 합니다.</p><p>우선 kernel size를 키우기 위해서는 depthwise conv. 의 위치를 1x1 conv. 앞으로 가져와야 합니다. 이는 Transformer에서 MSA(Multi-head Self Attention)가 MLP layer 앞에 있는 구조와 유사하게 가져가기 위함입니다.</p><p>이 상태에서 저자는 kernel size를 3, 5, 7, 9, 11 등의 크기로 바꿔가면서 실험을 진행했습니다. Kernel size가 달라진다고 FLOPs에 미치는 영향은 미미합니다. 저자들의 실험을 통해 7x7보다 큰 kernel에서는 성능이 더 증가하지 않고 saturate 되는 것을 확인했습니다.</p><p>이를 통해 inverted bottleneck까지 적용했을 때와 동일한 accuracy를 보이면서도 FLOPs는 4.6G에서 4.2G로 낮출 수 있었습니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="변화6-activation-normalization-layer-변경하기">변화6: Activation, Normalization Layer 변경하기<a class="hash-link" href="#변화6-activation-normalization-layer-변경하기" title="Direct link to heading">​</a></h2><img class="figCenter__7gH medium_sJ4m" src="/assets/images/figure3-block-design-60ac24931c3b5b08d1bcb0b361c1debf.png" alt="block-design"><p>가장 우측 ConvNeXt block이 저자가 최종적으로 디자인한 형태입니다. 아래 내용을 모두 확인하시고 저 block에 해당하는 내용들이 잘 반영되었는지 확인해보세요😊.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="relu를-gelu로-변경">ReLU를 GELU로 변경<a class="hash-link" href="#relu를-gelu로-변경" title="Direct link to heading">​</a></h3><p>ConvNet에서는 아직도 ReLU가 두루 쓰이고, original Transformer 논문에서도 ReLU를 사용하지만, 그 이후 등장한 NLP Transformer(BERT, GPT-2)와 ViT에서는 GELU(Gaussian Error Linear Unit)을 많이 사용합니다.</p><p>이를 적용했을 때, 별도의 성능 향상은 없었지만, GELU가 ConvNet에도 적용될 수 있음을 보여준 변화점이라고 볼 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="activation-function을-적게-쓰기">Activation Function을 적게 쓰기<a class="hash-link" href="#activation-function을-적게-쓰기" title="Direct link to heading">​</a></h3><p>Transformer block에서도 MLP block 내 activation 1번 쓰이는 것 외로는 쓰이지 않습니다. 이는 1x1 conv. 뒤에까지 activation을 붙이는 ConvNet에서의 행태와는 매우 다릅니다. 저자는 Transformer block design을 따라, residual block 내에 모든 GELU activation을 지우고, 1x1 layer 사이에 activation 하나만 남겨두었습니다. 이와 같은 변화를 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 80.6% -&gt; 81.3% (+0.7%, 누적: +5.2%)</li></ul><p>이 성능은 Swim-T와 동일한 성능입니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="normalization도-적게-쓰기">Normalization도 적게 쓰기<a class="hash-link" href="#normalization도-적게-쓰기" title="Direct link to heading">​</a></h3><p>Transformer에서는 normalization layer도 적게 사용합니다. 저자는 이를 따라 1x1 conv 앞에 하나의 BN(Batch Normalization)만을 놔두고, 나머지 normalization layer를 지웠습니다. 이를 통해 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 81.3% -&gt; 81.4% (+0.1%, 누적: +5.3%)</li></ul><p>재밌는 점은 이러한 변화를 통해 Transfomer보다도 오히려 normalization layer가 적어졌습니다. 이는 Transformer layer는 block 초입에 LN((Layer Normalization) 을 취하는데, 저자는 BN을 붙인다고 성능이 좋아지지 않아서 그냥 지웠다고 합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="batch-normalization-대신-layer-normalization">Batch Normalization 대신 Layer Normalization<a class="hash-link" href="#batch-normalization-대신-layer-normalization" title="Direct link to heading">​</a></h3><p>많은 vision task에서 BN이 사랑받고 있는 것은 사실인데요. 하지만, Transformer에서는 이미 LN을 쓰면서 좋은 성능을 보인 경우가 많았습니다. 저자는 이 상태에서 LN을 적용해서도 학습하는 데 무리가 없음을 확인했고 오히려 아래와 같은 성능 향상이 있었습니다.</p><ul><li>ResNet-50: 81.4% -&gt; 81.5% (+0.1%, 누적: +5.4%)</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="separate-downsampling-layer-사용">Separate Downsampling Layer 사용<a class="hash-link" href="#separate-downsampling-layer-사용" title="Direct link to heading">​</a></h3><p>이 내용은 block 내부에 반영된 내용이 아니라, stage 넘어가면서 downsample을 할 때에 해당하는 내용입니다. 위 diagram에는 드러나지 않는 부분이니 참고해주세요~<br>
<!-- -->Swim Transformer가 patch merging을 할 때 2x2 neighborhood patch들의 channel을 concat하여(4C) 이에 대해 2C 의 channel size가 되도록 내뱉습니다. 이를 Conv. 로 표현하면 channel size가 2배가 되고, kernel size는 2x2, stride 2인 것입니다. 저자는 ResNet에서 kernel size 3x3, stride 2, padding 1 로 마치 공식처럼 쓰이던 downsample하는 conv. layer 대신에 Swim Transformer에서의 downsample 방식을 사용합니다. 그냥 이를 대치시키면 학습이 diverge 되는데, block-level에서 spatial resolution이 변경되는 지점에 LN을 적당히 넣어주면 이에 대해 학습이 stabilize 되는 것을 찾아냅니다. 이는 Swim Transformer도 적용된 방식입니다.</p><p>그리고 이를 통해 최종적으로 ConvNeXt는 아래의 성능 향상을 보였습니다.</p><ul><li>ResNet-50: 81.5% -&gt; 82.0% (+0.5%, 누적: +5.9%)</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="training-details">Training Details<a class="hash-link" href="#training-details" title="Direct link to heading">​</a></h2><p>저자는 본 모델을 어떻게 학습했는 지 Hyperparameter와 학습 세팅도 함께 보여주고 있습니다.<br>
<!-- -->Augmentation, optimizer 등 여러 부분에서 등장한 최신의 학습 방법론들을 한 곳에 모아둔 느낌입니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="pre-training-settings">(Pre-)Training Settings<a class="hash-link" href="#pre-training-settings" title="Direct link to heading">​</a></h3><img class="figCenter__7gH small_f2c9" src="/assets/images/figure4-pretrain-abd6f5f77e98f8c2066000329b278825.png" alt="pretrain"><h3 class="anchor anchorWithStickyNavbar_mojV" id="finetuning-settings">Finetuning Settings<a class="hash-link" href="#finetuning-settings" title="Direct link to heading">​</a></h3><img class="figCenter__7gH small_f2c9" src="/assets/images/figure5-finetune-062a17c7c12e0d895436abd6ecdcb8a2.png" alt="finetune"></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_h6_j"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/papers/tags/paper-review">paper-review</a></li></ul></div></footer></article><div></div><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/papers/acon"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Activate or Not: Learning Customized Activation</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#main-idea" class="table-of-contents__link toc-highlight">Main Idea</a></li><li><a href="#background-knowledge" class="table-of-contents__link toc-highlight">Background Knowledge</a><ul><li><a href="#examples-of-representative-convnet" class="table-of-contents__link toc-highlight">Examples of Representative ConvNet</a></li><li><a href="#convnet-의-주요-특징들" class="table-of-contents__link toc-highlight">ConvNet 의 주요 특징들</a></li></ul></li><li><a href="#변화1-training-methodology" class="table-of-contents__link toc-highlight">변화1: Training Methodology</a></li><li><a href="#변화2-큰-틀에서의-구조-변경" class="table-of-contents__link toc-highlight">변화2: 큰 틀에서의 구조 변경</a><ul><li><a href="#stage-compute-ratio를-3-3-9-3으로-변경" class="table-of-contents__link toc-highlight">Stage Compute Ratio를 (3, 3, 9, 3)으로 변경</a></li><li><a href="#patch를-만드는-첫-stem-layer을-convks4-stride4로-변경-non-overlapping-conv" class="table-of-contents__link toc-highlight">Patch를 만드는 첫 stem layer을 Conv(ks=4, stride=4)로 변경 (non-overlapping conv.)</a></li></ul></li><li><a href="#변화3-resnext-아이디어-적용하기" class="table-of-contents__link toc-highlight">변화3: ResNeXt 아이디어 적용하기</a></li><li><a href="#변화4-inverted-bottleneck-적용하기" class="table-of-contents__link toc-highlight">변화4: Inverted Bottleneck 적용하기</a></li><li><a href="#변화5-큰-사이즈의-kernel을-이용하기" class="table-of-contents__link toc-highlight">변화5: 큰 사이즈의 Kernel을 이용하기</a></li><li><a href="#변화6-activation-normalization-layer-변경하기" class="table-of-contents__link toc-highlight">변화6: Activation, Normalization Layer 변경하기</a><ul><li><a href="#relu를-gelu로-변경" class="table-of-contents__link toc-highlight">ReLU를 GELU로 변경</a></li><li><a href="#activation-function을-적게-쓰기" class="table-of-contents__link toc-highlight">Activation Function을 적게 쓰기</a></li><li><a href="#normalization도-적게-쓰기" class="table-of-contents__link toc-highlight">Normalization도 적게 쓰기</a></li><li><a href="#batch-normalization-대신-layer-normalization" class="table-of-contents__link toc-highlight">Batch Normalization 대신 Layer Normalization</a></li><li><a href="#separate-downsampling-layer-사용" class="table-of-contents__link toc-highlight">Separate Downsampling Layer 사용</a></li></ul></li><li><a href="#training-details" class="table-of-contents__link toc-highlight">Training Details</a><ul><li><a href="#pre-training-settings" class="table-of-contents__link toc-highlight">(Pre-)Training Settings</a></li><li><a href="#finetuning-settings" class="table-of-contents__link toc-highlight">Finetuning Settings</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/papers">Reviews</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/story">Story</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/deepkyu" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Deepkyu. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.401e42fa.js"></script>
<script src="/assets/js/main.79fa2e00.js"></script>
</body>
</html>
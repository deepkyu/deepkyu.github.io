---
slug: convnext
title: "A ConvNet for the 2020s (ConvNeXt)"
description: Convolution이 말합니다. 형 아직 죽지 않았어 임마.
image: img/default.png
authors: [hkyu]
tags: [paper-review]
---

import clsx from 'clsx';
import styles from '../blog.module.css';

import figImagenetOneK from './image/figure1-imagenet1k.png';
import figBlock from './image/figure2-block.png';
import figBlockDesign from './image/figure3-block-design.png';
import figPretrain from './image/figure4-pretrain.png';
import figFinetune from './image/figure5-finetune.png';

[github](https://github.com/facebookresearch/ConvNeXt)

## Main Idea

Vision Task에 Transformer 기반 Architecture를 접목하는 ViT의 등장 이후, classification task에 대해 좋은 성능을 보였습니다. ViT 이후에 Swim Transformers 등의 방법론 등은 segmentation이나 object detection에도 transformer를 적용하기 위해 등장한 방법들이었습니다. Swim Transformer의 경우, 여러 ConvNet을 prior로 삼는 hybrid한 방법으로 이를 해결한 것이 특징입니다. 하지만, hybrid하다고 하기에는 기존 Transformer의 힘을 빌린 것일 뿐, ConvNet 자체가 가지고 있는 inductive bias를 최대한 사용한 방법은 아닙니다.

저자는 순수한 ConvNet(Transformer 구조를 곁들이지 않은)의 힘을 확인하고자 기존의 standard ResNet(architecture + 학습 방법론)에서 vision Transformer처럼 학습할 수 있게끔 지금까지 등장해온 여러 Novelty들을 접목해보고자 시도합니다. 특히, vision Transformer 모델들이 등장할 때 항상 새로운 학습 방법론을 함께 들고 나와서 성능 향상을 주장하고는 하는데, 그 학습 방법론들을 기존 ConvNet들에 적용해본 사례가 많이 없습니다. 여러 실험을 통해 최근에 등장한 학습 방법론을 적용하고 convolution block design을 새롭게 디자인하면 더 직관적인 구조로 Transformer에 근접한 성능을 보일 수 있음을 보여줍니다.

가끔 ConvNeXt 논문을 들어보기만 하고 착각하는 것 중 하나가, ConvNeXt의 FLOPs 가 Transformer보다 현저히 작은 채로 성능을 비등하게 낸 것으로 아시는 분들이 있습니다. ConvNeXt는 architecture가 (inductive bias에서 등장했던) ConvNet을 순수하게 기반으로 했을 뿐, 그 모델 사이즈가 Transformers 보다 절대 작지는 않습니다. 다만, 상대적으로 Transformer보다는 convolution layer에 대한 compression 방법론들이 더 많이 고안된만큼, 그 직관적인 구조에 의해 등장했던 compression 방법론들을 두루 적용하여 앞으로 경량화할 수 있는 가능성이 좀 더 있다는 게 제 개인적인 생각입니다.

주로 ResNet 50 을 기준으로 성능 report를 진행하고, 각 accuracy는 random seed를 다르게 하여 3번씩 실험한 결과입니다. 요즘 논문치고 철저하다고 보는 분도 계시고, 통계적으로 무슨 의미가 있냐고 하시는 분도 있더라고요 ㅎㅎ 다만, publish 이후 학계에서 검증하면서 ConvNeXt 실험 결과가 잘못됐다고 하는 논문은 본 적이 없습니다.

## Background Knowledge

### Examples of Representative ConvNet

VGGNet, Inceptions, ResNe(X)t, DenseNet, MobileNet, EfficientNet and RegNet

### ConvNet 의 주요 특징들

아래는 "sliding window"를 사용하는 convolution에서 고안되었기 때문에 생겨나는 특징들입니다.

- Translation equivariant
  - Object detection 등의 Task에 있어서 특히 유용합니다.
  - 가끔 equivariance, invariance 헷갈려 하시는 분들 계셔서 말씀드리면, Patch를 이동하든, 그 결과값을 이동한다고 feature vector 값이 바뀌는 건 아니여서 invariant 하다고도 볼 수 있습니다. (g: Identity Mapping)
- Weight Sharing

## 변화1: Training Methodology

저자는 ResNet 50 에 DeiT, Swim Transformer와 유사한 training recipe를 적용하여 성능 향상이 얼마나 일어나는 지를 살펴봅니다.

- Training epoch 증가
  - 90 -> 300
- Optimizer 변경
  - Adam -> AdamW
- Data augmentation
  - Mixup, Cutmix, RandAugment, Random Erasing을 추가
- Regularization scheme 추가
  - Stochastic depth
    - Depth를 이루는 ResBlocks 중 일부를 random하게 drop하면서 학습하는 방법입니다. 여기서 drop 한다는 의미는 구현에서 봤을 때는 ResBlock과 identity mapping(skip connection) 중 선택하는 것과 같습니다.
      - "... Aims to shrink the depth of a network during training, while keeping it unchanged during testing. This is achieved by randomly dropping entire ResBlocks during training and bypassing their transformations through skip connections."
  - Label smoothing

이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 76.1% -> 78.8% (+2.7%)

어쩌면 traditional ConvNets와 vision Transformers의 차이는 학습 방법에서 주로 기인한 게 아닐까 싶을 정도의 차이일 수도 있습니다.  
저자는 이 파트에서 찾은 training recipe를 hyperparameter와 함께 유지하면서 아래의 design 변경들을 진행합니다.

## 변화2: 큰 틀에서의 구조 변경

### Stage Compute Ratio를 (3, 3, 9, 3)으로 변경

기존 ResNet의 stage별 디자인은 매우 경험적으로 결정되었습니다.

<img className={styles.figCenter} src={figImagenetOneK} alt="imagenet1k" />

4번째 stage에 layer가 많기 때문에(ResNet50 기준 6), object detection 등의 downstream task에 접목되기 위한 backbone으로 많이 쓰일 수 있었고, 특히 이 때의 feature map 사이즈가 14 x 14 이기에 detector head로서의 역할도 겸할 수 있었습니다. Swim Transformer는 이와는 비슷하나 조금 다른 stage ratio를 보여주는데, 작은 모델의 경우 (1, 1, 3, 1), 큰 모델의 경우 (1, 1, 9, 1)의 비율을 보여줍니다.

저자는 Swim-T와의 FLOPs를 유사한 비율로 가져가기 위해, 기존의 stage ratio (3, 4, 6, 3)을 (3, 3, 9, 3)으로 변경했습니다.

이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 78.8% -> 79.4 (+0.6%, 누적: +3.3%)

Network design space에 관한 연구들을 살펴보면, 이보다 충분히 더 좋은 stage compute ratio가 있을 수도 있다고 저자는 말합니다.

### Patch를 만드는 첫 stem layer을 Conv(ks=4, stride=4)로 변경 (non-overlapping conv.)

Stem cell design은 아키텍쳐 가장 처음에 input image를 어떻게 처리해줄 지를 담고 있습니다. ResNet에서는 kernel size 7x7, stride 2 의 Conv layer(2x downsample)과 max-pooling(2x downsample)을 통해 4x downsample 시키는 stem cell design을 사용했습니다. vision Transformer 들은 이보다도 더 과감하게 patch를 만드는 전략을 사용하는 데, 14x14 또는 16x16의 아주 큰 kernel size를 가진 Conv layer를 겹치는 부분이 없게끔(kernel size와 stride가 동일) 설정합니다. Swim Transformer의 경우, 이 design에 추가로 multi-stage design을 위해 4x4 patch size의 layer를 별개로 두었습니다.

저자는 ResNet-style stem cell로서 kernel size 4x4, stride 4로 설정한 conv. layer(4x downsample)를 stem cell design으로 선정했습니다.

이 방법론 변경을 통해 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 79.4% -> 79.5% (+0.1%, 누적: +3.4%)

ResNet에서 patch를 만드는 방법에 있어서는 이렇게 ViT 등에서 보여준 더 쉬운 방법들을 사용하는 것으로 교체 가능할 수 있음을 시사합니다.

## 변화3: ResNeXt 아이디어 적용하기

ResNeXt 에서는 bottleneck block에서 grouped convolution을 사용함으로서, FLOPs를 줄이고 network width(# channel in hidden layers) 를 늘릴 수 있었습니다.

저자는 grouped convolution의 맥락에서 depthwise convolution을 사용합니다. Depthwise convolution은 per-channel로 self-attention의 weighted sum과 동일한 역할을 하게 되는데요. 즉, spatial dimension으로만 정보를 섞는 역할을 합니다.

이를 통해 FLOPs는 효과적으로 줄이면서도 정확도는 더욱 얻을 수 있습니다. Width를 확장할 수 있게 된 만큼, width를 Swim Transformer를 따라 64에서 96으로 늘리게 되었다. 그리고 이를 통해 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 79.5% -> 80.5% (+1.0%, 누적: +4.4%)

<img className={styles.figCenter} src={figBlock} alt="block" />

(a): ResNeXt block / (b): inverted bottleneck / (c): position switch of depthwise conv layer

## 변화4: Inverted Bottleneck 적용하기

Transformer에서의 inverted bottleneck은 MLP block 중 hidden layer의 dimension이 input dimensionqhek 4배 크게 디자인 된 것을 말합니다. ConvNet에서도 inverted bottleneck은 MobileNetV2에서 등장한 이후 두루 쓰이고 있는데, layer의 형태만 다를 뿐 Transformer에서 사용하는 inverted block과 유사하다고 볼 수 있습니다. 이를 반영하면, 위 figure 내 (a)에서 (b)로의 변화라고 볼 수 있습니다. Depthwise convolution(보라색)의 FLOPs 는 늘어나지만 전체 네트워크의 FLOPs는 줄어드는데, 이는 downsampling을 진행하는 residual block 내의 1x1 conv layer (shortcut layer) 의 FLOPs가 줄어들기 때문입니다(layer 별: (384 -> 384) 1x1 conv -> (96 -> 96) 1x1 conv).

저자는 inverted bottleneck 구조를 적용했고, 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 80.5% -> 80.6% (+0.1%, 누적: +4.5%)

## 변화5: 큰 사이즈의 Kernel을 이용하기

Vision Transformer에서 사용하는 self-attention은 non-local한 특성을 가지고 있어, 사실상 receptive field가 global하다고 볼 수 있습니다. 이와는 반대로 ConvNet에서는 GPU의 특성을 고려하여 효율적인 연산을 위해 VGG network를 시작으로 3x3의 작은 사이즈의 kernel 사이즈를 사용했습니다. Swim Transformer는 self-attention을 사용하는 데에 있어 local window를 적용하는데, 여기서는 kernel size를 최소 7x7 로 잡아서 사용했었습니다. 저자는 ConvNet에도 large kernel size를 적용해보고자 합니다.

우선 kernel size를 키우기 위해서는 depthwise conv. 의 위치를 1x1 conv. 앞으로 가져와야 합니다. 이는 Transformer에서 MSA(Multi-head Self Attention)가 MLP layer 앞에 있는 구조와 유사하게 가져가기 위함입니다.

이 상태에서 저자는 kernel size를 3, 5, 7, 9, 11 등의 크기로 바꿔가면서 실험을 진행했습니다. Kernel size가 달라진다고 FLOPs에 미치는 영향은 미미합니다. 저자들의 실험을 통해 7x7보다 큰 kernel에서는 성능이 더 증가하지 않고 saturate 되는 것을 확인했습니다.

이를 통해 inverted bottleneck까지 적용했을 때와 동일한 accuracy를 보이면서도 FLOPs는 4.6G에서 4.2G로 낮출 수 있었습니다.

## 변화6: Activation, Normalization Layer 변경하기

<img className={styles.figCenter} src={figBlockDesign} alt="block-design" />

가장 우측 ConvNeXt block이 저자가 최종적으로 디자인한 형태입니다. 아래 내용을 모두 확인하시고 저 block에 해당하는 내용들이 잘 반영되었는지 확인해보세요😊.

### ReLU를 GELU로 변경

ConvNet에서는 아직도 ReLU가 두루 쓰이고, original Transformer 논문에서도 ReLU를 사용하지만, 그 이후 등장한 NLP Transformer(BERT, GPT-2)와 ViT에서는 GELU(Gaussian Error Linear Unit)을 많이 사용합니다.

이를 적용했을 때, 별도의 성능 향상은 없었지만, GELU가 ConvNet에도 적용될 수 있음을 보여준 변화점이라고 볼 수 있습니다.

### Activation Function을 적게 쓰기

Transformer block에서도 MLP block 내 activation 1번 쓰이는 것 외로는 쓰이지 않습니다. 이는 1x1 conv. 뒤에까지 activation을 붙이는 ConvNet에서의 행태와는 매우 다릅니다. 저자는 Transformer block design을 따라, residual block 내에 모든 GELU activation을 지우고, 1x1 layer 사이에 activation 하나만 남겨두었습니다. 이와 같은 변화를 통해 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 80.6% -> 81.3% (+0.7%, 누적: +5.2%)

이 성능은 Swim-T와 동일한 성능입니다.

### Normalization도 적게 쓰기

Transformer에서는 normalization layer도 적게 사용합니다. 저자는 이를 따라 1x1 conv 앞에 하나의 BN(Batch Normalization)만을 놔두고, 나머지 normalization layer를 지웠습니다. 이를 통해 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 81.3% -> 81.4% (+0.1%, 누적: +5.3%)

재밌는 점은 이러한 변화를 통해 Transfomer보다도 오히려 normalization layer가 적어졌습니다. 이는 Transformer layer는 block 초입에 LN((Layer Normalization) 을 취하는데, 저자는 BN을 붙인다고 성능이 좋아지지 않아서 그냥 지웠다고 합니다.

### Batch Normalization 대신 Layer Normalization

많은 vision task에서 BN이 사랑받고 있는 것은 사실인데요. 하지만, Transformer에서는 이미 LN을 쓰면서 좋은 성능을 보인 경우가 많았습니다. 저자는 이 상태에서 LN을 적용해서도 학습하는 데 무리가 없음을 확인했고 오히려 아래와 같은 성능 향상이 있었습니다.

- ResNet-50: 81.4% -> 81.5% (+0.1%, 누적: +5.4%)

### Separate Downsampling Layer 사용

이 내용은 block 내부에 반영된 내용이 아니라, stage 넘어가면서 downsample을 할 때에 해당하는 내용입니다. 위 diagram에는 드러나지 않는 부분이니 참고해주세요~    
Swim Transformer가 patch merging을 할 때 2x2 neighborhood patch들의 channel을 concat하여(4C) 이에 대해 2C 의 channel size가 되도록 내뱉습니다. 이를 Conv. 로 표현하면 channel size가 2배가 되고, kernel size는 2x2, stride 2인 것입니다. 저자는 ResNet에서 kernel size 3x3, stride 2, padding 1 로 마치 공식처럼 쓰이던 downsample하는 conv. layer 대신에 Swim Transformer에서의 downsample 방식을 사용합니다. 그냥 이를 대치시키면 학습이 diverge 되는데, block-level에서 spatial resolution이 변경되는 지점에 LN을 적당히 넣어주면 이에 대해 학습이 stabilize 되는 것을 찾아냅니다. 이는 Swim Transformer도 적용된 방식입니다.

그리고 이를 통해 최종적으로 ConvNeXt는 아래의 성능 향상을 보였습니다.

- ResNet-50: 81.5% -> 82.0% (+0.5%, 누적: +5.9%)

## Training Details

저자는 본 모델을 어떻게 학습했는 지 Hyperparameter와 학습 세팅도 함께 보여주고 있습니다.  
Augmentation, optimizer 등 여러 부분에서 등장한 최신의 학습 방법론들을 한 곳에 모아둔 느낌입니다.

### (Pre-)Training Settings

<img className={styles.figCenter} src={figPretrain} alt="pretrain" />

### Finetuning Settings

<img className={styles.figCenter} src={figFinetune} alt="finetune" />

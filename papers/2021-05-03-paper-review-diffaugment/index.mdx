---
slug: diffaugment
title: "Differentiable Augmentation for Data-Efficient GAN Training"
description: ì ì€ ë°ì´í„°ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ GAN í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ í™•ì¸í•´ë´…ë‹ˆë‹¤.
image: img/default.png
authors: [hkyu]
tags: [paper-review, gan]
---

import clsx from 'clsx';
import styles from '../blog.module.css';

import figAblation from './image/ablation_augmentation.png';
import figAugResult from './image/augmentation_type_result.png';
import figAugType from './image/augmentation_type.png';
import figOverfitting from './image/d_overfitting.png';
import figInterpolation from './image/interpolation.png';
import figModelSize from './image/model_size_fid.png';
import figReg from './image/r1_regularization.png';
import figResultCompare from './image/result_compare.png';
import figTrainingMethod from './image/training_method.png';
import figVsStylegan from './image/vs_stylegan2.png';

*NeurIPS 2020. Jun-Yan Zhu and Song Han.*

ì°¸ê³ ë¡œ Song Han ì—°êµ¬ì‹¤ì€ Neural Network Compression ë¶„ì•¼ì— ìˆì–´ Topì„ ë‹¬ë¦¬ê³  ìˆëŠ” ì—°êµ¬ì‹¤ì´ë‹¤. í›„ì— ì†Œê°œí•  AnycostGANì—ì„œë„ DiffAugmentê°€ ì–¸ê¸‰ì´ ëœë‹¤.

Han Labì€ ê¸°ì¡´ì—ëŠ” ë°©ë²•ë¡ ì— ìˆì–´ Network Pruning, KD(Knowledge Distillation) ë“±ì— ì§‘ì¤‘ì„ í–ˆê³ , TinyTL ë“± Activationì— ëŒ€í•œ ê²½ëŸ‰í™”ë„ ì—°êµ¬ë¥¼ ì§„í–‰í–ˆë‹¤. ë‹¤ë§Œ, ëŒ€ë¶€ë¶„ Taskê°€ Image Recognitionì— êµ­í•œë˜ì–´ ìˆì—ˆë‹¤. Song Hanì€ CVPR2020ì—ì„œ GAN Compressionì´ë¼ëŠ” ë…¼ë¬¸ì„ í†µí•´ Image Generationì— ëŒ€í•œ Compression ë…¼ë¬¸ì„ ì‹œì‘ìœ¼ë¡œ, GAN ê´€ë ¨ ì—°êµ¬ë¥¼ ì§€ì†í•´ì„œ ì´ì–´ì˜¤ê³  ìˆë‹¤. 

## ë°°ê²½ ì§€ì‹

### ê¸°ì¡´ì˜ Regularization

GAN Training ìì²´ê°€ êµ‰ì¥íˆ Unstableí•œ Processì´ê¸° ë•Œë¬¸ì—, ì¶”ê°€ì ì¸ Regularizationì´ ë§ì´ í•„ìš”í•˜ë‹¤. ì§€ê¸ˆê¹Œì§€ ì—¬ëŸ¬ê°€ì§€ Regularization ë°©ì‹ë“¤ì´ ë“±ì¥í–ˆë‹¤.

- Instance Noise
- Jensen-Shannon Regularization
- Gradient Penalty
- Spectral Normalization
- Adversarial Defense Regularization
- Consistency Regularization

ì´ëŸ¬í•œ Regularization Methodë“¤ì€ Input ì´ë¯¸ì§€ì— ëŒ€í•´ ëŒ€ì‘í•˜ëŠ” ê²ƒì´ì§€, augmentationì— ëŒ€í•´ ì†Œí™”í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ì´ ì•„ë‹ˆë‹¤.

ì €ìëŠ” ì—¬ëŸ¬ ê°€ì§€ Augmentationì— ëŒ€í•´ì„œë„ ì˜ working í•˜ëŠ” Discriminatorë¥¼ êµ¬ì¶•í•˜ê³ ì í–ˆë‹¤.

### DëŠ” ì§€ê¸ˆê¹Œì§€ Overfitting í•´ì™”ë‹¤

<img className={styles.figCenter} src={figOverfitting} alt="d_overfitting" />

BigGANì„ ì ì€ ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµí–ˆì„ ë•Œ, í•™ìŠµì„ í•˜ë‹¤ê°€ collapseí•˜ëŠ” ê±¸ ë³´ì—¬ì£¼ëŠ” Figureë‹¤. ì™¼ìª½ì—ì„œ ë³´ë©´ ë¹¨ê°„ìƒ‰(CIFAR-10 10%ë§Œ ê°€ì§€ê³  í•™ìŠµ) ê·¸ë˜í”„ê°€ íŠ€ì–´ë²„ë¦¬ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. ì´ê²Œ ì™œ ê·¸ëŸ´ê¹Œ í•˜ê³  Discriminatorë¥¼ ë³´ë©´, ì•„ë‹ˆë‚˜ ë‹¤ë¥¼ê¹Œ Training Accuracyê°€ ë¹ ë¥´ê²Œ í•™ìŠµì´ ë¼ì„œ ê·¸ë ‡ë‹¤.

ê·¸ëŸ°ë° Dì— ëŒ€í•´ ê° Iterationì— ëŒ€í•œ Validation Acc.ë¥¼ ì¸¡ì •í•´ë³´ë©´, mode collapseê°€ ë˜ì—ˆë‹¤ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰, Dê°€ training set ì„ memorize í•´ì™”ê³ , ì´ë¡œ ì¸í•´ generalizeê°€ ì•ˆëë‹¤ëŠ” ê±¸ ë³´ì—¬ì¤€ë‹¤.

## Training Method

<img className={clsx(styles.figCenter, styles.medium)} src={figTrainingMethod} alt="training_method" />

DiffAugmentì˜ í•™ìŠµ ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼.

ì›ë³¸ ì´ë¯¸ì§€(x), ìƒì„±í•œ ì´ë¯¸ì§€(G(z)) ëª¨ë‘ì— augmentation(T)ì„ ì ìš©í•œë‹¤.

Augmentation Senarioì— ë”°ë¼ì„œ ì—¬ëŸ¬ ê°€ì§€ Caseë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

Augment Reals Only: Real ì´ë¯¸ì§€ì— ëŒ€í•´ë§Œ Augmentationì„ ì§„í–‰í•¨ (ië§Œ ì§„í–‰.)
â†’ Augmentation í•œ ê±¸ ê·¸ëŒ€ë¡œ ëª¨ë°©í•˜ì—¬ ìƒì„±í•œë‹¤.

Augment D Only: Discriminatorì— ë„£ëŠ” Inputë“¤ì— ëŒ€í•´ ì§„í–‰í•¨ (i, ii ë§Œ ì§„í–‰. iiiëŠ” ê·¸ëŒ€ë¡œ)
â†’ Unbalanced Optimization ì— ì˜í•´ Diverge í•´ë²„ë¦°ë‹¤.

D perfectly classifies the augmented images (both T(x) and T(G(z)) but barely
recognizes G(z) (i.e., fake images without augmentation)
ì´ ë•Œë¬¸ì— Gì˜ gradient updateë¥¼ í•  ë•Œ, Discriminatorê°€ ì˜ working í•˜ì§€ ëª»í•˜ë©´ì„œ, Gì— ëŒ€í•œ í•™ìŠµì— ë°©í•´ê°€ ë¨.

## Result

### StyleGAN2ì™€ ë¹„êµ

<img className={styles.figCenter} src={figVsStylegan} alt="vs_stylegan2" />

ê¸°ì¡´ StyleGAN2ê³¼ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” DiffAugmentë¥¼ ì ìš©í–ˆì„ ë•Œë¥¼ ë¹„êµí•˜ëŠ” Figure.  
StyleGAN2ëŠ” ë°ì´í„°ê°€ ì‘ì•„ì§ˆìˆ˜ë¡, FID, IS ê°’ì˜ ë³€í™”ê°€ dramaticí•œë°, DiffAugmentëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ generalizeë˜ì–´ ìˆë‹¤ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. StyleGAN2ê°€ ì‘ì€ ë°ì´í„°ì— generalizeê°€ ì•ˆëœë‹¤ëŠ” ê±´ ì¢€ ì•Œë ¤ì§„ ì‚¬ì‹¤ì´ì—ˆìœ¼ë‹ˆ, Discriminator Training Methodì— ì§‘ì¤‘í•œ ADAë‚˜ Freeze Dì™€ ë¹„êµí•˜ëŠ” Figureë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ê¸°ëŒ€í•˜ê²Œ ëœë‹¤.

### Low-Shot Generation

<img className={styles.figCenter} src={figResultCompare} alt="result_compare" />

Low-shot generation without pre-training.

ê°ê° ì˜¤ë°”ë§ˆ 100ì¥, ê³ ì–‘ì´ 160ì¥, ê°•ì•„ì§€ 389ì¥ë§Œì„ ê°€ì§€ê³  í•™ìŠµì„ í•˜ì—¬ ìƒì„±í•´ë‚¸ ì´ë¯¸ì§€ë“¤ì´ë‹¤. (w/o pre-training)

## Result

### Training Performance

<img className={styles.figCenter} src={figAugResult} alt="augmentation_type_result" />

Augmentation ì¡°í•©ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” DiffAugment Training Performance (CIFAR-10 Data 100% ì‚¬ìš©)

BigGAN ëŒ€ë¹„ Discriminatorê°€ Validationì— ìˆì–´ì„œë„ generalize í•˜ì—¬ í•™ìŠµë˜ì—ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. DiffAugment ìì²´ëŠ” Low Datasetì— ëŒ€í•´ì„œë„ Generationì„ ì•ˆì •ì ìœ¼ë¡œ í•  ìˆ˜ ìˆë‹¤ëŠ” Noveltyë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, ë³¸ FigureëŠ” BigGANë„ í•™ìŠµí–ˆë‹¤ê³  í•˜ëŠ” ë°ì´í„°ê°€ CIFAR-10 ì „ì²´ë¥¼ ì‚¬ìš©í•œ ê²ƒì´ì—ˆìœ¼ë‹ˆ, í•´ë‹¹ ì¡°ê±´ìœ¼ë¡œ Comparisonì„ í•œ ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.

### Interpolationë„ ê°€ëŠ¥í•˜ë‹¤

<img className={styles.figCenter} src={figInterpolation} alt="interpolation" />

Style Spaceì—ì„œ Interpolationì´ ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì£¼ëŠ” Figure.

ì ì€ ë°ì´í„°ì˜€ì§€ë§Œ (ì˜¤ë°”ë§ˆ, ê³ ì–‘ì´, ê°•ì•„ì§€ ì°¸ì¡°), Interpolationì´ ê°€ëŠ¥í•  ì •ë„ë¡œ generalizeí•˜ê²Œ í•™ìŠµì´ ë˜ì—ˆë‹¤.

### Model Sizeì— ëŒ€í•œ ë¶„ì„

<img className={styles.figCenter} src={figModelSize} alt="model_size_fid" />

Model sizeê°€ FIDì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì£¼ëŠ”ê°€ì— ëŒ€í•œ Figure

CIFAR-10 ë°ì´í„° 10%ì— ëŒ€í•´ì„œë§Œ í•™ìŠµì„ í–ˆì„ ë•Œ, generalizeê°€ ë  ìˆ˜ ìˆëŠ” ê°€ë¥¼ ë‹¤ë£¬ ë¶€ë¶„ì´ë‹¤. BigGAN ëŒ€ë¹„ channel sizeë¥¼ ì¤„ì—¬ê°€ë©° ë¹„êµë¥¼ í•´ë´ë„, ì–´ë–¤ capacityë“  ìƒê´€ì—†ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. StyleGAN2ì—ì„œ ì‚¬ìš©í•˜ëŠ” R1 Regularization ê´€ë ¨í•´ì„œë„, hyperparameterê°€ ë™ì¼í•œ ì„¸íŒ…ì¼ ë•Œ ë” ë‚˜ì€ FIDë¥¼ ë³´ì—¬ì¤¬ë‹¤.

### Regularization

:::note

ğŸ’¡ **R1 Regularization**  
Gradient Penaltyë¡œ regularize í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, Discriminatorì˜ Real Dataì— ëŒ€í•´ Penalizeë¥¼ ì¤€ë‹¤.

$$
R_{1}(\psi):=\frac{\gamma}{2} \mathrm{E}_{p_{\mathcal{D}}(x)}\left[\left\|\nabla D_{\psi}(x)\right\|^{2}\right]
$$

when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the GAN game.

:::

### Ablation for Augmentation

<img className={styles.figCenter} src={figAblation} alt="ablation_augmentation" />

ì—¬ëŸ¬ ê°€ì§€ Augmentationì— ëŒ€í•´ FIDë¥¼ í™•ì¸í•´ë³¸ ê²°ê³¼ë‹¤.
Ablationì„ ì§„í–‰í•œ ëì—, color distortion, translation, cutoutë§Œ ì¤€ ê²ƒìœ¼ë¡œ í™•ì¸í–ˆë‹¤. (ë¨¼ì € ìƒê°í•˜ê³  ì¤€ ê±´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ í™•ì¸í–ˆë‹¤.)
StyleGAN2ì˜ FIDì´ê³ , CIFAR-10 10% ë°ì´í„°ë§Œì„ ê°€ì§€ê³  Training í–ˆì„ ë•Œì˜ ê²°ê³¼ë‹¤.
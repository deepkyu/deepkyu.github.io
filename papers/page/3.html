<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-223362952-2","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="alternate" type="application/rss+xml" href="/story/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/story/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/papers/rss.xml" title="Hyoung-Kyu Song RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/papers/atom.xml" title="Hyoung-Kyu Song Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Blog | Hyoung-Kyu Song</title><meta data-rh="true" property="og:title" content="Blog | Hyoung-Kyu Song"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" property="og:url" content="https://deepkyu.github.io//papers/page/3"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://deepkyu.github.io//papers/page/3"><link data-rh="true" rel="alternate" href="https://deepkyu.github.io//papers/page/3" hreflang="en"><link data-rh="true" rel="alternate" href="https://deepkyu.github.io//papers/page/3" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.735bdc22.css">
<link rel="preload" href="/assets/js/runtime~main.34333774.js" as="script">
<link rel="preload" href="/assets/js/main.55df8712.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Deepkyu" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="Deepkyu" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title"></b></a><a class="navbar__item navbar__link" href="/blog">Blog</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/papers">Reviews</a><a class="navbar__item navbar__link" href="/story">Story</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/cv">CV</a><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://www.linkedin.com/in/deepkyu" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">All Posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/acon">Activate or Not: Learning Customized Activation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/diffaugment">Differentiable Augmentation for Data-Efficient GAN Training</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/papers/freezed">FreezeD: a Simple Baseline for Fine-tuning GANs</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/papers/freezed">FreezeD: a Simple Baseline for Fine-tuning GANs</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2020-09-24T00:00:00.000Z" itemprop="datePublished">September 24, 2020</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/deepkyu.png" alt="Hyoung-Kyu Song"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Hyoung-Kyu Song</span></a></div><small class="avatar__subtitle" itemprop="description">AI Researcher (Vision)</small></div></div></div></div></header><meta itemprop="image" content="https://deepkyu.github.io//img/default.png"><div class="markdown" itemprop="articleBody"><p><a href="https://arxiv.org/abs/2002.10964" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2002.10964-brightgreen.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/sangwoomo/FreezeD" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?message=Official%20Repo&amp;logo=Github&amp;labelColor=grey&amp;color=blue&amp;logoColor=white&amp;label=%20&amp;style=flat-square" alt="githubio"></a></p><blockquote><p>Mo, Sangwoo, et al. &quot;Freeze the discriminator: a simple baseline for fine-tuning gans.&quot;<br>
<!-- -->CVPR AI for Content Creation Workshop (2020).</p></blockquote><h2 class="anchor anchorWithStickyNavbar_mojV" id="contribution">Contribution<a class="hash-link" href="#contribution" title="Direct link to heading">​</a></h2><p>GAN을 fine-tuning하는 데에 있어, discriminator의 lower layer들(classifier 말고 visul repr. (=visual feature) extractor)에 대해 freeze하고 G/D에 대해 fine-tuning을 진행하면, limited data로 효과적인 transfer learning을 할 수 있다.</p><p>즉, image classification 등 recognition(또는 understanding)에서 사용하는 fine-tuning 방식 (visual representation은 freeze, classifier만 use_gradient)이 GAN 모델에서도 유용하게 사용될 수 있다.</p><img class="figCenter__7gH" src="/assets/images/result_graph-131a4e1493d0c89f7b52e6d88ab11441.png" alt="result_graph"><h2 class="anchor anchorWithStickyNavbar_mojV" id="background">Background<a class="hash-link" href="#background" title="Direct link to heading">​</a></h2><p>최근 SotA GAN들은 많은 양의 training data를 요구할 뿐만 아니라, 매우 큰 computational resource를 요구하는 경우가 많아, 실질적인 senario에 대입하는 데에 어려움이 많다.</p><p>그래서 이를 해결하고자 많은 접근법들이 recogntion task에서의 성공적인 사례들을 바탕으로 GAN에서의 transfer learning을 시도하고 있고, 이를 통해 한정된 데이터를 가지고 image generation 하는 방법들을 연구하고 있다.</p><p>하지만 현재까지 Transfer Learning을 진행한 많은 사례들이 한정된 training data로 진행했을 때, overfitting이 되는 경우가많았고, 특히 dataset의 distribution이 pre-training을 할 때 사용한 large dataset과 많이 다를 경우, robust하지 못하는 일이 많았다 (not robust in learning a significant distribution shift).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="previous-methods-for-gan-transfer-learning">Previous Methods for GAN transfer learning<a class="hash-link" href="#previous-methods-for-gan-transfer-learning" title="Direct link to heading">​</a></h3><p>우선 기존의 GAN Finetuning으로 시도된 방식은 아래와 같이 나열할 수 있다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="fine-tuning">Fine-tuning<a class="hash-link" href="#fine-tuning" title="Direct link to heading">​</a></h4><p>Target Model(=limited data에 대한 model)의 generator와 discriminator 각각에 대해 Source Model(=trained with large dataset)의 pre-trained weight를 load하여, 해당 checkpoint부터 training을 진행한다.</p><p>하지만, 이러한 fine-tuning 방식은 overfit되는 문제점을 항상 가지고 있고, 이를 위해 적절한 regularization이 매번 필요하다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="scaleshift">Scale/Shift<a class="hash-link" href="#scaleshift" title="Direct link to heading">​</a></h4><p>위와 같은 naive fine-tuning이 overfit에 빠지기 쉽다는 단점이 있기에, scale + shift는 다른 weight는 그대로 놔두고 batchnorm 등 normalization layer만 update하는 방식으로 이를 진행하고자 시도했다. 하지만, 이는 restriction이 명확해서 그리 좋은 결과를 보여주지 않았고, 특히 source dataset(=large dataset)과 target dataset(=limited dataset) 간 distribution shift가 극명할 경우, 결과가 더욱 안 좋게 나왔다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="generative-latent-optimization-glo">Generative latent optimization (GLO)<a class="hash-link" href="#generative-latent-optimization-glo" title="Direct link to heading">​</a></h4><p>GAN loss는 discriminator로부터 주어지는 loss이다보니, limited data에 reliable 하지 않다. GLO는 이로부터 source data로 학습한 generator만 떼어다가 L1 + perceptual loss의 조합으로 supervised learning 하는 방식을 말한다. 이 때, GLO는 overfitting을 방지하기 위해 generator와 latent codes를 동시에 optimize하는 방식으로 훈련한다. 즉, latent vector와 sample data가 항상 1:1로 되도록 학습한다.  이를 통해 generator가 sample들에 대해 generalize 되도록 하는 방법이다.</p><p>GLO가 이런 방향으로 학습을 하다 보니 stability는 보장이 되지만, 아무래도 adversarial loss가 없다보니 image generation 결과가 결국에는 blurry 해진다는 단점이 있다. 그렇다고 Adversarial 하게 별도로 discrimator를 붙일수도 있겠지만, 이러한 방향으로 진행될 때 기본적으로 source data에 대한 discriminator의 prior knowledge가 아예 증발해버리므로, 이 역시 그렇게 효율적이라고는 볼 수 없다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="minegan">MineGAN<a class="hash-link" href="#minegan" title="Direct link to heading">​</a></h4><p>MineGAN은 generator의 overfitting을 피하기 위해, generator를 수정하여 latent code를 수정하는 방법을 제시한다. MineGAN은 latent code 간 transfer를 담당하는 miner(채굴) network를 학습시키는데, 이러한 방식은 source와 target distribution이 공유하고 있는 바가 있을 때 효과적이겠지만, 두 dataset가 disjoint 한 상황에서는 generalize 될 수 없다는 단점을 가지고 있다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="suggested-methods">Suggested Methods<a class="hash-link" href="#suggested-methods" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_mojV" id="freezed">FreezeD<a class="hash-link" href="#freezed" title="Direct link to heading">​</a></h4><p>Discriminator의 lower layer(=visual representation)만 freeze, upper layer는 fine-tune. 저자가 성능한 것 중에 가장 좋은 성능을 보였음.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="l2-sp">L2-SP<a class="hash-link" href="#l2-sp" title="Direct link to heading">​</a></h4><p>Fine-tuning하는 방식인데, source model의 parameter와 target model의 parameter를 L2-norm regularize 하는 방식으로 진행하여 target model이 source model의 knowledge로부터 너무 멀어지지 않게 한다.</p><p>하지만, 이는 그렇게 좋은 성능을 보이지 못했다. L2-norm 한다고 하면 이 방식이 결국 freezing layer로 선택한 layer에는 infinite weight를 주고, 다른 layer에는 weight를 0으로 주는 방식으로 굳어지는데, 이것보다는 각 Layer 에 적절한 weight 를 주는 것이 당연 좋은 결과를 낼 것이다.</p><h4 class="anchor anchorWithStickyNavbar_mojV" id="feature-distillation">Feature Distillation<a class="hash-link" href="#feature-distillation" title="Direct link to heading">​</a></h4><p>Classifier의 transfer learning을 할 때, 요즘 제일 많이 사용하는 방식이다.</p><p>저자는 source model과 target model을 distill 하는 방식으로 진행했다. Computation으로 보면 FreezeD보다 2배 가량 연산량이 많은데, FreezeD랑 비교했을 때 우위를 점하는 경우도 있었다. 이 논문은 FreezeD가 최종적으로 제안하는 method이기는 하지만, Feature Distillation이 미래에 더 좋은 성능을 보이는 방향일 것이라고 이야기 한다.</p><img class="figCenter__7gH" src="/assets/images/table-8729e8237c9747aac68039866747ac52.png" alt="table"><h2 class="anchor anchorWithStickyNavbar_mojV" id="result">Result<a class="hash-link" href="#result" title="Direct link to heading">​</a></h2><p>저자는 Unconditional GAN과 Conditional GAN에 모두 generic 하게 적용될 수 있는 방법인지를 파악하기 위해, 두 가지 adversarial model을 들고 왔다. Unconditional GAN으로는 stylegan, conditional GAN으로는 SNGAN-projection 을 사용했다. 그리고 측정 metric 으로는 FID(Frechet Inception Distance)만을 사용했다.</p><p>각 Dataset class 별로 FID를 각각 계산했고, Fine-tuning 한 것과 FreezeD 두 case에 대한 결과만을 비교했다.</p><p>FreezeD 가 FID에 있어서 항상 앞서는 결과를 보여줬다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="특이점">특이점<a class="hash-link" href="#특이점" title="Direct link to heading">​</a></h2><p>StyleGAN을 실험할 때, tensorflow로 되어 있는 저자 repo가 아니라, unofficial pytorch repo를 사용했다.</p><p>pytorch repo도 많은 유저들이 안정성 검증을 하기는 했었는데, 이걸 사용하여 논문이 accept이 되었으니, 나도 써도 되겠다는 생각이 든다. (이렇게 guarantee 받는 방법이...!)</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/papers/tags/paper-review">paper-review</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/papers/tags/gan">gan</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/papers/page/2"><div class="pagination-nav__label">Newer Entries</div></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Contents</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/papers">Reviews</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/story">Story</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/deepkyu" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/deepkyu" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Deepkyu. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.34333774.js"></script>
<script src="/assets/js/main.55df8712.js"></script>
</body>
</html>